{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04/01/2020_FINAL_CS779_Ultimate_SubTask2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a223b994e7ea4676a7935d17a7408222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f67d236972ce4e8d8ffee5625f7e95ff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cc705f7a2a614ee58aff93ae2ff8535b",
              "IPY_MODEL_f89db72c59a944ea8f83b5694d92ef23"
            ]
          }
        },
        "f67d236972ce4e8d8ffee5625f7e95ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc705f7a2a614ee58aff93ae2ff8535b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f20017579d1e4e159858fba92f17483a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 385,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 385,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9339193a02744e2cbff7c0e270ae2763"
          }
        },
        "f89db72c59a944ea8f83b5694d92ef23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1107de7fc3f8494399486fa308665ccc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 385/385 [00:00&lt;00:00, 4.27kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca2865f1d63942f2bf2a48f08d3c6dc1"
          }
        },
        "f20017579d1e4e159858fba92f17483a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9339193a02744e2cbff7c0e270ae2763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1107de7fc3f8494399486fa308665ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca2865f1d63942f2bf2a48f08d3c6dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44ca003785034507a05bef4be9686c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f8a3b77ac8b34b6d9453330ecaac2856",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fdc36ed72a2e49b580ff0a137a57bbc7",
              "IPY_MODEL_6ddd9c5ad8ef48adb65bf6b924a7179d"
            ]
          }
        },
        "f8a3b77ac8b34b6d9453330ecaac2856": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdc36ed72a2e49b580ff0a137a57bbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9eb1b888fc8242e68ea1176af6c6d2b6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 227845,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 227845,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_672faf9fd8dc4597a748b11dd9c8e130"
          }
        },
        "6ddd9c5ad8ef48adb65bf6b924a7179d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f5d58794e7da4aa9bbdeb53da208a03f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 228k/228k [00:00&lt;00:00, 1.47MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85d23e14c2094ce1b848a1cfe2bfe0ff"
          }
        },
        "9eb1b888fc8242e68ea1176af6c6d2b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "672faf9fd8dc4597a748b11dd9c8e130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f5d58794e7da4aa9bbdeb53da208a03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85d23e14c2094ce1b848a1cfe2bfe0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3ec628bb771470ca732d68686e46d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cc678054bd10472caf3c276ebea8e72c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0cbbf03fe4b748f5bd0fe09cc3374c72",
              "IPY_MODEL_bd0e5eefd99748edb41367ad187aa0e1"
            ]
          }
        },
        "cc678054bd10472caf3c276ebea8e72c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0cbbf03fe4b748f5bd0fe09cc3374c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cda5b20916644fcb88cffc2351c2ff61",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442221694,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442221694,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c012a40dc5048e9bdcaf27ae453c243"
          }
        },
        "bd0e5eefd99748edb41367ad187aa0e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79c63becb6304dd391a6a2a78a4e61ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442M/442M [00:11&lt;00:00, 37.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_623428663e1843959d9f107b63b661d8"
          }
        },
        "cda5b20916644fcb88cffc2351c2ff61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c012a40dc5048e9bdcaf27ae453c243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79c63becb6304dd391a6a2a78a4e61ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "623428663e1843959d9f107b63b661d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01f98f7306544ee0aa000ceb61da4b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_052558e5f86d418d9d8f02f59444f612",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_87c08661fc4b487aacc7a641f1c9d618",
              "IPY_MODEL_41bf7d4b68884096b4e5577289678c3c"
            ]
          }
        },
        "052558e5f86d418d9d8f02f59444f612": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87c08661fc4b487aacc7a641f1c9d618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2f4be0e6df745068776030b5f43b609",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 385,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 385,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a28311b8061b4dc0945fdd9a0f170d15"
          }
        },
        "41bf7d4b68884096b4e5577289678c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0cba5eef1e1d4fac9e91a21c54e4b3e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 385/385 [00:01&lt;00:00, 246B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ddcaa51905b40c6886eeec5c05e0bb2"
          }
        },
        "d2f4be0e6df745068776030b5f43b609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a28311b8061b4dc0945fdd9a0f170d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0cba5eef1e1d4fac9e91a21c54e4b3e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ddcaa51905b40c6886eeec5c05e0bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51ff07514d334dbe9398aba41c61cdac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3f76c39eefab419aa0b20f71a420b4aa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2e557a37c1a44592966f71312000d495",
              "IPY_MODEL_fce7c90218724d37ab50429c2ac8ad7d"
            ]
          }
        },
        "3f76c39eefab419aa0b20f71a420b4aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e557a37c1a44592966f71312000d495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0f47104be3364d97884acc37ebce15d0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 227845,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 227845,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13cc564748fe4b0193276f9301ae15e9"
          }
        },
        "fce7c90218724d37ab50429c2ac8ad7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f76b6b04a10943fda70afa4dcfc252e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 228k/228k [00:01&lt;00:00, 206kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8cd6fbab9cd64e90afdb4f91f8c6752b"
          }
        },
        "0f47104be3364d97884acc37ebce15d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13cc564748fe4b0193276f9301ae15e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f76b6b04a10943fda70afa4dcfc252e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8cd6fbab9cd64e90afdb4f91f8c6752b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b97ba51e67e6438aba016d25b40ea52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d306924ebc649a8a682b27b661c99f1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4f11bd433f5949699d0f3e25a8173360",
              "IPY_MODEL_e360cdaa322045aa8f5c2377011a8390"
            ]
          }
        },
        "1d306924ebc649a8a682b27b661c99f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f11bd433f5949699d0f3e25a8173360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e462ff72ab1c41f6a4441622d49dca73",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442221694,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442221694,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65b21b70ab9c49bf88c50e553439df09"
          }
        },
        "e360cdaa322045aa8f5c2377011a8390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02169ecf7e1747ecb547487744085a64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442M/442M [00:36&lt;00:00, 12.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8f8fdea391d4a35a09c64361069ac7b"
          }
        },
        "e462ff72ab1c41f6a4441622d49dca73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65b21b70ab9c49bf88c50e553439df09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02169ecf7e1747ecb547487744085a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8f8fdea391d4a35a09c64361069ac7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuqOGwMHm9i0"
      },
      "source": [
        "#Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMYQJ7OM4OTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6b9ae7-2952-4598-fa80-df339e1203c7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EecjZopfSV0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ab052f-3a9c-4f22-fb56-62262b55f615"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/Sem Eval Task 11 Group 7/Train v2.zip\" -d \"/content/train\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Sem Eval Task 11 Group 7/Train v2.zip\n",
            "   creating: /content/train/Train v2/\n",
            "   creating: /content/train/Train v2/natural_language_inference/\n",
            "   creating: /content/train/Train v2/natural_language_inference/0/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/1606.01549v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/1606.01549v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/1606.01549v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/0/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/0/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/triples/ablation-analysis.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/0/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/1/\n",
            "   creating: /content/train/Train v2/natural_language_inference/10/\n",
            "   creating: /content/train/Train v2/natural_language_inference/100/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/1801.01641v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/1801.01641v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/1801.01641v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/100/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/100/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/100/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/1707.02786v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/1707.02786v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/1707.02786v4.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/10/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/10/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/10/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/11/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/1706.02596v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/1706.02596v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/1706.02596v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/11/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/11/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/11/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/12/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/1708.02312v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/1708.02312v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/1708.02312v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/12/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/12/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/triples/ablation-analysis.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/12/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/12/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/13/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/1803.09074v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/1803.09074v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/1803.09074v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/13/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/13/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/13/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/14/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/1904.04365v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/1904.04365v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/1904.04365v4.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/14/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/14/triples/\n",
            " extracting: /content/train/Train v2/natural_language_inference/14/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/14/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/15/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/1805.11360v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/1805.11360v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/1805.11360v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/15/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/15/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/15/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/16/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/1702.03814v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/1702.03814v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/1702.03814v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/16/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/16/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/16/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/17/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/1705.02798v6-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/1705.02798v6-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/1705.02798v6.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/17/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/17/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/17/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/17/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/18/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/1511.06038v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/1511.06038v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/1511.06038v4.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/18/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/18/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/18/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/18/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/19/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/1712.02047v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/1712.02047v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/1712.02047v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/19/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/19/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/19/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/1506.02075v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/1506.02075v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/1506.02075v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/1/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/1/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/2/\n",
            "   creating: /content/train/Train v2/natural_language_inference/20/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/1808.08762v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/1808.08762v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/1808.08762v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/20/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/20/triples/\n",
            " extracting: /content/train/Train v2/natural_language_inference/20/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/20/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/21/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/1709.04696v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/1709.04696v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/1709.04696v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/21/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/21/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/21/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/21/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/22/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/1704.07415v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/1704.07415v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/1704.07415v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/22/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/22/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/triples/ablation-analysis.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/22/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/22/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/23/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/1711.07341v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/1711.07341v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/1711.07341v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/23/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/info-units/results.json  \n",
            " extracting: /content/train/Train v2/natural_language_inference/23/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/23/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/23/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/24/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/1712.03556v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/1712.03556v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/1712.03556v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/24/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/24/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/24/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/25/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/1712.03609v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/1712.03609v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/1712.03609v4.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/25/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/info-units/results.json  \n",
            " extracting: /content/train/Train v2/natural_language_inference/25/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/25/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/25/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/26/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/1909.02209v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/1909.02209v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/1909.02209v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/26/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/26/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/26/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/27/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/1804.09541v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/1804.09541v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/1804.09541v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/27/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/27/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/27/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/28/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/1710.09537v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/1710.09537v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/1710.09537v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/28/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/28/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/28/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/29/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/1901.07696v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/1901.07696v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/1901.07696v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/29/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/29/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/29/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/1801.08290v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/1801.08290v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/1801.08290v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/2/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/2/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/3/\n",
            "   creating: /content/train/Train v2/natural_language_inference/30/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/1404.4326v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/1404.4326v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/1404.4326v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/30/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/30/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/30/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/31/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/31/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/P19-1465-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/P19-1465-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/P19-1465.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/31/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/31/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/31/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/32/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/1810.06683v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/1810.06683v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/1810.06683v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/32/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/32/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/32/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/32/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/33/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/1804.00079v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/1804.00079v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/1804.00079v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/33/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/33/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/33/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/34/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/1905.06933v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/1905.06933v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/1905.06933v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/34/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/34/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/34/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/35/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/1901.02262v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/1901.02262v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/1901.02262v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/35/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/35/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/35/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/36/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/1809.02794v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/1809.02794v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/1809.02794v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/36/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/36/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/36/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/37/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/1710.10723v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/1710.10723v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/1710.10723v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/37/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/37/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/37/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/38/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/1707.09098v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/1707.09098v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/1707.09098v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/38/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/38/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/38/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/39/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/39/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/N16-1108-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/N16-1108-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/N16-1108.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/39/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/39/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/1605.05573v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/1605.05573v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/1605.05573v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/3/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/3/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/4/\n",
            "   creating: /content/train/Train v2/natural_language_inference/40/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/1703.02620v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/1703.02620v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/1703.02620v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/40/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/40/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/40/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/41/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/1910.13461v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/1910.13461v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/1910.13461v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/41/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/41/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/41/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/42/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/1810.09580v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/1810.09580v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/1810.09580v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/42/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/42/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/42/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/43/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/1707.04412v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/1707.04412v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/1707.04412v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/43/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/43/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/43/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/43/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/44/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/1805.08092v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/1805.08092v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/1805.08092v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/44/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/44/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/44/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/45/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/45/1611.01724v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/45/1611.01724v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/45/1611.01724v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/45/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/45/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/45/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/45/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/45/info-units/research-problem.json  \n",
            " extracting: /content/train/Train v2/natural_language_inference/45/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/45/triples/\n",
            " extracting: /content/train/Train v2/natural_language_inference/45/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/45/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/45/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/46/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/1712.07040v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/1712.07040v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/1712.07040v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/46/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/46/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/46/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/47/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/1508.05326v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/1508.05326v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/1508.05326v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/47/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/47/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/47/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/48/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/1606.02245v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/1606.02245v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/1606.02245v4.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/48/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/48/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/48/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/49/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/1709.04348v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/1709.04348v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/1709.04348v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/49/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/49/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/49/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/1711.00106v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/1711.00106v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/1711.00106v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/4/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/4/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/4/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/4/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/5/\n",
            "   creating: /content/train/Train v2/natural_language_inference/50/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/1707.07847v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/1707.07847v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/1707.07847v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/50/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/50/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/50/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/50/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/51/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/1906.08862v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/1906.08862v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/1906.08862v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/51/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/51/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/51/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/52/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/52/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/K17-1009-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/K17-1009-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/K17-1009.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/52/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/52/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/53/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/1705.02364v5-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/1705.02364v5-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/1705.02364v5.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/53/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/53/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/53/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/54/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/1703.00572v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/1703.00572v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/1703.00572v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/54/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/54/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/54/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/55/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/1406.3676v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/1406.3676v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/1406.3676v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/55/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/55/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/55/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/56/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/1711.08028v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/1711.08028v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/1711.08028v4.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/56/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/56/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/56/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/57/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/1511.06361v6-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/1511.06361v6-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/1511.06361v6.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/57/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/57/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/57/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/58/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/1609.05284v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/1609.05284v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/1609.05284v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/58/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/58/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/58/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/58/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/59/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/1704.04565v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/1704.04565v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/1704.04565v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/59/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/59/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/59/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/1905.12897v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/1905.12897v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/1905.12897v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/5/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/5/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/5/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/6/\n",
            "   creating: /content/train/Train v2/natural_language_inference/60/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/60/1804.07983v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/60/1804.07983v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/60/1804.07983v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/60/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/60/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/60/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/60/info-units/research-problem.json  \n",
            " extracting: /content/train/Train v2/natural_language_inference/60/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/60/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/60/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/60/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/61/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/1812.01840v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/1812.01840v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/1812.01840v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/61/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/61/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/61/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/62/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/1809.03449v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/1809.03449v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/1809.03449v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/62/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/62/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/62/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/63/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/63/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/63/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/W18-2603-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/W18-2603-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/63/W18-2603.pdf  \n",
            "   creating: /content/train/Train v2/natural_language_inference/64/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/1911.04118-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/1911.04118-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/1911.04118.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/64/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/64/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/64/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/65/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/1602.03609v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/1602.03609v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/1602.03609v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/65/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/65/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/65/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/66/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/1512.08849v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/1512.08849v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/1512.08849v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/66/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/66/triples/\n",
            " extracting: /content/train/Train v2/natural_language_inference/66/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/66/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/67/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/1610.09996v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/1610.09996v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/1610.09996v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/67/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/67/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/67/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/68/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/1608.07905v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/1608.07905v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/1608.07905v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/68/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/68/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/68/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/69/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/1710.06481v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/1710.06481v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/1710.06481v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/69/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/69/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/69/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/1812.10464v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/1812.10464v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/1812.10464v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/6/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/6/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/triples/ablation-analysis.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/6/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/6/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/7/\n",
            "   creating: /content/train/Train v2/natural_language_inference/70/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/70/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/S16-1172-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/S16-1172-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/S16-1172.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/70/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/70/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/71/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/1706.02761v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/1706.02761v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/1706.02761v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/71/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/71/triples/\n",
            " extracting: /content/train/Train v2/natural_language_inference/71/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/71/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/72/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/72/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/N18-1140-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/N18-1140-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/N18-1140.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/72/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/72/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/73/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/1801.10296v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/1801.10296v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/1801.10296v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/73/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/73/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/triples/ablation-analysis.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/73/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/73/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/73/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/74/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/74/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/P18-1091-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/P18-1091-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/P18-1091.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/74/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/74/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/75/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/1606.03126v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/1606.03126v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/1606.03126v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/75/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/75/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/75/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/76/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/1509.06664v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/1509.06664v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/1509.06664v4.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/76/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/info-units/results.json  \n",
            " extracting: /content/train/Train v2/natural_language_inference/76/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/76/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/76/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/77/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/1703.04816v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/1703.04816v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/1703.04816v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/77/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/77/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/77/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/78/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/1801.00102v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/1801.00102v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/1801.00102v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/78/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/78/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/78/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/79/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/1405.4053v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/1405.4053v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/1405.4053v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/79/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/79/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/79/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/1611.01436v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/1611.01436v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/1611.01436v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/7/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/7/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/7/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/8/\n",
            "   creating: /content/train/Train v2/natural_language_inference/80/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/1802.05577v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/1802.05577v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/1802.05577v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/80/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/80/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/80/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/81/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/1901.00603v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/1901.00603v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/1901.00603v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/81/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/81/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/81/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/82/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/82/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/N16-1099-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/N16-1099-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/N16-1099.pdf  \n",
            " extracting: /content/train/Train v2/natural_language_inference/82/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/82/triples/\n",
            " extracting: /content/train/Train v2/natural_language_inference/82/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/82/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/82/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/83/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/D17-1168-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/D17-1168-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/D17-1168.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/83/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/83/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/83/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/84/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/1706.00286v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/1706.00286v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/1706.00286v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/84/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/84/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/84/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/85/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/1609.06038v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/1609.06038v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/1609.06038v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/85/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/85/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/85/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/86/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/1612.04211v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/1612.04211v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/1612.04211v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/86/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/86/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/86/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/87/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/1908.05147v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/1908.05147v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/1908.05147v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/87/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/87/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/87/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/88/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/1601.06733v7-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/1601.06733v7-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/1601.06733v7.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/88/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/88/triples/\n",
            " extracting: /content/train/Train v2/natural_language_inference/88/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/88/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/88/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/89/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/1808.05759v5-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/1808.05759v5-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/1808.05759v5.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/89/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/89/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/89/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/89/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/1412.1632v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/1412.1632v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/1412.1632v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/8/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/8/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/triples/model.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/8/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/8/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/9/\n",
            "   creating: /content/train/Train v2/natural_language_inference/90/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/1606.01933v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/1606.01933v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/1606.01933v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/90/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/90/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/90/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/91/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/1603.06021v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/1603.06021v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/1603.06021v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/91/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/91/triples/\n",
            " extracting: /content/train/Train v2/natural_language_inference/91/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/91/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/92/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/1909.04849v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/1909.04849v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/1909.04849v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/92/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/92/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/92/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/93/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/93/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/P17-1018-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/P17-1018-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/P17-1018.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/93/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/93/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/94/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/1809.06309v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/1809.06309v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/1809.06309v3.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/94/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/94/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/triples/ablation-analysis.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/94/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/94/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/95/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/1805.02220v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/1805.02220v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/1805.02220v2.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/95/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/95/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/95/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/96/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/1808.05326v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/1808.05326v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/1808.05326v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/96/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/96/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/96/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/97/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/1603.08884v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/1603.08884v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/1603.08884v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/97/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/97/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/97/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/98/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/1708.01353v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/1708.01353v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/1708.01353v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/98/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/98/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/triples/ablation-analysis.txt  \n",
            " extracting: /content/train/Train v2/natural_language_inference/98/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/98/triples/results.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/99/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/1710.02772v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/1710.02772v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/1710.02772v1.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/99/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/99/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/99/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/1606.04582v6-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/1606.04582v6-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/1606.04582v6.pdf  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/entities.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/9/info-units/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/sentences.txt  \n",
            "   creating: /content/train/Train v2/natural_language_inference/9/triples/\n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/natural_language_inference/9/triples/results.txt  \n",
            "   creating: /content/train/Train v2/negation_scope_resolution/\n",
            "   creating: /content/train/Train v2/negation_scope_resolution/0/\n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/1911.04211v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/1911.04211v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/1911.04211v3.pdf  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/entities.txt  \n",
            "   creating: /content/train/Train v2/negation_scope_resolution/0/info-units/\n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/negation_scope_resolution/0/triples/\n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/negation_scope_resolution/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/paraphrase_generation/\n",
            "   creating: /content/train/Train v2/paraphrase_generation/0/\n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/1806.00807v5-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/1806.00807v5-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/1806.00807v5.pdf  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/entities.txt  \n",
            "   creating: /content/train/Train v2/paraphrase_generation/0/info-units/\n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/paraphrase_generation/0/triples/\n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/0/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/paraphrase_generation/1/\n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/1709.05074v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/1709.05074v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/1709.05074v1.pdf  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/entities.txt  \n",
            "   creating: /content/train/Train v2/paraphrase_generation/1/info-units/\n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/paraphrase_generation/1/triples/\n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/paraphrase_generation/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/\n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/0/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/1711.04903v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/1711.04903v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/1711.04903v2.pdf  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/entities.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/0/info-units/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/0/triples/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/1/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/1810.12443v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/1810.12443v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/1810.12443v1.pdf  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/entities.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/1/info-units/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/1/triples/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/triples/model.txt  \n",
            " extracting: /content/train/Train v2/part-of-speech_tagging/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/2/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/1703.06345v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/1703.06345v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/1703.06345v1.pdf  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/entities.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/2/info-units/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/2/triples/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/triples/approach.txt  \n",
            " extracting: /content/train/Train v2/part-of-speech_tagging/2/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/2/triples/experiments.txt  \n",
            " extracting: /content/train/Train v2/part-of-speech_tagging/2/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/3/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/1603.01354v5-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/1603.01354v5-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/1603.01354v5.pdf  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/entities.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/3/info-units/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/3/triples/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/4/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/1908.08676v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/1908.08676v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/1908.08676v3.pdf  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/entities.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/4/info-units/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/sentences.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/4/triples/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/4/triples/results.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/5/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/1805.08237v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/1805.08237v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/1805.08237v1.pdf  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/entities.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/5/info-units/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/sentences.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/5/triples/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/triples/model.txt  \n",
            " extracting: /content/train/Train v2/part-of-speech_tagging/5/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/5/triples/results.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/6/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/1705.05952v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/1705.05952v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/1705.05952v2.pdf  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/entities.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/6/info-units/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/sentences.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/6/triples/\n",
            " extracting: /content/train/Train v2/part-of-speech_tagging/6/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/6/triples/results.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/7/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/1604.05529v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/1604.05529v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/1604.05529v3.pdf  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/entities.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/7/info-units/\n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/sentences.txt  \n",
            "   creating: /content/train/Train v2/part-of-speech_tagging/7/triples/\n",
            " extracting: /content/train/Train v2/part-of-speech_tagging/7/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/part-of-speech_tagging/7/triples/results.txt  \n",
            "   creating: /content/train/Train v2/passage_re-ranking/\n",
            "   creating: /content/train/Train v2/passage_re-ranking/0/\n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/1904.08375v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/1904.08375v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/1904.08375v2.pdf  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/entities.txt  \n",
            "   creating: /content/train/Train v2/passage_re-ranking/0/info-units/\n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/passage_re-ranking/0/triples/\n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/passage_re-ranking/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/passage_re-ranking/1/\n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/1901.04085v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/1901.04085v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/1901.04085v4.pdf  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/entities.txt  \n",
            "   creating: /content/train/Train v2/passage_re-ranking/1/info-units/\n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/passage_re-ranking/1/triples/\n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/passage_re-ranking/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/phrase_grounding/\n",
            "   creating: /content/train/Train v2/phrase_grounding/0/\n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/1811.11683v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/1811.11683v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/1811.11683v2.pdf  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/entities.txt  \n",
            "   creating: /content/train/Train v2/phrase_grounding/0/info-units/\n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/phrase_grounding/0/triples/\n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/phrase_grounding/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/prosody_prediction/\n",
            "   creating: /content/train/Train v2/prosody_prediction/0/\n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/1908.02262v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/1908.02262v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/1908.02262v1.pdf  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/entities.txt  \n",
            "   creating: /content/train/Train v2/prosody_prediction/0/info-units/\n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/prosody_prediction/0/triples/\n",
            " extracting: /content/train/Train v2/prosody_prediction/0/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/prosody_prediction/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/query_wellformedness/\n",
            "   creating: /content/train/Train v2/query_wellformedness/0/\n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/1808.09419v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/1808.09419v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/1808.09419v1.pdf  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/entities.txt  \n",
            "   creating: /content/train/Train v2/query_wellformedness/0/info-units/\n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/info-units/results.json  \n",
            " extracting: /content/train/Train v2/query_wellformedness/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/query_wellformedness/0/triples/\n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/query_wellformedness/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/question_answering/\n",
            "   creating: /content/train/Train v2/question_answering/0/\n",
            "  inflating: /content/train/Train v2/question_answering/0/C18-1280-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/0/C18-1280-Stanza-analysis-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/0/C18-1280-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/0/C18-1280.pdf  \n",
            "  inflating: /content/train/Train v2/question_answering/0/entities.txt  \n",
            "   creating: /content/train/Train v2/question_answering/0/info-units/\n",
            "  inflating: /content/train/Train v2/question_answering/0/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/question_answering/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/question_answering/0/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/question_answering/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/question_answering/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/question_answering/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_answering/0/triples/\n",
            "  inflating: /content/train/Train v2/question_answering/0/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/0/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/question_answering/1/\n",
            "  inflating: /content/train/Train v2/question_answering/1/1611.01603v6-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/1/1611.01603v6-Stanza-analysis-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/1/1611.01603v6-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/1/1611.01603v6.pdf  \n",
            "  inflating: /content/train/Train v2/question_answering/1/entities.txt  \n",
            "   creating: /content/train/Train v2/question_answering/1/info-units/\n",
            "  inflating: /content/train/Train v2/question_answering/1/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/question_answering/1/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/question_answering/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/question_answering/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/question_answering/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/question_answering/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_answering/1/triples/\n",
            "  inflating: /content/train/Train v2/question_answering/1/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/1/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/question_answering/2/\n",
            "  inflating: /content/train/Train v2/question_answering/2/1806.01873v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/2/1806.01873v2-Stanza-analysis-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/2/1806.01873v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/2/1806.01873v2.pdf  \n",
            "  inflating: /content/train/Train v2/question_answering/2/entities.txt  \n",
            "   creating: /content/train/Train v2/question_answering/2/info-units/\n",
            "  inflating: /content/train/Train v2/question_answering/2/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/question_answering/2/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/question_answering/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/question_answering/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_answering/2/triples/\n",
            "  inflating: /content/train/Train v2/question_answering/2/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/2/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/2/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/question_answering/3/\n",
            "  inflating: /content/train/Train v2/question_answering/3/D18-1238-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/3/D18-1238-Stanza-analysis-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/3/D18-1238-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/3/D18-1238.pdf  \n",
            "  inflating: /content/train/Train v2/question_answering/3/entities.txt  \n",
            "   creating: /content/train/Train v2/question_answering/3/info-units/\n",
            "  inflating: /content/train/Train v2/question_answering/3/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/question_answering/3/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/question_answering/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/question_answering/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/question_answering/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/question_answering/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_answering/3/triples/\n",
            "  inflating: /content/train/Train v2/question_answering/3/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/3/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/question_answering/4/\n",
            "  inflating: /content/train/Train v2/question_answering/4/1811.04210v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/4/1811.04210v2-Stanza-analysis-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/4/1811.04210v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/4/1811.04210v2.pdf  \n",
            "  inflating: /content/train/Train v2/question_answering/4/entities.txt  \n",
            "   creating: /content/train/Train v2/question_answering/4/info-units/\n",
            "  inflating: /content/train/Train v2/question_answering/4/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/question_answering/4/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/question_answering/4/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/question_answering/4/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/question_answering/4/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/question_answering/4/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/question_answering/4/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_answering/4/triples/\n",
            "  inflating: /content/train/Train v2/question_answering/4/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/4/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/4/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/4/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/4/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/4/triples/results.txt  \n",
            "   creating: /content/train/Train v2/question_answering/5/\n",
            "  inflating: /content/train/Train v2/question_answering/5/1711.05116v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/5/1711.05116v2-Stanza-analysis-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/5/1711.05116v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/5/1711.05116v2.pdf  \n",
            "  inflating: /content/train/Train v2/question_answering/5/entities.txt  \n",
            "   creating: /content/train/Train v2/question_answering/5/info-units/\n",
            "  inflating: /content/train/Train v2/question_answering/5/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/question_answering/5/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/question_answering/5/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/question_answering/5/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/question_answering/5/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/question_answering/5/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/question_answering/5/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_answering/5/triples/\n",
            "  inflating: /content/train/Train v2/question_answering/5/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/question_answering/5/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/5/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/5/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/5/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/question_answering/5/triples/results.txt  \n",
            "   creating: /content/train/Train v2/question_generation/\n",
            "   creating: /content/train/Train v2/question_generation/0/\n",
            "  inflating: /content/train/Train v2/question_generation/0/1704.01792v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/0/1704.01792v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/0/1704.01792v3.pdf  \n",
            "  inflating: /content/train/Train v2/question_generation/0/entities.txt  \n",
            "   creating: /content/train/Train v2/question_generation/0/info-units/\n",
            "  inflating: /content/train/Train v2/question_generation/0/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/question_generation/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/question_generation/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/question_generation/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/question_generation/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/question_generation/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_generation/0/triples/\n",
            "  inflating: /content/train/Train v2/question_generation/0/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/question_generation/1/\n",
            "  inflating: /content/train/Train v2/question_generation/1/1808.03986v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/1/1808.03986v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/1/1808.03986v2.pdf  \n",
            "  inflating: /content/train/Train v2/question_generation/1/entities.txt  \n",
            "   creating: /content/train/Train v2/question_generation/1/info-units/\n",
            "  inflating: /content/train/Train v2/question_generation/1/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/question_generation/1/info-units/research-problem.json  \n",
            " extracting: /content/train/Train v2/question_generation/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_generation/1/triples/\n",
            "  inflating: /content/train/Train v2/question_generation/1/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/question_generation/1/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/question_similarity/\n",
            "   creating: /content/train/Train v2/question_similarity/0/\n",
            "  inflating: /content/train/Train v2/question_similarity/0/1912.12514v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/1912.12514v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/1912.12514v1.pdf  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/entities.txt  \n",
            "   creating: /content/train/Train v2/question_similarity/0/info-units/\n",
            "  inflating: /content/train/Train v2/question_similarity/0/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/question_similarity/0/triples/\n",
            "  inflating: /content/train/Train v2/question_similarity/0/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/question_similarity/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/\n",
            "   creating: /content/train/Train v2/relation_extraction/0/\n",
            "  inflating: /content/train/Train v2/relation_extraction/0/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/0/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/P17-1085-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/P17-1085-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/P17-1085.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/0/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/1/\n",
            "   creating: /content/train/Train v2/relation_extraction/10/\n",
            "  inflating: /content/train/Train v2/relation_extraction/10/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/10/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/10/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/P19-1525-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/P19-1525-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/P19-1525.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/10/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/10/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/10/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/11/\n",
            "  inflating: /content/train/Train v2/relation_extraction/11/1812.04361v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/1812.04361v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/1812.04361v2.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/11/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/11/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/11/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/11/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/relation_extraction/11/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/11/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/12/\n",
            "  inflating: /content/train/Train v2/relation_extraction/12/1906.07510v5-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/1906.07510v5-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/1906.07510v5.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/12/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/12/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/12/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/12/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/relation_extraction/12/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/triples/model.txt  \n",
            " extracting: /content/train/Train v2/relation_extraction/12/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/12/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/13/\n",
            "  inflating: /content/train/Train v2/relation_extraction/13/1906.03158v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/13/1906.03158v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/13/1906.03158v1.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/13/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/13/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/13/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/13/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/13/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/13/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/13/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/13/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/13/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/13/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/1904.05255v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/1904.05255v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/1904.05255v1.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/1/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/info-units/results.json  \n",
            " extracting: /content/train/Train v2/relation_extraction/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/1/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/2/\n",
            "  inflating: /content/train/Train v2/relation_extraction/2/1905.08284v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/1905.08284v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/1905.08284v1.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/2/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/2/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/2/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/2/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/triples/model.txt  \n",
            " extracting: /content/train/Train v2/relation_extraction/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/3/\n",
            "  inflating: /content/train/Train v2/relation_extraction/3/1902.01030v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/1902.01030v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/1902.01030v2.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/3/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/3/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/3/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/3/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/4/\n",
            "  inflating: /content/train/Train v2/relation_extraction/4/D17-1004-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/D17-1004-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/D17-1004.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/4/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/4/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/4/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/4/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/4/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/5/\n",
            "  inflating: /content/train/Train v2/relation_extraction/5/1809.10185v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/1809.10185v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/1809.10185v1.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/5/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/5/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/5/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/5/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/triples/model.txt  \n",
            " extracting: /content/train/Train v2/relation_extraction/5/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/5/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/6/\n",
            "  inflating: /content/train/Train v2/relation_extraction/6/D17-1188-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/D17-1188-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/D17-1188.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/6/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/6/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/6/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/6/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/6/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/7/\n",
            "  inflating: /content/train/Train v2/relation_extraction/7/1808.06738v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/1808.06738v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/1808.06738v2.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/7/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/7/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/7/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/7/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/7/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/8/\n",
            "  inflating: /content/train/Train v2/relation_extraction/8/D15-1203-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/D15-1203-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/D15-1203.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/8/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/8/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/8/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/8/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/8/triples/results.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/9/\n",
            "  inflating: /content/train/Train v2/relation_extraction/9/entities.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/9/info-units/\n",
            "  inflating: /content/train/Train v2/relation_extraction/9/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/P16-1123-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/P16-1123-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/P16-1123.pdf  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/sentences.txt  \n",
            "   creating: /content/train/Train v2/relation_extraction/9/triples/\n",
            "  inflating: /content/train/Train v2/relation_extraction/9/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/triples/model.txt  \n",
            " extracting: /content/train/Train v2/relation_extraction/9/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/relation_extraction/9/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sarcasm_detection/\n",
            "   creating: /content/train/Train v2/sarcasm_detection/0/\n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/1704.05579v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/1704.05579v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/1704.05579v4.pdf  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/entities.txt  \n",
            "   creating: /content/train/Train v2/sarcasm_detection/0/info-units/\n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/info-units/results.json  \n",
            " extracting: /content/train/Train v2/sarcasm_detection/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/sarcasm_detection/0/triples/\n",
            " extracting: /content/train/Train v2/sarcasm_detection/0/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/triples/dataset.txt  \n",
            " extracting: /content/train/Train v2/sarcasm_detection/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sarcasm_detection/1/\n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/1805.06413v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/1805.06413v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/1805.06413v1.pdf  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/entities.txt  \n",
            "   creating: /content/train/Train v2/sarcasm_detection/1/info-units/\n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/sarcasm_detection/1/triples/\n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sarcasm_detection/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/\n",
            "   creating: /content/train/Train v2/semantic_parsing/0/\n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/1809.08887v5-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/1809.08887v5-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/1809.08887v5.pdf  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/entities.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/0/info-units/\n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/0/triples/\n",
            " extracting: /content/train/Train v2/semantic_parsing/0/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/1/\n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/1810.02720v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/1810.02720v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/1810.02720v1.pdf  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/entities.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/1/info-units/\n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/1/triples/\n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/2/\n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/1805.04793v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/1805.04793v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/1805.04793v1.pdf  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/entities.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/2/info-units/\n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/semantic_parsing/2/triples/\n",
            " extracting: /content/train/Train v2/semantic_parsing/2/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/semantic_parsing/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/\n",
            "   creating: /content/train/Train v2/semantic_role_labeling/0/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/entities.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/0/info-units/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/P18-2058-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/P18-2058-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/P18-2058.pdf  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/0/triples/\n",
            " extracting: /content/train/Train v2/semantic_role_labeling/0/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/1/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/1804.08199v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/1804.08199v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/1804.08199v3.pdf  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/entities.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/1/info-units/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/1/triples/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/2/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/entities.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/2/info-units/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/P17-1044-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/P17-1044-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/P17-1044.pdf  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/2/triples/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/3/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/1712.01586v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/1712.01586v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/1712.01586v1.pdf  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/entities.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/3/info-units/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/3/triples/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/4/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/1810.02245v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/1810.02245v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/1810.02245v1.pdf  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/entities.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/4/info-units/\n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/sentences.txt  \n",
            "   creating: /content/train/Train v2/semantic_role_labeling/4/triples/\n",
            " extracting: /content/train/Train v2/semantic_role_labeling/4/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/semantic_role_labeling/4/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/\n",
            "   creating: /content/train/Train v2/sentence_classification/0/\n",
            "  inflating: /content/train/Train v2/sentence_classification/0/1904.01608v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/1904.01608v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/1904.01608v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/entities.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/0/info-units/\n",
            "  inflating: /content/train/Train v2/sentence_classification/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/0/triples/\n",
            "  inflating: /content/train/Train v2/sentence_classification/0/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/sentence_classification/0/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/1/\n",
            "  inflating: /content/train/Train v2/sentence_classification/1/1808.06161v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/1808.06161v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/1808.06161v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/entities.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/1/info-units/\n",
            "  inflating: /content/train/Train v2/sentence_classification/1/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/info-units/models.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/1/triples/\n",
            "  inflating: /content/train/Train v2/sentence_classification/1/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/triples/models.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/1/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/2/\n",
            "  inflating: /content/train/Train v2/sentence_classification/2/1806.05516v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/1806.05516v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/1806.05516v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/entities.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/2/info-units/\n",
            "  inflating: /content/train/Train v2/sentence_classification/2/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentence_classification/2/triples/\n",
            "  inflating: /content/train/Train v2/sentence_classification/2/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/triples/hyperparameters.txt  \n",
            " extracting: /content/train/Train v2/sentence_classification/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentence_classification/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/\n",
            "   creating: /content/train/Train v2/sentence_compression/0/\n",
            "  inflating: /content/train/Train v2/sentence_compression/0/entities.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/0/info-units/\n",
            "  inflating: /content/train/Train v2/sentence_compression/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/P17-1127-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/P17-1127-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/P17-1127.pdf  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/0/triples/\n",
            "  inflating: /content/train/Train v2/sentence_compression/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/triples/model.txt  \n",
            " extracting: /content/train/Train v2/sentence_compression/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/1/\n",
            "  inflating: /content/train/Train v2/sentence_compression/1/D15-1042-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/1/D15-1042-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/1/D15-1042.pdf  \n",
            "  inflating: /content/train/Train v2/sentence_compression/1/entities.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/1/info-units/\n",
            "  inflating: /content/train/Train v2/sentence_compression/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/1/info-units/results.json  \n",
            " extracting: /content/train/Train v2/sentence_compression/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/1/triples/\n",
            "  inflating: /content/train/Train v2/sentence_compression/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/2/\n",
            "  inflating: /content/train/Train v2/sentence_compression/2/1604.03357v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/1604.03357v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/1604.03357v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/entities.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/2/info-units/\n",
            "  inflating: /content/train/Train v2/sentence_compression/2/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/2/triples/\n",
            "  inflating: /content/train/Train v2/sentence_compression/2/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/triples/model.txt  \n",
            " extracting: /content/train/Train v2/sentence_compression/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/3/\n",
            "  inflating: /content/train/Train v2/sentence_compression/3/entities.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/3/info-units/\n",
            "  inflating: /content/train/Train v2/sentence_compression/3/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/P18-2028-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/P18-2028-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/P18-2028.pdf  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentence_compression/3/triples/\n",
            "  inflating: /content/train/Train v2/sentence_compression/3/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/sentence_compression/3/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentence_compression/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/\n",
            "   creating: /content/train/Train v2/sentiment_analysis/0/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/1810.04635v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/1810.04635v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/1810.04635v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/0/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/0/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/triples/model.txt  \n",
            " extracting: /content/train/Train v2/sentiment_analysis/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/1/\n",
            "   creating: /content/train/Train v2/sentiment_analysis/10/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/1811.00405v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/1811.00405v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/1811.00405v4.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/10/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/10/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/10/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/11/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/11/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/N18-1193-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/N18-1193-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/N18-1193.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/11/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/11/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/12/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/1906.01213v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/1906.01213v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/1906.01213v3.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/12/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/12/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/12/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/13/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/1904.02232v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/1904.02232v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/1904.02232v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/13/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/13/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/13/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/14/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/1809.04505v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/1809.04505v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/1809.04505v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/14/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/14/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/14/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/15/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/D13-1170-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/D13-1170-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/D13-1170.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/15/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/15/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/15/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/16/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/16/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/P18-1088-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/P18-1088-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/P18-1088.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/16/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/16/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/17/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/1503.00075v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/1503.00075v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/1503.00075v3.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/17/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/17/triples/\n",
            " extracting: /content/train/Train v2/sentiment_analysis/17/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/17/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/18/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/C18-1096-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/C18-1096-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/C18-1096.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/18/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/18/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/18/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/19/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/1805.07340v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/1805.07340v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/1805.07340v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/19/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/19/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/19/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/1907.07835v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/1907.07835v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/1907.07835v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/1/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/1/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/2/\n",
            "   creating: /content/train/Train v2/sentiment_analysis/20/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/20/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/S17-2126-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/S17-2126-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/S17-2126.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/20/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/20/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/21/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/21/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/P19-2057-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/P19-2057-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/P19-2057.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/21/triples/\n",
            " extracting: /content/train/Train v2/sentiment_analysis/21/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/21/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/22/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/22/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/K18-1018-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/K18-1018-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/K18-1018.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/22/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/sentiment_analysis/22/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/22/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/23/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/1504.01106v5-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/1504.01106v5-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/1504.01106v5.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/23/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/23/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/23/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/24/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/1906.06906v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/1906.06906v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/1906.06906v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/24/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/24/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/24/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/25/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/1805.07043v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/1805.07043v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/1805.07043v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/25/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/25/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/25/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/26/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/26/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/P18-1235-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/P18-1235-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/P18-1235.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/26/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/26/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/27/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/1906.04501v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/1906.04501v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/1906.04501v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/27/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/27/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/27/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/28/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/1902.09314v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/1902.09314v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/1902.09314v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/28/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/28/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/28/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/29/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/1804.06536v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/1804.06536v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/1804.06536v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/29/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/29/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/29/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/C18-1066-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/C18-1066-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/C18-1066.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/2/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/2/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/3/\n",
            "   creating: /content/train/Train v2/sentiment_analysis/30/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/30/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/N18-2045-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/N18-2045-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/N18-2045.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/30/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/30/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/31/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/D18-1136-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/D18-1136-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/D18-1136.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/31/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/31/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/31/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/32/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/1709.00893v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/1709.00893v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/1709.00893v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/32/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/32/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/32/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/33/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/1808.09315v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/1808.09315v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/1808.09315v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/33/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/33/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/33/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/34/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/34/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/34/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/W19-4621-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/W19-4621-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/34/W19-4621.pdf  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/35/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/1811.10999-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/1811.10999-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/1811.10999.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/35/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/35/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/35/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/36/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/1805.01086v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/1805.01086v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/1805.01086v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/36/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/info-units/models.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/36/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/triples/models.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/36/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/37/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/D18-1377-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/D18-1377-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/D18-1377.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/37/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/37/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/37/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/38/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/1704.01444v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/1704.01444v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/1704.01444v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/38/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/38/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/38/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/39/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/1610.03771v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/1610.03771v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/1610.03771v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/39/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/info-units/dataset.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/39/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/triples/dataset.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/39/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/1909.10681v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/1909.10681v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/1909.10681v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/3/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/3/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/4/\n",
            "   creating: /content/train/Train v2/sentiment_analysis/40/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/D17-1047-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/D17-1047-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/D17-1047.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/40/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/40/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/triples/model.txt  \n",
            " extracting: /content/train/Train v2/sentiment_analysis/40/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/40/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/41/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/D16-1058-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/D16-1058-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/D16-1058.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/41/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/41/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/41/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/42/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/1605.08900v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/1605.08900v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/1605.08900v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/42/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/42/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/42/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/43/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/D18-1380-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/D18-1380-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/D18-1380.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/43/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/43/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/43/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/44/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/44/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/44/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/W17-5535-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/W17-5535-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/44/W17-5535.pdf  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/45/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/1903.09588v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/1903.09588v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/1903.09588v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/45/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/45/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/45/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/46/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/1807.04990v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/1807.04990v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/1807.04990v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/46/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/46/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/triples/model.txt  \n",
            " extracting: /content/train/Train v2/sentiment_analysis/46/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/46/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/47/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/1802.00892v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/1802.00892v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/1802.00892v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/47/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/47/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/47/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/48/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/1810.10437v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/1810.10437v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/1810.10437v3.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/48/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/48/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/48/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/49/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/D18-1382-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/D18-1382-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/D18-1382.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/49/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/49/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/49/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/D18-1280-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/D18-1280-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/D18-1280.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/4/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/4/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/4/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/5/\n",
            "   creating: /content/train/Train v2/sentiment_analysis/50/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/1806.04346v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/1806.04346v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/1806.04346v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/50/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/50/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/50/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/51/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/1910.03474v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/1910.03474v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/1910.03474v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/51/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/51/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/51/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/1906.01704-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/1906.01704-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/1906.01704.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/5/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/5/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/5/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/6/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/6/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/P17-1081-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/P17-1081-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/P17-1081.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/6/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/6/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/7/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/1804.05788v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/1804.05788v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/1804.05788v3.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/7/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/info-units/results.json  \n",
            " extracting: /content/train/Train v2/sentiment_analysis/7/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/7/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/7/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/8/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/1904.06022v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/1904.06022v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/1904.06022v1.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/8/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/8/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/8/triples/results.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/9/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/1912.07976v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/1912.07976v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/1912.07976v2.pdf  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/entities.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/9/info-units/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/sentences.txt  \n",
            "   creating: /content/train/Train v2/sentiment_analysis/9/triples/\n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/sentiment_analysis/9/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/sentiment_analysis/9/triples/results.txt  \n",
            "   creating: /content/train/Train v2/smile_recognition/\n",
            "   creating: /content/train/Train v2/smile_recognition/0/\n",
            "  inflating: /content/train/Train v2/smile_recognition/0/1602.00172v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/smile_recognition/0/1602.00172v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/smile_recognition/0/1602.00172v2.pdf  \n",
            "  inflating: /content/train/Train v2/smile_recognition/0/entities.txt  \n",
            "   creating: /content/train/Train v2/smile_recognition/0/info-units/\n",
            "  inflating: /content/train/Train v2/smile_recognition/0/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/smile_recognition/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/smile_recognition/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/smile_recognition/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/smile_recognition/0/triples/\n",
            "  inflating: /content/train/Train v2/smile_recognition/0/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/smile_recognition/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/smile_recognition/0/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/temporal_information_extraction/\n",
            "   creating: /content/train/Train v2/temporal_information_extraction/0/\n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/C16-1007-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/C16-1007-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/C16-1007.pdf  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/entities.txt  \n",
            "   creating: /content/train/Train v2/temporal_information_extraction/0/info-units/\n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/temporal_information_extraction/0/triples/\n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/temporal_information_extraction/1/\n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/D17-1108-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/D17-1108-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/D17-1108.pdf  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/entities.txt  \n",
            "   creating: /content/train/Train v2/temporal_information_extraction/1/info-units/\n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/temporal_information_extraction/1/triples/\n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/temporal_information_extraction/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/\n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/0/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/1904.03446v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/1904.03446v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/1904.03446v3.pdf  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/entities.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/0/info-units/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/info-units/experiments.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/0/triples/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/triples/experiments.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/1/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/1905.09263v5-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/1905.09263v5-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/1905.09263v5.pdf  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/entities.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/1/info-units/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/1/triples/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/2/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/1806.04558v4-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/1806.04558v4-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/1806.04558v4.pdf  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/entities.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/2/info-units/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/text-to-speech_synthesis/2/triples/\n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text-to-speech_synthesis/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_generation/\n",
            "   creating: /content/train/Train v2/text_generation/0/\n",
            "  inflating: /content/train/Train v2/text_generation/0/1609.05473v6-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/0/1609.05473v6-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/0/1609.05473v6.pdf  \n",
            "  inflating: /content/train/Train v2/text_generation/0/entities.txt  \n",
            "   creating: /content/train/Train v2/text_generation/0/info-units/\n",
            "  inflating: /content/train/Train v2/text_generation/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_generation/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/text_generation/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_generation/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_generation/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_generation/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_generation/0/triples/\n",
            "  inflating: /content/train/Train v2/text_generation/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_generation/1/\n",
            "  inflating: /content/train/Train v2/text_generation/1/1705.11001v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/1/1705.11001v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/1/1705.11001v3.pdf  \n",
            "  inflating: /content/train/Train v2/text_generation/1/entities.txt  \n",
            "   creating: /content/train/Train v2/text_generation/1/info-units/\n",
            "  inflating: /content/train/Train v2/text_generation/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_generation/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_generation/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_generation/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_generation/1/triples/\n",
            "  inflating: /content/train/Train v2/text_generation/1/triples/model.txt  \n",
            " extracting: /content/train/Train v2/text_generation/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_generation/2/\n",
            "  inflating: /content/train/Train v2/text_generation/2/1709.08624v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/2/1709.08624v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/2/1709.08624v2.pdf  \n",
            "  inflating: /content/train/Train v2/text_generation/2/entities.txt  \n",
            "   creating: /content/train/Train v2/text_generation/2/info-units/\n",
            "  inflating: /content/train/Train v2/text_generation/2/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/text_generation/2/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/text_generation/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_generation/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_generation/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_generation/2/triples/\n",
            "  inflating: /content/train/Train v2/text_generation/2/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/2/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_generation/3/\n",
            "  inflating: /content/train/Train v2/text_generation/3/1808.08795v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/3/1808.08795v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/3/1808.08795v1.pdf  \n",
            "  inflating: /content/train/Train v2/text_generation/3/entities.txt  \n",
            "   creating: /content/train/Train v2/text_generation/3/info-units/\n",
            "  inflating: /content/train/Train v2/text_generation/3/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/text_generation/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_generation/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_generation/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_generation/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_generation/3/triples/\n",
            "  inflating: /content/train/Train v2/text_generation/3/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_generation/4/\n",
            "  inflating: /content/train/Train v2/text_generation/4/1808.08703v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/4/1808.08703v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/4/1808.08703v2.pdf  \n",
            "  inflating: /content/train/Train v2/text_generation/4/entities.txt  \n",
            "   creating: /content/train/Train v2/text_generation/4/info-units/\n",
            "  inflating: /content/train/Train v2/text_generation/4/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/text_generation/4/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/text_generation/4/info-units/research-problem.json  \n",
            " extracting: /content/train/Train v2/text_generation/4/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_generation/4/triples/\n",
            "  inflating: /content/train/Train v2/text_generation/4/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/4/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/4/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/text_generation/5/\n",
            "  inflating: /content/train/Train v2/text_generation/5/1702.08139v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/5/1702.08139v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/5/1702.08139v2.pdf  \n",
            "  inflating: /content/train/Train v2/text_generation/5/entities.txt  \n",
            "   creating: /content/train/Train v2/text_generation/5/info-units/\n",
            "  inflating: /content/train/Train v2/text_generation/5/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/text_generation/5/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/text_generation/5/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_generation/5/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_generation/5/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_generation/5/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_generation/5/triples/\n",
            "  inflating: /content/train/Train v2/text_generation/5/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/5/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/5/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/5/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_generation/5/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/\n",
            "   creating: /content/train/Train v2/text_summarization/0/\n",
            "  inflating: /content/train/Train v2/text_summarization/0/1812.05407v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/1812.05407v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/1812.05407v1.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/0/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/0/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/0/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/0/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/0/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/1/\n",
            "   creating: /content/train/Train v2/text_summarization/10/\n",
            "  inflating: /content/train/Train v2/text_summarization/10/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/10/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/10/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/P18-1064-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/P18-1064-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/P18-1064.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/10/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/10/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/10/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/11/\n",
            "  inflating: /content/train/Train v2/text_summarization/11/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/11/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/11/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/P18-2027-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/P18-2027-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/P18-2027.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/11/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/11/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/11/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/12/\n",
            "  inflating: /content/train/Train v2/text_summarization/12/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/12/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/12/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/P17-1101-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/P17-1101-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/P17-1101.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/12/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/12/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/12/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/13/\n",
            "  inflating: /content/train/Train v2/text_summarization/13/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/13/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/13/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/13/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/13/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/triples/experimental-setup.txt  \n",
            " extracting: /content/train/Train v2/text_summarization/13/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/W17-4505-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/W17-4505-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/13/W17-4505.pdf  \n",
            "   creating: /content/train/Train v2/text_summarization/14/\n",
            "  inflating: /content/train/Train v2/text_summarization/14/C18-1121-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/C18-1121-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/C18-1121.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/14/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/14/info-units/ablation-analysis.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/14/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/14/triples/ablation-analysis.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/14/triples/results.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/1909.01953v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/1909.01953v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/1909.01953v1.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/1/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/1/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/1/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/1/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/1/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/2/\n",
            "  inflating: /content/train/Train v2/text_summarization/2/C18-1146-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/2/C18-1146-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/2/C18-1146.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/2/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/2/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/2/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/2/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/2/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/2/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/2/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/2/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/2/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/2/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/3/\n",
            "  inflating: /content/train/Train v2/text_summarization/3/1910.08486v1-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/1910.08486v1-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/1910.08486v1.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/3/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/3/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/3/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/3/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/text_summarization/3/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/3/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/4/\n",
            "  inflating: /content/train/Train v2/text_summarization/4/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/4/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/4/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/N18-1064-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/N18-1064-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/N18-1064.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/4/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/4/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/4/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/5/\n",
            "  inflating: /content/train/Train v2/text_summarization/5/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/5/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/5/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/info-units/code.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/P18-1015-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/P18-1015-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/P18-1015.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/5/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/5/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/triples/baselines.txt  \n",
            " extracting: /content/train/Train v2/text_summarization/5/triples/code.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/5/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/6/\n",
            "  inflating: /content/train/Train v2/text_summarization/6/D17-1222-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/D17-1222-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/D17-1222.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/6/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/6/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/6/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/6/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/6/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/7/\n",
            "  inflating: /content/train/Train v2/text_summarization/7/E17-2047-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/7/E17-2047-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/7/E17-2047.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/7/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/7/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/7/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/7/info-units/research-problem.json  \n",
            " extracting: /content/train/Train v2/text_summarization/7/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/7/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/7/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/7/triples/research-problem.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/8/\n",
            "  inflating: /content/train/Train v2/text_summarization/8/1808.10792v2-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/1808.10792v2-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/1808.10792v2.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/8/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/8/info-units/approach.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/8/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/8/triples/approach.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/8/triples/results.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/9/\n",
            "  inflating: /content/train/Train v2/text_summarization/9/entities.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/9/info-units/\n",
            "  inflating: /content/train/Train v2/text_summarization/9/info-units/experimental-setup.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/N16-1012-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/N16-1012-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/N16-1012.pdf  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/sentences.txt  \n",
            "   creating: /content/train/Train v2/text_summarization/9/triples/\n",
            "  inflating: /content/train/Train v2/text_summarization/9/triples/experimental-setup.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/text_summarization/9/triples/results.txt  \n",
            "   creating: /content/train/Train v2/topic_models/\n",
            "   creating: /content/train/Train v2/topic_models/0/\n",
            "  inflating: /content/train/Train v2/topic_models/0/1908.07599v3-Grobid-out.txt  \n",
            "  inflating: /content/train/Train v2/topic_models/0/1908.07599v3-Stanza-out.txt  \n",
            "  inflating: /content/train/Train v2/topic_models/0/1908.07599v3.pdf  \n",
            "  inflating: /content/train/Train v2/topic_models/0/entities.txt  \n",
            "   creating: /content/train/Train v2/topic_models/0/info-units/\n",
            "  inflating: /content/train/Train v2/topic_models/0/info-units/baselines.json  \n",
            "  inflating: /content/train/Train v2/topic_models/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/train/Train v2/topic_models/0/info-units/model.json  \n",
            "  inflating: /content/train/Train v2/topic_models/0/info-units/research-problem.json  \n",
            "  inflating: /content/train/Train v2/topic_models/0/info-units/results.json  \n",
            "  inflating: /content/train/Train v2/topic_models/0/sentences.txt  \n",
            "   creating: /content/train/Train v2/topic_models/0/triples/\n",
            "  inflating: /content/train/Train v2/topic_models/0/triples/baselines.txt  \n",
            "  inflating: /content/train/Train v2/topic_models/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/train/Train v2/topic_models/0/triples/model.txt  \n",
            "  inflating: /content/train/Train v2/topic_models/0/triples/research-problem.txt  \n",
            "  inflating: /content/train/Train v2/topic_models/0/triples/results.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXY4Lsd9S5rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09da957-7cc8-492d-c444-5c36ca3514fc"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/Sem Eval Task 11 Group 7/Dev v2.zip\" -d \"/content/valid\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Sem Eval Task 11 Group 7/Dev v2.zip\n",
            "   creating: /content/valid/Dev v2/\n",
            "   creating: /content/valid/Dev v2/machine-translation/\n",
            "   creating: /content/valid/Dev v2/machine-translation/0/\n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/1606.04199v3-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/1606.04199v3-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/1606.04199v3.pdf  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/entities.txt  \n",
            "   creating: /content/valid/Dev v2/machine-translation/0/info-units/\n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/info-units/experimental-setup.json  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/info-units/model.json  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/machine-translation/0/triples/\n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/triples/experimental-setup.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/triples/model.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/0/triples/results.txt  \n",
            "   creating: /content/valid/Dev v2/machine-translation/1/\n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/1809.06858v1-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/1809.06858v1-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/1809.06858v1.pdf  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/entities.txt  \n",
            "   creating: /content/valid/Dev v2/machine-translation/1/info-units/\n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/info-units/approach.json  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/info-units/experiments.json  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/machine-translation/1/triples/\n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/triples/approach.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/triples/experiments.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/machine-translation/1/triples/results.txt  \n",
            "   creating: /content/valid/Dev v2/named-entity-recognition/\n",
            "   creating: /content/valid/Dev v2/named-entity-recognition/0/\n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/1702.02098v3-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/1702.02098v3-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/1702.02098v3.pdf  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/entities.txt  \n",
            "   creating: /content/valid/Dev v2/named-entity-recognition/0/info-units/\n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/info-units/approach.json  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/info-units/baselines.json  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/named-entity-recognition/0/triples/\n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/triples/approach.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/triples/baselines.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/0/triples/results.txt  \n",
            "   creating: /content/valid/Dev v2/named-entity-recognition/1/\n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/1802.05365v2-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/1802.05365v2-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/1802.05365v2.pdf  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/entities.txt  \n",
            "   creating: /content/valid/Dev v2/named-entity-recognition/1/info-units/\n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/info-units/approach.json  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/info-units/tasks.json  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/named-entity-recognition/1/triples/\n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/triples/approach.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/named-entity-recognition/1/triples/tasks.txt  \n",
            "   creating: /content/valid/Dev v2/question-answering/\n",
            "   creating: /content/valid/Dev v2/question-answering/0/\n",
            "  inflating: /content/valid/Dev v2/question-answering/0/1503.03244v1-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/1503.03244v1-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/1503.03244v1.pdf  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/entities.txt  \n",
            "   creating: /content/valid/Dev v2/question-answering/0/info-units/\n",
            "  inflating: /content/valid/Dev v2/question-answering/0/info-units/baselines.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/info-units/hyperparameters.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/info-units/model.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/question-answering/0/triples/\n",
            "  inflating: /content/valid/Dev v2/question-answering/0/triples/baselines.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/triples/hyperparameters.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/triples/model.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/0/triples/results.txt  \n",
            "   creating: /content/valid/Dev v2/question-answering/1/\n",
            "  inflating: /content/valid/Dev v2/question-answering/1/1608.07905v2-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/1608.07905v2-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/1608.07905v2.pdf  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/entities.txt  \n",
            "   creating: /content/valid/Dev v2/question-answering/1/info-units/\n",
            "  inflating: /content/valid/Dev v2/question-answering/1/info-units/hyperparameters.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/info-units/model.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/question-answering/1/triples/\n",
            "  inflating: /content/valid/Dev v2/question-answering/1/triples/hyperparameters.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/triples/model.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/question-answering/1/triples/results.txt  \n",
            "   creating: /content/valid/Dev v2/relation-classification/\n",
            "   creating: /content/valid/Dev v2/relation-classification/0/\n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/1808.06876v3-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/1808.06876v3-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/1808.06876v3.pdf  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/entities.txt  \n",
            "   creating: /content/valid/Dev v2/relation-classification/0/info-units/\n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/info-units/experimental-setup.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/info-units/model.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/relation-classification/0/triples/\n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/triples/experimental-setup.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/triples/model.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/0/triples/results.txt  \n",
            "   creating: /content/valid/Dev v2/relation-classification/1/\n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/1903.10676v3-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/1903.10676v3-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/1903.10676v3.pdf  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/entities.txt  \n",
            "   creating: /content/valid/Dev v2/relation-classification/1/info-units/\n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/info-units/approach.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/info-units/code.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/info-units/tasks.json  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/relation-classification/1/triples/\n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/triples/approach.txt  \n",
            " extracting: /content/valid/Dev v2/relation-classification/1/triples/code.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/triples/results.txt  \n",
            "  inflating: /content/valid/Dev v2/relation-classification/1/triples/tasks.txt  \n",
            "   creating: /content/valid/Dev v2/text-classification/\n",
            "   creating: /content/valid/Dev v2/text-classification/0/\n",
            "  inflating: /content/valid/Dev v2/text-classification/0/1707.01780v3-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/1707.01780v3-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/1707.01780v3.pdf  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/entities.txt  \n",
            "   creating: /content/valid/Dev v2/text-classification/0/info-units/\n",
            "  inflating: /content/valid/Dev v2/text-classification/0/info-units/approach.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/info-units/code.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/info-units/experimental-setup.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/text-classification/0/triples/\n",
            "  inflating: /content/valid/Dev v2/text-classification/0/triples/approach.txt  \n",
            " extracting: /content/valid/Dev v2/text-classification/0/triples/code.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/triples/experimental-setup.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/0/triples/results.txt  \n",
            "   creating: /content/valid/Dev v2/text-classification/1/\n",
            "  inflating: /content/valid/Dev v2/text-classification/1/1803.11175v2-Grobid-out.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/1803.11175v2-Stanza-out.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/1803.11175v2.pdf  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/entities.txt  \n",
            "   creating: /content/valid/Dev v2/text-classification/1/info-units/\n",
            "  inflating: /content/valid/Dev v2/text-classification/1/info-units/baselines.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/info-units/code.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/info-units/model.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/info-units/research-problem.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/info-units/results.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/info-units/tasks.json  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/sentences.txt  \n",
            "   creating: /content/valid/Dev v2/text-classification/1/triples/\n",
            "  inflating: /content/valid/Dev v2/text-classification/1/triples/baselines.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/triples/code.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/triples/model.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/triples/research-problem.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/triples/results.txt  \n",
            "  inflating: /content/valid/Dev v2/text-classification/1/triples/tasks.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BGgoMLPzYxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89237c10-f02a-413b-8bd0-3e14ddb1c72e"
      },
      "source": [
        "# Training dataset reading\n",
        "input_dir = \"/content/train/Train v2/\"\n",
        "import os\n",
        "import torch\n",
        "list_of_folders = [\"query_wellformedness\", \"passage_re-ranking\", \"part-of-speech_tagging\", \n",
        "         \"sentence_compression\", \"sentiment_analysis\", \"temporal_information_extraction\", \n",
        "         \"phrase_grounding\", \"text_generation\", \"text-to-speech_synthesis\", \n",
        "         \"smile_recognition\", \"topic_models\", \"question_generation\", \n",
        "         \"relation_extraction\", \"paraphrase_generation\", \"question_similarity\", \n",
        "         \"question_answering\", \"sentence_classification\", \"prosody_prediction\", \n",
        "         \"semantic_role_labeling\", \"text_summarization\", \"semantic_parsing\", \n",
        "         \"sarcasm_detection\", \"natural_language_inference\", \"negation_scope_resolution\"]\n",
        "input_stanza_list = []\n",
        "input_sent_num_list = []\n",
        "input_entity_list = []\n",
        "file_name_list = []\n",
        "total_phrases_truth = 0\n",
        "for fls in list_of_folders:\n",
        "  count=0\n",
        "  for i in os.listdir(input_dir + fls + '/'):\n",
        "    count=count+1\n",
        "    for files in os.listdir(input_dir + fls + '/' + str(i)):\n",
        "      if files.endswith(\"Stanza-out.txt\"):\n",
        "        stanza_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        stanza_lines = (stanza_file.read()).lower()\n",
        "        stanza_lines_list = list(filter(None,stanza_lines.splitlines())) # filter empty strings and split into lines\n",
        "        input_stanza_list.append(stanza_lines_list)\n",
        "      if files.endswith(\"sentences.txt\"):\n",
        "        sentence_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        sentence_num_list = list(filter(None,(sentence_file.read().lower()).splitlines())) # filter empty strings and split into lines\n",
        "        input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
        "      if files.endswith(\"entities.txt\"):\n",
        "        entities_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        entities_list = list(filter(None,(entities_file.read().lower()).splitlines())) # filter empty strings and split into lines\n",
        "        input_entity_list.append(entities_list)\n",
        "        total_phrases_truth = total_phrases_truth + len(entities_list)\n",
        "    file_name_list.append(fls + '/' + str(i))\n",
        "  print(\"completed\",fls,total_phrases_truth, count)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed query_wellformedness 55 1\n",
            "completed passage_re-ranking 270 2\n",
            "completed part-of-speech_tagging 1084 8\n",
            "completed sentence_compression 1513 4\n",
            "completed sentiment_analysis 9006 52\n",
            "completed temporal_information_extraction 9161 2\n",
            "completed phrase_grounding 9334 1\n",
            "completed text_generation 10049 6\n",
            "completed text-to-speech_synthesis 10375 3\n",
            "completed smile_recognition 10461 1\n",
            "completed topic_models 10535 1\n",
            "completed question_generation 10668 2\n",
            "completed relation_extraction 12596 14\n",
            "completed paraphrase_generation 12891 2\n",
            "completed question_similarity 12975 1\n",
            "completed question_answering 14045 6\n",
            "completed sentence_classification 14554 3\n",
            "completed prosody_prediction 14729 1\n",
            "completed semantic_role_labeling 15275 5\n",
            "completed text_summarization 17059 15\n",
            "completed semantic_parsing 17337 3\n",
            "completed sarcasm_detection 17560 2\n",
            "completed natural_language_inference 30291 101\n",
            "completed negation_scope_resolution 30445 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkJpU55b5lw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a4c5f5-ae5a-4c37-a908-44e8be55becd"
      },
      "source": [
        "# Validation dataset reading\n",
        "val_input_dir = \"/content/valid/Dev v2/\"\n",
        "val_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
        "         \"relation-classification\", \"text-classification\"]\n",
        "val_input_stanza_list = []\n",
        "val_input_sent_num_list = []\n",
        "val_input_entity_list = []\n",
        "val_file_name_list = []\n",
        "val_total_phrases_truth = 0\n",
        "for fls in val_list_of_folders:\n",
        "  count=0\n",
        "  for i in os.listdir(val_input_dir + fls + '/'):\n",
        "    count=count+1\n",
        "    for files in os.listdir(val_input_dir + fls + '/' + str(i)):\n",
        "      if files.endswith(\"Stanza-out.txt\"):\n",
        "        stanza_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        stanza_lines = stanza_file.read().lower()\n",
        "        stanza_lines_list = list(filter(None,stanza_lines.splitlines())) # filter empty strings and split into lines\n",
        "        val_input_stanza_list.append(stanza_lines_list)\n",
        "      if files.endswith(\"sentences.txt\"):\n",
        "        sentence_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        sentence_num_list = list(filter(None,(sentence_file.read().lower()).splitlines())) # filter empty strings and split into lines\n",
        "        val_input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
        "      if files.endswith(\"entities.txt\"):\n",
        "        entities_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        entities_list = list(filter(None,(entities_file.read().lower()).splitlines())) # filter empty strings and split into lines\n",
        "        val_input_entity_list.append(entities_list)\n",
        "        val_total_phrases_truth = val_total_phrases_truth + len(entities_list)\n",
        "    val_file_name_list.append(fls + '/' + str(i))\n",
        "  print(\"completed\",fls,val_total_phrases_truth,count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed machine-translation 915 10\n",
            "completed named-entity-recognition 1707 10\n",
            "completed question-answering 2738 10\n",
            "completed relation-classification 3728 10\n",
            "completed text-classification 4777 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf8Tvo_h7MLX"
      },
      "source": [
        "def make_substring(n): # replace phrases with labels\n",
        "  if n==0:\n",
        "    return ''\n",
        "  elif n ==1:\n",
        "    return 'U'\n",
        "  elif n==2:\n",
        "    return 'B L'\n",
        "  else:\n",
        "    t1 = 'I '*(n-2)\n",
        "    return 'B '+t1+'L'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZx-f4yZPugG"
      },
      "source": [
        "# train function\n",
        "import copy\n",
        "task2_in = [] #input sentences of examples\n",
        "task2_label = []# replaced the input by labels for phrases\n",
        "for i in range(len(input_stanza_list)): #process each file for labels\n",
        "  entity_list = [j.split('\\t') for j in input_entity_list[i]] # split the entities line\n",
        "  entity_list.sort(key=lambda x: (int(x[0]),int(x[1]))) # arrange in ascending order w.r.t sentence, char number\n",
        "  sent_num_list = copy.deepcopy(input_sent_num_list[i]) # copy of the sentences list\n",
        "  sent_num_list.sort()                          # sorting the sentences list\n",
        "  sent_list = []  # list containg sentence strings\n",
        "  \n",
        "  for x in sent_num_list: \n",
        "    sent_list.append(input_stanza_list[i][x-1])\n",
        "  \n",
        "  sent_dict_list = dict(zip(sent_num_list,sent_list)) # dictionary of sentence number and strings\n",
        "  for n ,ind_s ,ind_e, ph in entity_list: # entity list to label formation\n",
        "    \n",
        "    if int(n) in sent_num_list:\n",
        "      sent_dict_list[int(n)] = sent_dict_list[int(n)].replace(ph,make_substring(len(ph.split())),1)\n",
        "      \n",
        "  sent_label_list = list(sent_dict_list.values())\n",
        "  task2_in.append(sent_list)\n",
        "  task2_label.append(sent_label_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyzZwVYnP7-y"
      },
      "source": [
        "# valid function\n",
        "import copy\n",
        "val_task2_in = [] #input sentences of examples\n",
        "val_task2_label = []# replaced the input by labels for phrases\n",
        "for i in range(len(val_input_stanza_list)): #process each file for labels\n",
        "  val_entity_list = [j.split('\\t') for j in val_input_entity_list[i]] # split the entities line\n",
        "  val_entity_list.sort(key=lambda x: (int(x[0]),int(x[1]))) # arrange in ascending order w.r.t sentence, char number\n",
        "  val_sent_num_list = copy.deepcopy(val_input_sent_num_list[i]) # copy of the sentences list\n",
        "  val_sent_num_list.sort()                          # sorting the sentences list\n",
        "  val_sent_list = []  # list containg sentence strings\n",
        "  #print(sentence_num_list)\n",
        "  for x in val_sent_num_list: \n",
        "    val_sent_list.append(val_input_stanza_list[i][x-1])\n",
        "  #print(c,len(sent_list),len(c))\n",
        "  #print(sent_list[0])\n",
        "  val_sent_dict_list = dict(zip(val_sent_num_list,val_sent_list)) # dictionary of sentence number and strings\n",
        "  for n ,ind_s ,ind_e, ph in val_entity_list: # entity list to label formation\n",
        "    if int(n) in val_sent_num_list:\n",
        "      val_sent_dict_list[int(n)] = val_sent_dict_list[int(n)].replace(ph,make_substring(len(ph.split())),1)\n",
        "      \n",
        "  val_sent_label_list = list(val_sent_dict_list.values())\n",
        "  val_task2_in.append(val_sent_list)\n",
        "  val_task2_label.append(val_sent_label_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3hT4uteRYF2"
      },
      "source": [
        "# train dataset replacing all unnecessary tokens with 'O' token \n",
        "for i,out in enumerate(task2_label): \n",
        "  for j,line in enumerate(out):\n",
        "    for k,tok in enumerate(line.split()):\n",
        "      if tok not in ['B','I','L','U']:\n",
        "        task2_label[i][j]=task2_label[i][j].replace(tok,'O',1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm2Gp5TOoAsm"
      },
      "source": [
        "# valid dataset replacing all unnecessary tokens with 'O' token \r\n",
        "for i,out in enumerate(val_task2_label): \r\n",
        "  for j,line in enumerate(out):\r\n",
        "    for k,tok in enumerate(line.split()):\r\n",
        "      if tok not in ['B','I','L','U']:\r\n",
        "        val_task2_label[i][j]=val_task2_label[i][j].replace(tok,'O',1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8tYVQRhRqqa"
      },
      "source": [
        "# Helper Functions\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq):\n",
        "    for count,i in enumerate(seq):\n",
        "       temp = tokenizer.tokenize(i) \n",
        "       if(len(temp)>1):\n",
        "         seq[count] = temp[0]           \n",
        "    #idxs = [to_ix[w] for w in seq if w in to_ix.keys()]\n",
        "    sentences = \" \".join(seq)\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-XL1T2ySEwF"
      },
      "source": [
        "#Defining the Model\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        #Pretrianed SciBert downloaded from allenai\n",
        "        self.modell = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        \n",
        "        outputs = self.modell(**sentence, output_hidden_states = True)\n",
        "        scibert_out = ((outputs[2][12])[0]).view(len(sentence[\"input_ids\"][0]), 1, -1)\n",
        "        self.hidden = self.init_hidden()\n",
        "        lstm_out, self.hidden = self.lstm(scibert_out, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence[\"input_ids\"][0]), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = [] # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL9ZAlF8qpWh"
      },
      "source": [
        "#Model hyperparameters and Word to index dictionary defined\r\n",
        "START_TAG = \"<START>\"\r\n",
        "STOP_TAG = \"<STOP>\"\r\n",
        "EMBEDDING_DIM = 768\r\n",
        "HIDDEN_DIM = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ayHv-LxSHSx"
      },
      "source": [
        "word_to_ix = {'<UNK>': 0}\n",
        "for i,file in enumerate(task2_in):\n",
        "  for j,sent in enumerate(file):\n",
        "    for k,tok in enumerate(sent.split()):\n",
        "       if tok not in word_to_ix:\n",
        "         word_to_ix[tok] = len(word_to_ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLTBLDJWq492"
      },
      "source": [
        "tag_to_ix = {\"B\": 0, \"I\": 1, \"L\": 2, \"U\": 3, \"O\": 4, START_TAG: 5, STOP_TAG: 6}\r\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASxEf_1yT3Im",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727,
          "referenced_widgets": [
            "a223b994e7ea4676a7935d17a7408222",
            "f67d236972ce4e8d8ffee5625f7e95ff",
            "cc705f7a2a614ee58aff93ae2ff8535b",
            "f89db72c59a944ea8f83b5694d92ef23",
            "f20017579d1e4e159858fba92f17483a",
            "9339193a02744e2cbff7c0e270ae2763",
            "1107de7fc3f8494399486fa308665ccc",
            "ca2865f1d63942f2bf2a48f08d3c6dc1",
            "44ca003785034507a05bef4be9686c23",
            "f8a3b77ac8b34b6d9453330ecaac2856",
            "fdc36ed72a2e49b580ff0a137a57bbc7",
            "6ddd9c5ad8ef48adb65bf6b924a7179d",
            "9eb1b888fc8242e68ea1176af6c6d2b6",
            "672faf9fd8dc4597a748b11dd9c8e130",
            "f5d58794e7da4aa9bbdeb53da208a03f",
            "85d23e14c2094ce1b848a1cfe2bfe0ff"
          ]
        },
        "outputId": "98af3fb8-d37a-48ae-8d8c-e1afc50ff83b"
      },
      "source": [
        "!pip install transformers==3.5\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 28.7MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 26.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 23.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 20.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 15.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 16.4MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 16.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 15.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 15.0MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 15.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 15.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 15.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 15.0MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (20.8)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 46.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5) (51.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=fbc2f72d9e00b194c86811080f12b8f8d23919fa6130810a830ae938b6bdcec5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a223b994e7ea4676a7935d17a7408222",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44ca003785034507a05bef4be9686c23",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=227845.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9s0cfdzQC0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c3ec628bb771470ca732d68686e46d02",
            "cc678054bd10472caf3c276ebea8e72c",
            "0cbbf03fe4b748f5bd0fe09cc3374c72",
            "bd0e5eefd99748edb41367ad187aa0e1",
            "cda5b20916644fcb88cffc2351c2ff61",
            "2c012a40dc5048e9bdcaf27ae453c243",
            "79c63becb6304dd391a6a2a78a4e61ce",
            "623428663e1843959d9f107b63b661d8"
          ]
        },
        "outputId": "776ab0c2-7a74-4dd3-8b4c-17e311cad907"
      },
      "source": [
        "# Model initialized and Seeds defined\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "model = BiLSTM_CRF(12345, tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\r\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\r\n",
        "model.cuda()\r\n",
        "seed_val = 66\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3ec628bb771470ca732d68686e46d02",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442221694.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27KBpWDwk2LE"
      },
      "source": [
        "def calc_eval(listp, listg):\n",
        "  correct = 0\n",
        "  total_p = 0\n",
        "  for i in zip(listp, listg):\n",
        "    flag = 0\n",
        "    for j in zip(i[0], i[1]):\n",
        "      if j[0]=='U' or j[0]=='B':\n",
        "        total_p += 1\n",
        "      if flag==0:\n",
        "        if j[0]=='U' and j[1]=='U':\n",
        "          correct += 1\n",
        "        if j[0]=='B' and j[1]=='B':\n",
        "          flag = 1\n",
        "      else:\n",
        "        if j[0]=='L' and j[1]=='L':\n",
        "          correct += 1\n",
        "          flag = 0\n",
        "        elif j[0]==j[1]:\n",
        "          continue\n",
        "        else:\n",
        "          flag = 0\n",
        "  return correct,total_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpbr0G7vtFQc"
      },
      "source": [
        "def training_eval():\n",
        "  task2_out_label = []\n",
        "  for i,file in enumerate(task2_in):\n",
        "    output = []\n",
        "    for j,sent in enumerate(file):\n",
        "      with torch.no_grad():\n",
        "        precheck_sent = prepare_sequence(task2_in[i][j].split()).to(device)\n",
        "        sent_out_label = model(precheck_sent)[1]\n",
        "        sent_str_label = [ix_to_tag[t] for t in sent_out_label]\n",
        "        #print(sent_str_label)\n",
        "        #break\n",
        "        output.append(sent_str_label)\n",
        "    task2_out_label.append(output)\n",
        "    #break\n",
        "  print(len(task2_out_label))\n",
        "  true_pos = 0\n",
        "  total_ph_pred = 0\n",
        "  for i,file in enumerate(task2_label):\n",
        "    file_true_pos, file_total_ph_pred = calc_eval(task2_out_label[i],[[\"O\"] + s.split() + [\"O\"] for s in task2_label[i]])\n",
        "    true_pos = true_pos + file_true_pos\n",
        "    total_ph_pred = total_ph_pred + file_total_ph_pred\n",
        "    if i%20==0:\n",
        "      print(\"Each example\",i,file_true_pos,file_total_ph_pred)\n",
        "  print(true_pos,total_ph_pred,total_phrases_truth)\n",
        "  precision = 0\n",
        "  recall = 0\n",
        "  F1score=  0\n",
        "  if(total_ph_pred!=0):\n",
        "    precision = true_pos/total_ph_pred\n",
        "  if(total_phrases_truth!=0):\n",
        "    recall = true_pos/total_phrases_truth\n",
        "  if((precision + recall)!=0):  \n",
        "    F1score = 2 * precision*recall/(precision+recall)\n",
        "  print(\"Precision is {} and recall is {} and F1 Score is {}\".format(precision,recall,F1score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sow8PPlLtVn8"
      },
      "source": [
        "def validation_eval():\n",
        "  val_task2_out_label = []\n",
        "  for i,file in enumerate(val_task2_in):\n",
        "    output = []\n",
        "    for j,sent in enumerate(file):\n",
        "      with torch.no_grad():\n",
        "        precheck_sent = prepare_sequence(val_task2_in[i][j].split()).to(device)\n",
        "        sent_out_label = model(precheck_sent)[1]\n",
        "        sent_str_label = [ix_to_tag[t] for t in sent_out_label]\n",
        "        #print(sent_str_label)\n",
        "        #break\n",
        "        output.append(sent_str_label)\n",
        "    val_task2_out_label.append(output)\n",
        "    #break\n",
        "  print(len(val_task2_out_label))\n",
        "  val_true_pos = 0\n",
        "  val_total_ph_pred = 0\n",
        "  for i,file in enumerate(val_task2_label):\n",
        "    val_file_true_pos, val_file_total_ph_pred = calc_eval(val_task2_out_label[i],[[\"O\"] + s.split() + [\"O\"] for s in val_task2_label[i]])\n",
        "    val_true_pos = val_true_pos + val_file_true_pos\n",
        "    val_total_ph_pred = val_total_ph_pred + val_file_total_ph_pred\n",
        "    #if i%20==0:\n",
        "    print(\"Each example\",i,\"true pos\",val_file_true_pos,\"phrase in pred\",val_file_total_ph_pred)\n",
        "  print(val_true_pos,val_total_ph_pred,val_total_phrases_truth)\n",
        "  val_precision = 0\n",
        "  val_recall = 0\n",
        "  val_F1score=  0\n",
        "  if(val_total_ph_pred!=0):\n",
        "    val_precision = val_true_pos/val_total_ph_pred\n",
        "  if(val_total_phrases_truth!=0):\n",
        "    val_recall = val_true_pos/val_total_phrases_truth\n",
        "  if((val_precision + val_recall)!=0):  \n",
        "    val_F1score = 2 * val_precision*val_recall/(val_precision+val_recall)\n",
        "  print(\"Precision is {} and recall is {} and F1 Score is {}\".format(val_precision,val_recall,val_F1score))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSX1B3FRpKxc"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BozoCn89SRmm"
      },
      "source": [
        "#The MAIN TRAINING CELL\n",
        "\n",
        "#Check predictions before training\n",
        "with torch.no_grad():\n",
        "    #print(task2_in[0][0].split())\n",
        "    precheck_sent = prepare_sequence(task2_in[0][0].split()).to(device)\n",
        "    #print(precheck_sent)\n",
        "    precheck_tags = torch.tensor([4] + [tag_to_ix[t] for t in task2_label[0][0].split()] + [4], dtype=torch.long).to(device)\n",
        "    print(precheck_tags)\n",
        "    print(model(precheck_sent),'\\n',precheck_tags)\n",
        "    print(\"checkpoint passed\")\n",
        "\n",
        "\n",
        "for epoch in range(50):  #  or 300 epochs\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    for i,file in enumerate(task2_in):\n",
        "      for j,sent in enumerate(file):\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        sentence_in = prepare_sequence(sent.split()).to(device)\n",
        "    \n",
        "        targets = torch.tensor([4] + [tag_to_ix[t] for t in task2_label[i][j].split()] + [4], dtype=torch.long).to(device)\n",
        "        \n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "    end = time.time()\n",
        "\n",
        "    #Saving the Model, note that it will be very heavy  \n",
        "    \"\"\"\n",
        "    if epoch%1 == 0: \n",
        "      torch.save(model,\"/content/drive/My Drive/train/ULTIMATEEEEEEEEE_finality_GPU_scibert_LSTM_adam_validation_train_version1_mod\" + str(epoch) + \".pt\")\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"The epoch completed is\",epoch, \"and time\", end-start)\n",
        "\n",
        "    #Validation Loss computation\n",
        "    if epoch%1 ==0:\n",
        "      model.eval()\n",
        "      training_eval()\n",
        "      validation_eval()\n",
        "      val_total_loss = 0.\n",
        "      with torch.no_grad():\n",
        "        for k, val_file in enumerate(val_task2_in):\n",
        "          for l, val_sent in enumerate(val_file):\n",
        "            val_sentence_in = prepare_sequence(val_sent.split()).to(device)\n",
        "            val_targets = torch.tensor([4] + [tag_to_ix[t] for t in val_task2_label[k][l].split()] + [4], dtype=torch.long).to(device)\n",
        "            loss = model.neg_log_likelihood(val_sentence_in, val_targets)\n",
        "            val_total_loss += loss.item()\n",
        "      print(\"validation loss after epoch\",epoch,\" is\", val_total_loss)\n",
        "      \n",
        "\n",
        "# Simple check\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(task2_in[0][0].split()).to(device)\n",
        "    print(model(precheck_sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KnHmMCj6sJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2497b1b-9b9c-4238-d9bb-822e55593300"
      },
      "source": [
        "#Model Evaluation\r\n",
        "model.eval()\r\n",
        "\r\n",
        "#Train Metrics\r\n",
        "training_eval()\r\n",
        "\r\n",
        "#Validation Metrics\r\n",
        "validation_eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237\n",
            "Each example 0 37 50\n",
            "Each example 20 197 226\n",
            "Each example 40 117 141\n",
            "Each example 60 73 94\n",
            "Each example 80 55 106\n",
            "Each example 100 125 142\n",
            "Each example 120 103 154\n",
            "Each example 140 60 67\n",
            "Each example 160 121 143\n",
            "Each example 180 177 196\n",
            "Each example 200 134 179\n",
            "Each example 220 143 164\n",
            "25989 31053 30445\n",
            "Precision is 0.8369239686986765 and recall is 0.8536377073411069 and F1 Score is 0.8451982178282219\n",
            "50\n",
            "Each example 0 true pos 18 phrase in pred 71\n",
            "Each example 1 true pos 48 phrase in pred 114\n",
            "Each example 2 true pos 92 phrase in pred 233\n",
            "Each example 3 true pos 20 phrase in pred 56\n",
            "Each example 4 true pos 55 phrase in pred 105\n",
            "Each example 5 true pos 99 phrase in pred 209\n",
            "Each example 6 true pos 74 phrase in pred 132\n",
            "Each example 7 true pos 37 phrase in pred 80\n",
            "Each example 8 true pos 80 phrase in pred 148\n",
            "Each example 9 true pos 73 phrase in pred 182\n",
            "Each example 10 true pos 51 phrase in pred 95\n",
            "Each example 11 true pos 91 phrase in pred 173\n",
            "Each example 12 true pos 50 phrase in pred 108\n",
            "Each example 13 true pos 23 phrase in pred 84\n",
            "Each example 14 true pos 54 phrase in pred 111\n",
            "Each example 15 true pos 39 phrase in pred 142\n",
            "Each example 16 true pos 85 phrase in pred 125\n",
            "Each example 17 true pos 58 phrase in pred 128\n",
            "Each example 18 true pos 42 phrase in pred 65\n",
            "Each example 19 true pos 56 phrase in pred 112\n",
            "Each example 20 true pos 77 phrase in pred 128\n",
            "Each example 21 true pos 37 phrase in pred 75\n",
            "Each example 22 true pos 90 phrase in pred 154\n",
            "Each example 23 true pos 37 phrase in pred 86\n",
            "Each example 24 true pos 151 phrase in pred 228\n",
            "Each example 25 true pos 34 phrase in pred 68\n",
            "Each example 26 true pos 80 phrase in pred 134\n",
            "Each example 27 true pos 81 phrase in pred 135\n",
            "Each example 28 true pos 77 phrase in pred 136\n",
            "Each example 29 true pos 78 phrase in pred 127\n",
            "Each example 30 true pos 33 phrase in pred 72\n",
            "Each example 31 true pos 56 phrase in pred 117\n",
            "Each example 32 true pos 109 phrase in pred 176\n",
            "Each example 33 true pos 50 phrase in pred 136\n",
            "Each example 34 true pos 84 phrase in pred 124\n",
            "Each example 35 true pos 73 phrase in pred 133\n",
            "Each example 36 true pos 61 phrase in pred 94\n",
            "Each example 37 true pos 90 phrase in pred 191\n",
            "Each example 38 true pos 108 phrase in pred 173\n",
            "Each example 39 true pos 31 phrase in pred 53\n",
            "Each example 40 true pos 94 phrase in pred 191\n",
            "Each example 41 true pos 78 phrase in pred 131\n",
            "Each example 42 true pos 34 phrase in pred 68\n",
            "Each example 43 true pos 18 phrase in pred 30\n",
            "Each example 44 true pos 77 phrase in pred 147\n",
            "Each example 45 true pos 74 phrase in pred 108\n",
            "Each example 46 true pos 70 phrase in pred 116\n",
            "Each example 47 true pos 42 phrase in pred 70\n",
            "Each example 48 true pos 112 phrase in pred 154\n",
            "Each example 49 true pos 85 phrase in pred 179\n",
            "3266 6207 4777\n",
            "Precision is 0.5261801192202352 and recall is 0.6836926941595144 and F1 Score is 0.5946831755280408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIy6mvKWqnxJ"
      },
      "source": [
        "#Evaluation Script (StreamLined)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pLXR9epu4Es",
        "outputId": "daddceaf-e4b2-4d6d-ed37-f0e15eab2219"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFqvypeRu99_"
      },
      "source": [
        "# Helper Functions\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq):\n",
        "    for count,i in enumerate(seq):\n",
        "       temp = tokenizer.tokenize(i) \n",
        "       if(len(temp)>1):\n",
        "         seq[count] = temp[0]           \n",
        "    #idxs = [to_ix[w] for w in seq if w in to_ix.keys()]\n",
        "    sentences = \" \".join(seq)\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HxfSWhQvD7a"
      },
      "source": [
        "#Defining the Model\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        #Pretrianed SciBert downloaded from allenai\n",
        "        self.modell = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        \n",
        "        outputs = self.modell(**sentence, output_hidden_states = True)\n",
        "        scibert_out = ((outputs[2][12])[0]).view(len(sentence[\"input_ids\"][0]), 1, -1)\n",
        "        self.hidden = self.init_hidden()\n",
        "        lstm_out, self.hidden = self.lstm(scibert_out, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence[\"input_ids\"][0]), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = [] # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQjSIa1LvL0A"
      },
      "source": [
        "#Model hyperparameters and Word to index dictionary defined\r\n",
        "START_TAG = \"<START>\"\r\n",
        "STOP_TAG = \"<STOP>\"\r\n",
        "EMBEDDING_DIM = 768\r\n",
        "HIDDEN_DIM = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu2TYLGqvQeq"
      },
      "source": [
        "tag_to_ix = {\"B\": 0, \"I\": 1, \"L\": 2, \"U\": 3, \"O\": 4, START_TAG: 5, STOP_TAG: 6}\r\n",
        "ix_to_tag = {v: k for k, v in tag_to_ix.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727,
          "referenced_widgets": [
            "01f98f7306544ee0aa000ceb61da4b47",
            "052558e5f86d418d9d8f02f59444f612",
            "87c08661fc4b487aacc7a641f1c9d618",
            "41bf7d4b68884096b4e5577289678c3c",
            "d2f4be0e6df745068776030b5f43b609",
            "a28311b8061b4dc0945fdd9a0f170d15",
            "0cba5eef1e1d4fac9e91a21c54e4b3e0",
            "8ddcaa51905b40c6886eeec5c05e0bb2",
            "51ff07514d334dbe9398aba41c61cdac",
            "3f76c39eefab419aa0b20f71a420b4aa",
            "2e557a37c1a44592966f71312000d495",
            "fce7c90218724d37ab50429c2ac8ad7d",
            "0f47104be3364d97884acc37ebce15d0",
            "13cc564748fe4b0193276f9301ae15e9",
            "f76b6b04a10943fda70afa4dcfc252e3",
            "8cd6fbab9cd64e90afdb4f91f8c6752b"
          ]
        },
        "id": "WKm4IzIivWV9",
        "outputId": "c10aba01-2ceb-4bf1-a385-81dbcfbe8cfb"
      },
      "source": [
        "!pip install transformers==3.5\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (2019.12.20)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5) (4.41.1)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5) (51.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=c1d56f3259874885ee7cb673b5a48628c456ad316b78a947686d096bc448bd5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01f98f7306544ee0aa000ceb61da4b47",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51ff07514d334dbe9398aba41c61cdac",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=227845.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMmgytxDvbly",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b97ba51e67e6438aba016d25b40ea52d",
            "1d306924ebc649a8a682b27b661c99f1",
            "4f11bd433f5949699d0f3e25a8173360",
            "e360cdaa322045aa8f5c2377011a8390",
            "e462ff72ab1c41f6a4441622d49dca73",
            "65b21b70ab9c49bf88c50e553439df09",
            "02169ecf7e1747ecb547487744085a64",
            "a8f8fdea391d4a35a09c64361069ac7b"
          ]
        },
        "outputId": "5434997a-264d-4930-fc8e-d59ba4f0c1ba"
      },
      "source": [
        "# Model initialized and Seeds defined\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "model = BiLSTM_CRF(12345, tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\r\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\r\n",
        "model.cuda()\r\n",
        "seed_val = 66\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b97ba51e67e6438aba016d25b40ea52d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442221694.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqRglUUEsYaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee51eed-69f6-4bb7-8629-9f40dceacbfe"
      },
      "source": [
        "model = torch.load(\"/content/drive/MyDrive/CS779_All_Subtasks/Final_sub_task_2_scibert_LSTM_1.pt\")\r\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_CRF(\n",
              "  (lstm): LSTM(768, 100, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=200, out_features=7, bias=True)\n",
              "  (modell): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTeYARlS6Jj5"
      },
      "source": [
        "def calc_eval(listp, listg):\n",
        "  correct = 0\n",
        "  total_p = 0\n",
        "  for i in zip(listp, listg):\n",
        "    flag = 0\n",
        "    for j in zip(i[0], i[1]):\n",
        "      if j[0]=='U' or j[0]=='B':\n",
        "        total_p += 1\n",
        "      if flag==0:\n",
        "        if j[0]=='U' and j[1]=='U':\n",
        "          correct += 1\n",
        "        if j[0]=='B' and j[1]=='B':\n",
        "          flag = 1\n",
        "      else:\n",
        "        if j[0]=='L' and j[1]=='L':\n",
        "          correct += 1\n",
        "          flag = 0\n",
        "        elif j[0]==j[1]:\n",
        "          continue\n",
        "        else:\n",
        "          flag = 0\n",
        "  return correct,total_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lIX8f7mrWz3"
      },
      "source": [
        "def make_substring(n): # replace phrases with labels\n",
        "  if n==0:\n",
        "    return ''\n",
        "  elif n ==1:\n",
        "    return 'U'\n",
        "  elif n==2:\n",
        "    return 'B L'\n",
        "  else:\n",
        "    t1 = 'I '*(n-2)\n",
        "    return 'B '+t1+'L'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WncAQV-oq3BT"
      },
      "source": [
        "#output_dir = \"/content/drive/MyDrive/result_cs779/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPkKBpGYhhIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762d79da-eab8-400e-abed-40680d03572c"
      },
      "source": [
        "# Test dataset reading\n",
        "import os\n",
        "test_input_dir = \"/content/drive/MyDrive/submission8/\"\n",
        "# test_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
        "#          \"relation-classification\", \"text-classification\"]\n",
        "#test_list_of_folders = [\"entity-linking\", \"face-alignment\", \"face-detection\", \"natural-language-inference\"]\n",
        "test_list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
        "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
        "                   \"document_classification\",\"entity_linking\",\n",
        "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
        "                   \"natural_language_inference\"]\n",
        "test_input_stanza_list = []\n",
        "test_input_sent_num_list = []\n",
        "test_input_entity_list = []\n",
        "test_file_name_list = []\n",
        "test_total_phrases_truth = 0\n",
        "Capital_test_input_stanza_list = []\n",
        "for fls in test_list_of_folders:\n",
        "  count=0\n",
        "  for i in os.listdir(test_input_dir + fls + '/'):\n",
        "    count=count+1\n",
        "    for files in os.listdir(test_input_dir + fls + '/' + str(i)):\n",
        "      if files.endswith(\"Stanza-out.txt\"):\n",
        "        stanza_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        print(test_input_dir + fls + '/' + str(i))\n",
        "        Capital_stanza_lines = stanza_file.read()\n",
        "        Capital_stanza_lines_list = list(filter(None,Capital_stanza_lines.splitlines())) # filter empty strings and split into lines\n",
        "        Capital_test_input_stanza_list.append(Capital_stanza_lines_list)\n",
        "\n",
        "        stanza_lines = Capital_stanza_lines.lower()\n",
        "        stanza_lines_list = list(filter(None,stanza_lines.splitlines())) # filter empty strings and split into lines\n",
        "        test_input_stanza_list.append(stanza_lines_list)\n",
        "      if files.endswith(\"sentences.txt\"):\n",
        "        sentence_file = open(test_input_dir + fls + '/' + str(i) + '/' + 'sentences.txt', \"r\")\n",
        "        sentence_num_list = list(filter(None,(sentence_file.read().lower()).splitlines())) # filter empty strings and split into lines\n",
        "        test_input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
        "      # if files.endswith(\"entities.txt\"):\n",
        "      #   entities_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "      #   entities_list = list(filter(None,(entities_file.read().lower()).splitlines())) # filter empty strings and split into lines\n",
        "      #   test_input_entity_list.append(entities_list)\n",
        "      #   test_total_phrases_truth = test_total_phrases_truth + len(entities_list)\n",
        "    test_file_name_list.append(fls + '/' + str(i))\n",
        "  print(\"completed\",fls,count)\n",
        "##########Check test data\n",
        "print(\"Test no. of examples for each stanza,sentences and entities\")\n",
        "print(len(test_input_sent_num_list),len(test_file_name_list))\n",
        "#print(len(test_input_entity_list),len(test_input_sent_num_list),len(test_input_stanza_list),len(test_file_name_list))\n",
        "#test_input_sent_num_list = [[int(s) for s in sublist] for sublist in test_input_sent_num_list] # convert sentence list string to integer\n",
        "#test_input_sent_num_list = [list(set(x)) for x in test_input_sent_num_list]\n",
        "print(\"print one example to show all files\")\n",
        "print(len(test_input_sent_num_list[0]),test_file_name_list[0])\n",
        "#print(len(test_input_entity_list[0]),len(test_input_sent_num_list[0]),len(test_input_stanza_list[0]),test_file_name_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/submission8/constituency_parsing/6\n",
            "/content/drive/MyDrive/submission8/constituency_parsing/5\n",
            "/content/drive/MyDrive/submission8/constituency_parsing/8\n",
            "/content/drive/MyDrive/submission8/constituency_parsing/7\n",
            "/content/drive/MyDrive/submission8/constituency_parsing/0\n",
            "/content/drive/MyDrive/submission8/constituency_parsing/4\n",
            "/content/drive/MyDrive/submission8/constituency_parsing/3\n",
            "/content/drive/MyDrive/submission8/constituency_parsing/2\n",
            "/content/drive/MyDrive/submission8/constituency_parsing/1\n",
            "completed constituency_parsing 9\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/9\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/6\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/8\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/7\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/4\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/2\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/5\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/3\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/1\n",
            "/content/drive/MyDrive/submission8/coreference_resolution/0\n",
            "completed coreference_resolution 10\n",
            "/content/drive/MyDrive/submission8/data-to-text_generation/6\n",
            "/content/drive/MyDrive/submission8/data-to-text_generation/2\n",
            "/content/drive/MyDrive/submission8/data-to-text_generation/4\n",
            "/content/drive/MyDrive/submission8/data-to-text_generation/1\n",
            "/content/drive/MyDrive/submission8/data-to-text_generation/3\n",
            "/content/drive/MyDrive/submission8/data-to-text_generation/5\n",
            "/content/drive/MyDrive/submission8/data-to-text_generation/0\n",
            "completed data-to-text_generation 7\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/6\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/0\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/1\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/7\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/8\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/2\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/5\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/4\n",
            "/content/drive/MyDrive/submission8/dependency_parsing/3\n",
            "completed dependency_parsing 9\n",
            "/content/drive/MyDrive/submission8/document_classification/9\n",
            "/content/drive/MyDrive/submission8/document_classification/3\n",
            "/content/drive/MyDrive/submission8/document_classification/6\n",
            "/content/drive/MyDrive/submission8/document_classification/2\n",
            "/content/drive/MyDrive/submission8/document_classification/8\n",
            "/content/drive/MyDrive/submission8/document_classification/18\n",
            "/content/drive/MyDrive/submission8/document_classification/19\n",
            "/content/drive/MyDrive/submission8/document_classification/4\n",
            "/content/drive/MyDrive/submission8/document_classification/7\n",
            "/content/drive/MyDrive/submission8/document_classification/5\n",
            "/content/drive/MyDrive/submission8/document_classification/20\n",
            "/content/drive/MyDrive/submission8/document_classification/0\n",
            "/content/drive/MyDrive/submission8/document_classification/1\n",
            "/content/drive/MyDrive/submission8/document_classification/14\n",
            "/content/drive/MyDrive/submission8/document_classification/17\n",
            "/content/drive/MyDrive/submission8/document_classification/12\n",
            "/content/drive/MyDrive/submission8/document_classification/15\n",
            "/content/drive/MyDrive/submission8/document_classification/16\n",
            "/content/drive/MyDrive/submission8/document_classification/10\n",
            "/content/drive/MyDrive/submission8/document_classification/11\n",
            "/content/drive/MyDrive/submission8/document_classification/13\n",
            "completed document_classification 21\n",
            "/content/drive/MyDrive/submission8/entity_linking/3\n",
            "/content/drive/MyDrive/submission8/entity_linking/2\n",
            "/content/drive/MyDrive/submission8/entity_linking/5\n",
            "/content/drive/MyDrive/submission8/entity_linking/9\n",
            "/content/drive/MyDrive/submission8/entity_linking/4\n",
            "/content/drive/MyDrive/submission8/entity_linking/7\n",
            "/content/drive/MyDrive/submission8/entity_linking/6\n",
            "/content/drive/MyDrive/submission8/entity_linking/8\n",
            "/content/drive/MyDrive/submission8/entity_linking/11\n",
            "/content/drive/MyDrive/submission8/entity_linking/16\n",
            "/content/drive/MyDrive/submission8/entity_linking/0\n",
            "/content/drive/MyDrive/submission8/entity_linking/10\n",
            "/content/drive/MyDrive/submission8/entity_linking/1\n",
            "/content/drive/MyDrive/submission8/entity_linking/13\n",
            "/content/drive/MyDrive/submission8/entity_linking/15\n",
            "/content/drive/MyDrive/submission8/entity_linking/14\n",
            "/content/drive/MyDrive/submission8/entity_linking/12\n",
            "completed entity_linking 17\n",
            "/content/drive/MyDrive/submission8/face_alignment/3\n",
            "/content/drive/MyDrive/submission8/face_alignment/7\n",
            "/content/drive/MyDrive/submission8/face_alignment/8\n",
            "/content/drive/MyDrive/submission8/face_alignment/5\n",
            "/content/drive/MyDrive/submission8/face_alignment/4\n",
            "/content/drive/MyDrive/submission8/face_alignment/9\n",
            "/content/drive/MyDrive/submission8/face_alignment/6\n",
            "/content/drive/MyDrive/submission8/face_alignment/2\n",
            "/content/drive/MyDrive/submission8/face_alignment/10\n",
            "/content/drive/MyDrive/submission8/face_alignment/12\n",
            "/content/drive/MyDrive/submission8/face_alignment/17\n",
            "/content/drive/MyDrive/submission8/face_alignment/15\n",
            "/content/drive/MyDrive/submission8/face_alignment/13\n",
            "/content/drive/MyDrive/submission8/face_alignment/11\n",
            "/content/drive/MyDrive/submission8/face_alignment/18\n",
            "/content/drive/MyDrive/submission8/face_alignment/16\n",
            "/content/drive/MyDrive/submission8/face_alignment/14\n",
            "/content/drive/MyDrive/submission8/face_alignment/0\n",
            "/content/drive/MyDrive/submission8/face_alignment/1\n",
            "completed face_alignment 19\n",
            "/content/drive/MyDrive/submission8/face_detection/21\n",
            "/content/drive/MyDrive/submission8/face_detection/9\n",
            "/content/drive/MyDrive/submission8/face_detection/4\n",
            "/content/drive/MyDrive/submission8/face_detection/20\n",
            "/content/drive/MyDrive/submission8/face_detection/3\n",
            "/content/drive/MyDrive/submission8/face_detection/6\n",
            "/content/drive/MyDrive/submission8/face_detection/5\n",
            "/content/drive/MyDrive/submission8/face_detection/7\n",
            "/content/drive/MyDrive/submission8/face_detection/8\n",
            "/content/drive/MyDrive/submission8/face_detection/19\n",
            "/content/drive/MyDrive/submission8/face_detection/12\n",
            "/content/drive/MyDrive/submission8/face_detection/16\n",
            "/content/drive/MyDrive/submission8/face_detection/14\n",
            "/content/drive/MyDrive/submission8/face_detection/2\n",
            "/content/drive/MyDrive/submission8/face_detection/15\n",
            "/content/drive/MyDrive/submission8/face_detection/13\n",
            "/content/drive/MyDrive/submission8/face_detection/17\n",
            "/content/drive/MyDrive/submission8/face_detection/18\n",
            "/content/drive/MyDrive/submission8/face_detection/11\n",
            "/content/drive/MyDrive/submission8/face_detection/10\n",
            "/content/drive/MyDrive/submission8/face_detection/1\n",
            "/content/drive/MyDrive/submission8/face_detection/0\n",
            "completed face_detection 22\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/7\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/2\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/4\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/8\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/5\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/1\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/6\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/3\n",
            "/content/drive/MyDrive/submission8/hypernym_discovery/0\n",
            "completed hypernym_discovery 9\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/4\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/5\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/30\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/6\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/8\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/9\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/7\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/29\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/3\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/31\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/2\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/22\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/26\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/21\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/24\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/23\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/28\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/20\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/25\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/27\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/16\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/10\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/19\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/15\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/17\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/11\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/13\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/12\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/18\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/14\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/1\n",
            "/content/drive/MyDrive/submission8/natural_language_inference/0\n",
            "completed natural_language_inference 32\n",
            "Test no. of examples for each stanza,sentences and entities\n",
            "155 155\n",
            "print one example to show all files\n",
            "13 constituency_parsing/6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_9-OxSsrxpD"
      },
      "source": [
        "#########test function\n",
        "import copy\n",
        "test_task2_in = [] #input sentences of examples\n",
        "test_task2_label = []# replaced the input by labels for phrases\n",
        "Capital_test_task2_in = []\n",
        "for i in range(len(test_input_sent_num_list)): #process each file for labels\n",
        "  #test_entity_list = [j.split('\\t') for j in test_input_entity_list[i]] # split the entities line\n",
        "  #test_entity_list.sort(key=lambda x: (int(x[0]),int(x[1]))) # arrange in ascending order w.r.t sentence, char number\n",
        "  test_sent_num_list = copy.deepcopy(test_input_sent_num_list[i]) # copy of the sentences list\n",
        "  test_sent_num_list.sort()                          # sorting the sentences list\n",
        "  test_sent_list = []  # list containg sentence strings\n",
        "  Capital_test_sent_list = []\n",
        "  #print(sentence_num_list)\n",
        "  for x in test_sent_num_list: \n",
        "    test_sent_list.append(test_input_stanza_list[i][x-1])\n",
        "    Capital_test_sent_list.append(Capital_test_input_stanza_list[i][x-1])\n",
        "  #print(c,len(sent_list),len(c))\n",
        "  #print(sent_list[0])\n",
        "  test_sent_dict_list = dict(zip(test_sent_num_list,test_sent_list)) # dictionary of sentence number and strings\n",
        "  # for n ,ind_s ,ind_e, ph in test_entity_list: # entity list to label formation\n",
        "  #   if int(n) in test_sent_num_list:\n",
        "  #     test_sent_dict_list[int(n)] = test_sent_dict_list[int(n)].replace(ph,make_substring(len(ph.split())),1)\n",
        "      \n",
        "  test_sent_label_list = list(test_sent_dict_list.values())\n",
        "  test_task2_in.append(test_sent_list)\n",
        "  Capital_test_task2_in.append(Capital_test_sent_list)\n",
        "  test_task2_label.append(copy.deepcopy(test_sent_label_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6U8d2gkruks"
      },
      "source": [
        "# Check the test label processing\r\n",
        "for i in range(len(test_task2_in)):\r\n",
        "  if len(test_task2_in[i]) != len(test_task2_label[i]):\r\n",
        "    print(len(test_task2_in[i]),len(test_task2_label[i]),i)\r\n",
        "# Flag checker to check whether all sizes are correct or not\r\n",
        "for i,file in enumerate(test_task2_in):\r\n",
        "    for j,sent in enumerate(file):\r\n",
        "      temp1 = sent.split()\r\n",
        "      temp2 = test_task2_label[i][j].split()\r\n",
        "      if len(temp1)!=len(temp2):\r\n",
        "        print(len(temp1), len(temp2), i,j,test_file_name_list[i])\r\n",
        "        print(sent,temp1,temp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tLT39WY3OIy"
      },
      "source": [
        "# Test dataset replacing all unnecessary tokens with 'O' token \n",
        "for i,out in enumerate(test_task2_label): \n",
        "  for j,line in enumerate(out):\n",
        "    for k,tok in enumerate(line.split()):\n",
        "      if tok not in ['B','I','L','U']:\n",
        "        test_task2_label[i][j]=test_task2_label[i][j].replace(tok,'O',1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrD-Kcr5LV0S"
      },
      "source": [
        "#This is where we make a dictionary that can map sentence to its corresponding number in stanza_out.txt \r\n",
        "\r\n",
        "import copy\r\n",
        "list_of_dict_for_number_to_sentence = []\r\n",
        "for i in range(len(test_input_stanza_list)): #process each file for labels\r\n",
        "  # test_entity_list = [j.split('\\t') for j in test_input_entity_list[i]] # split the entities line\r\n",
        "  # test_entity_list.sort(key=lambda x: (int(x[0]),int(x[1]))) # arrange in ascending order w.r.t sentence, char number\r\n",
        "  test_sent_num_list = copy.deepcopy(test_input_sent_num_list[i]) # copy of the sentences list\r\n",
        "  test_sent_num_list.sort()                          # sorting the sentences list\r\n",
        "  test_sent_list = []  # list containg sentence strings\r\n",
        "\r\n",
        "  for x in test_sent_num_list: \r\n",
        "    test_sent_list.append(test_input_stanza_list[i][x-1])\r\n",
        "  test_sent_dict_list = dict(zip(test_sent_num_list,test_sent_list)) # dictionary of sentence number and strings\r\n",
        "  list_of_dict_for_number_to_sentence.append(test_sent_dict_list)  \r\n",
        "\r\n",
        "#This is the mapper list of dictionaries\r\n",
        "list_of_dict_for_sentence_to_number = [dict((v,k) for k,v in a.items()) for a in list_of_dict_for_number_to_sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOyoMdEQiWKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d70f3b6-2991-477d-9ee8-3c2744004f71"
      },
      "source": [
        "# #####test label evaluate\n",
        "test_task2_out_label = []\n",
        "for i,file in enumerate(test_task2_in):\n",
        "  output = []\n",
        "  for j,sent in enumerate(file):\n",
        "    with torch.no_grad():\n",
        "      precheck_sent = prepare_sequence(test_task2_in[i][j].split()).to(device)\n",
        "      sent_out_label = model(precheck_sent)[1]\n",
        "      sent_str_label = [ix_to_tag[t] for t in sent_out_label]\n",
        "      output.append(sent_str_label)\n",
        "  test_task2_out_label.append(output)\n",
        "print(len(test_task2_out_label))\n",
        "test_true_pos = 0\n",
        "test_total_ph_pred = 0\n",
        "for i,file in enumerate(test_task2_label):\n",
        "  test_file_true_pos, test_file_total_ph_pred = calc_eval(test_task2_out_label[i],[[\"O\"] + s.split() + [\"O\"] for s in test_task2_label[i]])\n",
        "  test_true_pos = test_true_pos + test_file_true_pos\n",
        "  test_total_ph_pred = test_total_ph_pred + test_file_total_ph_pred\n",
        "  if i%5==0:\n",
        "    print(\"Each example\",i,\"true pos\",test_file_true_pos,\"phrase in pred\",test_file_total_ph_pred)\n",
        "print(test_true_pos,test_total_ph_pred,test_total_phrases_truth)\n",
        "\n",
        "\n",
        "# test_precision = 0\n",
        "# test_recall = 0\n",
        "# test_F1score=  0\n",
        "# if(test_total_ph_pred!=0):\n",
        "#   test_precision = test_true_pos/test_total_ph_pred\n",
        "# if(test_total_phrases_truth!=0):\n",
        "#   test_recall = test_true_pos/test_total_phrases_truth\n",
        "# if((test_precision + test_recall)!=0):  \n",
        "#   test_F1score = 2 * test_precision*test_recall/(test_precision+test_recall)\n",
        "# print(\"Precision is {} and recall is {} and F1 Score is {}\".format(test_precision,test_recall,test_F1score))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "155\n",
            "Each example 0 true pos 0 phrase in pred 81\n",
            "Each example 5 true pos 0 phrase in pred 151\n",
            "Each example 10 true pos 0 phrase in pred 128\n",
            "Each example 15 true pos 0 phrase in pred 174\n",
            "Each example 20 true pos 0 phrase in pred 116\n",
            "Each example 25 true pos 0 phrase in pred 197\n",
            "Each example 30 true pos 0 phrase in pred 119\n",
            "Each example 35 true pos 0 phrase in pred 113\n",
            "Each example 40 true pos 0 phrase in pred 50\n",
            "Each example 45 true pos 0 phrase in pred 135\n",
            "Each example 50 true pos 0 phrase in pred 270\n",
            "Each example 55 true pos 0 phrase in pred 112\n",
            "Each example 60 true pos 0 phrase in pred 180\n",
            "Each example 65 true pos 0 phrase in pred 77\n",
            "Each example 70 true pos 0 phrase in pred 153\n",
            "Each example 75 true pos 0 phrase in pred 276\n",
            "Each example 80 true pos 0 phrase in pred 179\n",
            "Each example 85 true pos 0 phrase in pred 134\n",
            "Each example 90 true pos 0 phrase in pred 81\n",
            "Each example 95 true pos 0 phrase in pred 213\n",
            "Each example 100 true pos 0 phrase in pred 237\n",
            "Each example 105 true pos 0 phrase in pred 309\n",
            "Each example 110 true pos 0 phrase in pred 174\n",
            "Each example 115 true pos 0 phrase in pred 59\n",
            "Each example 120 true pos 0 phrase in pred 70\n",
            "Each example 125 true pos 0 phrase in pred 218\n",
            "Each example 130 true pos 0 phrase in pred 101\n",
            "Each example 135 true pos 0 phrase in pred 203\n",
            "Each example 140 true pos 0 phrase in pred 333\n",
            "Each example 145 true pos 0 phrase in pred 153\n",
            "Each example 150 true pos 0 phrase in pred 110\n",
            "0 25401 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49rtZJFq-SwC"
      },
      "source": [
        "# %rm -r \"/content/machine-translation\"\r\n",
        "# %rm -r \"/content/named-entity-recognition\"\r\n",
        "# %rm -r \"/content/question-answering\"\r\n",
        "# %rm -r \"/content/relation-classification\"\r\n",
        "# %rm -r \"/content/text-classification\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItNq2nmD5Sou"
      },
      "source": [
        "# %mkdir \"/content/machine-translation\"\r\n",
        "# %mkdir \"/content/named-entity-recognition\"\r\n",
        "# %mkdir \"/content/question-answering\"\r\n",
        "# %mkdir \"/content/relation-classification\"\r\n",
        "# %mkdir \"/content/text-classification\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7bXVSMUV2xs"
      },
      "source": [
        "#### Undo the third task inference (Delete all the triples folder in the test data)\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "#####Test model\r\n",
        "#test_input_dir = \"/content/drive/My Drive/Dev_v2/\"\r\n",
        "test_input_dir = \"/content/drive/MyDrive/submission8/\"\r\n",
        "# test_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\r\n",
        "#          \"relation-classification\", \"text-classification\"]\r\n",
        "#test_list_of_folders = [\"entity-linking\", \"face-alignment\", \"face-detection\", \"natural-language-inference\"]\r\n",
        "test_list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\r\n",
        "                   \"data-to-text_generation\",\"dependency_parsing\",\r\n",
        "                   \"document_classification\",\"entity_linking\",\r\n",
        "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\r\n",
        "                   \"natural_language_inference\"]\r\n",
        "for fls in test_list_of_folders:\r\n",
        "  for i in os.listdir(test_input_dir + fls + '/'):\r\n",
        "    for ii in os.listdir(test_input_dir + fls + '/' + str(i) + \"/\"):\r\n",
        "      if(ii==\"entities.txt\"):\r\n",
        "       os.remove(test_input_dir + fls + '/' + str(i) + \"/\" + \"entities.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCNP8QFpMFJ0",
        "outputId": "e107b4ad-83b3-42a3-84da-44e87cd11c23"
      },
      "source": [
        "import copy\r\n",
        "print(len(test_task2_in))\r\n",
        "print(len(test_task2_in[0]))\r\n",
        "print(test_task2_in[0])\r\n",
        "print(len(Capital_test_task2_in))\r\n",
        "print(len(Capital_test_task2_in[0]))\r\n",
        "print(Capital_test_task2_in[0])\r\n",
        "print(test_file_name_list[0])\r\n",
        "print(len(test_task2_out_label))\r\n",
        "print(len(test_task2_out_label[0]))\r\n",
        "print((test_task2_out_label[0]))\r\n",
        "print((test_task2_in[0][2]))\r\n",
        "print(len(test_task2_out_label[0][2]))\r\n",
        "print(len(test_task2_in[0][2].split()))\r\n",
        "\r\n",
        "output_dir = \"/content/drive/MyDrive/submission8/\"\r\n",
        "\r\n",
        "phrase_storer = []\r\n",
        "f = open(\"demofile3.txt\", \"w\")\r\n",
        "for i,file in enumerate(test_task2_in):\r\n",
        "  \r\n",
        "  print(test_file_name_list[i])\r\n",
        "  # filenametemp = \"/content/\" + test_file_name_list[i]\r\n",
        "  # %mkdir $filenametemp\r\n",
        "  f1 = open(output_dir + test_file_name_list[i] + \"/entities.txt\", \"w\")\r\n",
        "\r\n",
        "  for j,sent in enumerate(file):\r\n",
        "\r\n",
        "    biluo_list = (test_task2_out_label[i][j])[1:-1]\r\n",
        "    respective_sentence = Capital_test_task2_in[i][j].split()\r\n",
        "    sentence_number = (list_of_dict_for_sentence_to_number[i])[test_task2_in[i][j]]\r\n",
        "\r\n",
        "    if(len(respective_sentence) != len(biluo_list)):\r\n",
        "      print(\"Length mismatch in the sentence and BILUO sequence\")\r\n",
        "      continue\r\n",
        "\r\n",
        "    temp_phrase_storer = []\r\n",
        "    temp_phrase = []\r\n",
        "    count_of_words_in_sentence = 0\r\n",
        "    for k in zip(biluo_list,respective_sentence):\r\n",
        "\r\n",
        "      if (k[0]==\"U\"):\r\n",
        "        temp_phrase_storer = temp_phrase_storer + [k[1]]\r\n",
        "\r\n",
        "        start_of_word = 0\r\n",
        "        if(count_of_words_in_sentence == 0):\r\n",
        "          start_of_word = 0\r\n",
        "        else:  \r\n",
        "          start_of_word = len((\" \".join(respective_sentence[0:count_of_words_in_sentence])).strip() + \" \") \r\n",
        "\r\n",
        "        end_of_word = start_of_word + len(k[1].strip())\r\n",
        "\r\n",
        "        f.write(str(sentence_number) + \"\\t\" +  str(start_of_word) + \"\\t\" + str(end_of_word) + \"\\t\" + k[1].strip() + \"\\n\")\r\n",
        "        f1.write(str(sentence_number) + \"\\t\" +  str(start_of_word) + \"\\t\" + str(end_of_word) + \"\\t\" + k[1].strip() + \"\\n\")\r\n",
        "\r\n",
        "      elif (k[0]==\"B\"):\r\n",
        "        temp_phrase = temp_phrase + [k[1]] \r\n",
        "\r\n",
        "      elif (k[0]==\"I\"):\r\n",
        "        temp_phrase = temp_phrase + [\" \", k[1]]\r\n",
        "\r\n",
        "      elif (k[0]==\"L\"):\r\n",
        "        temp_phrase = temp_phrase + [\" \", k[1]]\r\n",
        "\r\n",
        "        end_of_words = len((\" \".join(respective_sentence[0:count_of_words_in_sentence])).strip() + \" \") + len(respective_sentence[count_of_words_in_sentence].strip())\r\n",
        "        start_of_words = end_of_words - len((\"\".join(temp_phrase)).strip())\r\n",
        "\r\n",
        "        f.write(str(sentence_number) + \"\\t\" + str(start_of_words) + \"\\t\" + str(end_of_words) + \"\\t\" + (\"\".join(temp_phrase)).strip() + \"\\n\")\r\n",
        "        f1.write(str(sentence_number) + \"\\t\" + str(start_of_words) + \"\\t\" + str(end_of_words) + \"\\t\" + (\"\".join(temp_phrase)).strip() + \"\\n\")\r\n",
        "        temp_phrase_storer =  temp_phrase_storer + copy.deepcopy([\"\".join(temp_phrase)])\r\n",
        "        temp_phrase = []\r\n",
        "\r\n",
        "      count_of_words_in_sentence += 1 \r\n",
        "\r\n",
        "  f1.close()  \r\n",
        "  phrase_storer =  phrase_storer + copy.deepcopy([temp_phrase_storer])\r\n",
        "  print(\"done\")\r\n",
        "\r\n",
        "print(\"Here: \",len(phrase_storer))    \r\n",
        "\r\n",
        "f.close()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "155\n",
            "13\n",
            "['parsing as language modeling', 'we recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency penn treebank parsing - 93.8 f 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .', 'when trees are converted to stanford dependencies , uas and las are 95.9 % and 94.1 % .', 'in this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 f 1 , with a comparatively simple architecture .', 'language modeling', 'in this paper , we build a parsing model based on the lstm - lm of .', 'the forget gate bias is initialized to be one and the rest of model parameters are sampled from u ( ? 0.05 , 0.05 ) .', 'dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .', 'the learning rate is 0.25 0.85 max where is an epoch number .', 'as shown in , with 92.6 f 1 lstm - lm ( g ) outperforms an ensemble of five mtps and rnng , both of which are trained on the wsj only .', 'lstm - lm ( gs ) outperforms all the other parsers with 93.1 f 1 .', 'a single lstm - lm ( gs ) together with charniak ( gs ) reaches 93.6 and an ensemble of eight lstm - lms ( gs ) with charniak ( gs ) achieves a new state of the art , 93.8 f 1 .', 'when trees are converted to stanford dependencies , 5 uas and las are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser .']\n",
            "155\n",
            "13\n",
            "['Parsing as Language Modeling', 'We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .', 'When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .', 'In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .', 'Language Modeling', 'In this paper , we build a parsing model based on the LSTM - LM of .', 'The forget gate bias is initialized to be one and the rest of model parameters are sampled from U ( ? 0.05 , 0.05 ) .', 'Dropout is applied to non-recurrent connections and gradients are clipped when their norm is bigger than 20 .', 'The learning rate is 0.25 0.85 max where is an epoch number .', 'As shown in , with 92.6 F 1 LSTM - LM ( G ) outperforms an ensemble of five MTPs and RNNG , both of which are trained on the WSJ only .', 'LSTM - LM ( GS ) outperforms all the other parsers with 93.1 F 1 .', 'A single LSTM - LM ( GS ) together with Charniak ( GS ) reaches 93.6 and an ensemble of eight LSTM - LMs ( GS ) with Charniak ( GS ) achieves a new state of the art , 93.8 F 1 .', 'When trees are converted to Stanford dependencies , 5 UAS and LAS are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser .']\n",
            "constituency_parsing/6\n",
            "155\n",
            "13\n",
            "[['O', 'B', 'I', 'I', 'L', 'O'], ['O', 'O', 'U', 'B', 'L', 'U', 'O', 'B', 'I', 'L', 'O', 'U', 'O', 'O', 'O', 'O', 'I', 'O', 'O', 'B', 'L', 'O', 'B', 'O', 'I', 'I', 'L', 'U', 'B', 'I', 'I', 'L', 'O', 'O', 'I', 'L', 'O', 'O', 'O', 'O', 'U', 'B', 'I', 'L', 'U', 'O', 'O', 'U', 'O', 'U', 'O', 'O', 'U', 'O', 'O'], ['O', 'O', 'U', 'O', 'B', 'L', 'B', 'L', 'O', 'B', 'I', 'L', 'U', 'B', 'I', 'I', 'I', 'L', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U', 'O', 'B', 'I', 'I', 'I', 'L', 'O', 'U', 'B', 'I', 'L', 'O', 'B', 'I', 'L', 'O', 'O', 'O', 'B', 'I', 'L', 'O', 'O'], ['O', 'B', 'L', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'U', 'O', 'B', 'L', 'B', 'L', 'O', 'B', 'I', 'L', 'O', 'O', 'O'], ['O', 'O', 'B', 'I', 'L', 'O', 'B', 'I', 'L', 'U', 'O', 'O', 'B', 'I', 'I', 'L', 'O', 'B', 'L', 'B', 'I', 'I', 'I', 'I', 'I', 'L', 'O', 'O'], ['O', 'U', 'O', 'B', 'L', 'B', 'I', 'O', 'L', 'U', 'U', 'U', 'O', 'U', 'O', 'B', 'I', 'L', 'O', 'O'], ['O', 'O', 'B', 'L', 'U', 'B', 'I', 'L', 'O', 'O', 'O', 'B', 'L', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'U', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'L', 'U', 'O', 'B', 'I', 'I', 'I', 'I', 'L', 'O', 'O', 'O', 'O', 'O', 'B', 'L', 'O', 'O', 'O', 'O', 'O'], ['O', 'B', 'I', 'I', 'I', 'I', 'L', 'U', 'B', 'I', 'I', 'L', 'U', 'B', 'I', 'L', 'O', 'O'], ['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'L', 'B', 'L', 'B', 'I', 'I', 'L', 'U', 'U', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'L', 'U', 'B', 'I', 'I', 'L', 'U', 'O', 'B', 'I', 'I', 'I', 'L', 'O', 'B', 'I', 'L', 'O', 'O'], ['O', 'O', 'U', 'O', 'B', 'L', 'B', 'L', 'O', 'B', 'I', 'I', 'L', 'U', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'L', 'U', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'L', 'O', 'O']]\n",
            "when trees are converted to stanford dependencies , uas and las are 95.9 % and 94.1 % .\n",
            "20\n",
            "18\n",
            "constituency_parsing/6\n",
            "done\n",
            "constituency_parsing/5\n",
            "done\n",
            "constituency_parsing/8\n",
            "done\n",
            "constituency_parsing/7\n",
            "done\n",
            "constituency_parsing/0\n",
            "done\n",
            "constituency_parsing/4\n",
            "done\n",
            "constituency_parsing/3\n",
            "done\n",
            "constituency_parsing/2\n",
            "done\n",
            "constituency_parsing/1\n",
            "done\n",
            "coreference_resolution/9\n",
            "done\n",
            "coreference_resolution/6\n",
            "done\n",
            "coreference_resolution/8\n",
            "done\n",
            "coreference_resolution/7\n",
            "done\n",
            "coreference_resolution/4\n",
            "done\n",
            "coreference_resolution/2\n",
            "done\n",
            "coreference_resolution/5\n",
            "done\n",
            "coreference_resolution/3\n",
            "done\n",
            "coreference_resolution/1\n",
            "done\n",
            "coreference_resolution/0\n",
            "done\n",
            "data-to-text_generation/6\n",
            "done\n",
            "data-to-text_generation/2\n",
            "done\n",
            "data-to-text_generation/4\n",
            "done\n",
            "data-to-text_generation/1\n",
            "done\n",
            "data-to-text_generation/3\n",
            "done\n",
            "data-to-text_generation/5\n",
            "done\n",
            "data-to-text_generation/0\n",
            "done\n",
            "dependency_parsing/6\n",
            "done\n",
            "dependency_parsing/0\n",
            "done\n",
            "dependency_parsing/1\n",
            "done\n",
            "dependency_parsing/7\n",
            "done\n",
            "dependency_parsing/8\n",
            "done\n",
            "dependency_parsing/2\n",
            "done\n",
            "dependency_parsing/5\n",
            "done\n",
            "dependency_parsing/4\n",
            "done\n",
            "dependency_parsing/3\n",
            "done\n",
            "document_classification/9\n",
            "done\n",
            "document_classification/3\n",
            "done\n",
            "document_classification/6\n",
            "done\n",
            "document_classification/2\n",
            "done\n",
            "document_classification/8\n",
            "done\n",
            "document_classification/18\n",
            "done\n",
            "document_classification/19\n",
            "done\n",
            "document_classification/4\n",
            "done\n",
            "document_classification/7\n",
            "done\n",
            "document_classification/5\n",
            "done\n",
            "document_classification/20\n",
            "done\n",
            "document_classification/0\n",
            "done\n",
            "document_classification/1\n",
            "done\n",
            "document_classification/14\n",
            "done\n",
            "document_classification/17\n",
            "done\n",
            "document_classification/12\n",
            "done\n",
            "document_classification/15\n",
            "done\n",
            "document_classification/16\n",
            "done\n",
            "document_classification/10\n",
            "done\n",
            "document_classification/11\n",
            "done\n",
            "document_classification/13\n",
            "done\n",
            "entity_linking/3\n",
            "done\n",
            "entity_linking/2\n",
            "done\n",
            "entity_linking/5\n",
            "done\n",
            "entity_linking/9\n",
            "done\n",
            "entity_linking/4\n",
            "done\n",
            "entity_linking/7\n",
            "done\n",
            "entity_linking/6\n",
            "done\n",
            "entity_linking/8\n",
            "done\n",
            "entity_linking/11\n",
            "done\n",
            "entity_linking/16\n",
            "done\n",
            "entity_linking/0\n",
            "done\n",
            "entity_linking/10\n",
            "done\n",
            "entity_linking/1\n",
            "done\n",
            "entity_linking/13\n",
            "done\n",
            "entity_linking/15\n",
            "done\n",
            "entity_linking/14\n",
            "done\n",
            "entity_linking/12\n",
            "done\n",
            "face_alignment/3\n",
            "done\n",
            "face_alignment/7\n",
            "done\n",
            "face_alignment/8\n",
            "done\n",
            "face_alignment/5\n",
            "done\n",
            "face_alignment/4\n",
            "done\n",
            "face_alignment/9\n",
            "done\n",
            "face_alignment/6\n",
            "done\n",
            "face_alignment/2\n",
            "done\n",
            "face_alignment/10\n",
            "done\n",
            "face_alignment/12\n",
            "done\n",
            "face_alignment/17\n",
            "done\n",
            "face_alignment/15\n",
            "done\n",
            "face_alignment/13\n",
            "done\n",
            "face_alignment/11\n",
            "done\n",
            "face_alignment/18\n",
            "done\n",
            "face_alignment/16\n",
            "done\n",
            "face_alignment/14\n",
            "done\n",
            "face_alignment/0\n",
            "done\n",
            "face_alignment/1\n",
            "done\n",
            "face_detection/21\n",
            "done\n",
            "face_detection/9\n",
            "done\n",
            "face_detection/4\n",
            "done\n",
            "face_detection/20\n",
            "done\n",
            "face_detection/3\n",
            "done\n",
            "face_detection/6\n",
            "done\n",
            "face_detection/5\n",
            "done\n",
            "face_detection/7\n",
            "done\n",
            "face_detection/8\n",
            "done\n",
            "face_detection/19\n",
            "done\n",
            "face_detection/12\n",
            "done\n",
            "face_detection/16\n",
            "done\n",
            "face_detection/14\n",
            "done\n",
            "face_detection/2\n",
            "done\n",
            "face_detection/15\n",
            "done\n",
            "face_detection/13\n",
            "done\n",
            "face_detection/17\n",
            "done\n",
            "face_detection/18\n",
            "done\n",
            "face_detection/11\n",
            "done\n",
            "face_detection/10\n",
            "done\n",
            "face_detection/1\n",
            "done\n",
            "face_detection/0\n",
            "done\n",
            "hypernym_discovery/7\n",
            "done\n",
            "hypernym_discovery/2\n",
            "done\n",
            "hypernym_discovery/4\n",
            "done\n",
            "hypernym_discovery/8\n",
            "done\n",
            "hypernym_discovery/5\n",
            "done\n",
            "hypernym_discovery/1\n",
            "done\n",
            "hypernym_discovery/6\n",
            "done\n",
            "hypernym_discovery/3\n",
            "done\n",
            "hypernym_discovery/0\n",
            "done\n",
            "natural_language_inference/4\n",
            "done\n",
            "natural_language_inference/5\n",
            "done\n",
            "natural_language_inference/30\n",
            "done\n",
            "natural_language_inference/6\n",
            "done\n",
            "natural_language_inference/8\n",
            "done\n",
            "natural_language_inference/9\n",
            "done\n",
            "natural_language_inference/7\n",
            "done\n",
            "natural_language_inference/29\n",
            "done\n",
            "natural_language_inference/3\n",
            "done\n",
            "natural_language_inference/31\n",
            "done\n",
            "natural_language_inference/2\n",
            "done\n",
            "natural_language_inference/22\n",
            "done\n",
            "natural_language_inference/26\n",
            "done\n",
            "natural_language_inference/21\n",
            "done\n",
            "natural_language_inference/24\n",
            "done\n",
            "natural_language_inference/23\n",
            "done\n",
            "natural_language_inference/28\n",
            "done\n",
            "natural_language_inference/20\n",
            "done\n",
            "natural_language_inference/25\n",
            "done\n",
            "natural_language_inference/27\n",
            "done\n",
            "natural_language_inference/16\n",
            "done\n",
            "natural_language_inference/10\n",
            "done\n",
            "natural_language_inference/19\n",
            "done\n",
            "natural_language_inference/15\n",
            "done\n",
            "natural_language_inference/17\n",
            "done\n",
            "natural_language_inference/11\n",
            "done\n",
            "natural_language_inference/13\n",
            "done\n",
            "natural_language_inference/12\n",
            "done\n",
            "natural_language_inference/18\n",
            "done\n",
            "natural_language_inference/14\n",
            "done\n",
            "natural_language_inference/1\n",
            "done\n",
            "natural_language_inference/0\n",
            "done\n",
            "Here:  155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KrP0XYR7C38"
      },
      "source": [
        "#Evaluation Script for Phrases\r\n",
        "\r\n",
        "This is the script provided by the organizers for phrase evaluation.\r\n",
        "The first argument is the path to the folder containing true labels, second argument is the path to the folder containing predicted labels,\r\n",
        "third argument is the path to the folder where the \"scores.txt\" will be stored.\r\n",
        "###**IMPORTANT**\r\n",
        "\r\n",
        "Please download the evaluation_for_phrases.py file and place it in /content folder and direct it appropriately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iM7tNohcZs8"
      },
      "source": [
        "!python /content/drive/MyDrive/CS779_All_Subtasks/evaluation_for_phrases.py \"/content/drive/MyDrive/test\" \"/content/\" \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}