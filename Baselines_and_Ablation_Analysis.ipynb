{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baselines and Ablation_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9IMLZXsvYZ2"
      },
      "source": [
        "# **Baselines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-VTYNj6QWs7"
      },
      "source": [
        "#### Load the required packages\r\n",
        "import os\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8Z1f5clQRxr"
      },
      "source": [
        "#### Point it to the directory where your Test Data is present\r\n",
        "input_dir = \"/content/drive/MyDrive/sub3_ph22/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLLi78yKwqc"
      },
      "source": [
        "#### List all the folders present in the directory\r\n",
        "list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\r\n",
        "                   \"data-to-text_generation\",\"dependency_parsing\",\r\n",
        "                   \"document_classification\",\"entity_linking\",\r\n",
        "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\r\n",
        "                   \"natural_language_inference\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-CuBXj1vj39"
      },
      "source": [
        "#### Load the Test Data\r\n",
        "\r\n",
        "input_stanza_list = [] #Stores individual lines from Stanza_out.txt file\r\n",
        "input_stanza_len = [] #Stores the number of lines present in each Stanza_out.txt file \r\n",
        "input_sent_num_list = [] #Stores individual lines from sentences.txt file\r\n",
        "file_name_list = [] #Stores the name of individual files\r\n",
        "input_entity_list = [] #Stores the list of all phrases from entities.txt\r\n",
        "\r\n",
        "\r\n",
        "for fls in list_of_folders:\r\n",
        "  count=0\r\n",
        "  for i in os.listdir(input_dir + fls + '/'):\r\n",
        "    for files in os.listdir(input_dir + fls + '/' + i + \"/\"):\r\n",
        "      if files.endswith(\"Stanza-out.txt\"):\r\n",
        "        stanza_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        stanza_lines = stanza_file.read()\r\n",
        "        stanza_lines_list = list(filter(None,map(lambda x:x,stanza_lines.lower().splitlines()))) # filter empty strings and split into lines\r\n",
        "        input_stanza_len.append(len(stanza_lines_list))\r\n",
        "        input_stanza_list.append(stanza_lines_list)  \r\n",
        "      if files.endswith(\"sentences.txt\"):\r\n",
        "        sentence_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\r\n",
        "        input_sent_num_list.append(sentence_num_list)\r\n",
        "      if files.endswith(\"entities.txt\"):\r\n",
        "        entities_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        entities_list = list(filter(None,(entities_file.read()).splitlines())) # filter empty strings and split into lines\r\n",
        "        input_entity_list.append(entities_list)\r\n",
        "    count=count+1\r\n",
        "    temp_list_code_triple = []\r\n",
        "    \r\n",
        "    file_name_list.append(fls + '/' + str(i))\r\n",
        "  print(\"completed\",fls, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8qGeYcbwo_R"
      },
      "source": [
        "print(len(file_name_list))\r\n",
        "print()\r\n",
        "print(file_name_list[1])\r\n",
        "print()\r\n",
        "print(len(input_stanza_list))\r\n",
        "print()\r\n",
        "print(input_stanza_list[1])\r\n",
        "print()\r\n",
        "print(len(input_entity_list))\r\n",
        "print()\r\n",
        "print(input_entity_list[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEb1p6AgxA86"
      },
      "source": [
        "#### Checks if that sentence has a punctuation or not.\r\n",
        "#### We extract out headings from stanza_out.txt file by checking if that sentence contains a punctuation or not.\r\n",
        "import string\r\n",
        "result = string.punctuation\r\n",
        "def check_punctuation(sentence):\r\n",
        "  flag = 0\r\n",
        "  for i in sentence: \r\n",
        "    if i in result:   \r\n",
        "        flag = 1\r\n",
        "        break\r\n",
        "  if flag == 1:     \r\n",
        "    return 1\r\n",
        "  else:\r\n",
        "    return 0         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFtlzx88w2tC"
      },
      "source": [
        "#### This block finds out all the files and the corresponding sentence numbers which should belong to Baselines Information unit\r\n",
        "#### We store all the sentence numbers between a \"Baseline\" heading and the next consequetive heading (basically all sentences in the \"baseline\" paragraph)\r\n",
        "\r\n",
        "regex = r\"comp\" #Most of the baseline heading have \"comp\" as starting word. Eg. \"Comparision\", \"Compare\", \"Competitive\", etc.\r\n",
        "regex4  = r\"baseline\" #Most of the headings also have \"baseline\" word in them\r\n",
        "regex5 = r\"compr\" #We do not want words such as \"compressed\", \"compromised\", etc.\r\n",
        "regex6 = r\"compu\" #We do not want words such as \"computed\", \"computes\", etc.\r\n",
        "regex7 = r\"compl\" #We do not want words such as \"complain\", \"comply\", etc.\r\n",
        "regex8 = r\"compo\" #We do not want words such as \"composed\", \"composition\", etc.\r\n",
        "c3 = 0 #Keeps a count of all baseline headings found\r\n",
        "\r\n",
        "dict_of_baselines = {} #Stores the filename as key and the eligible baseline sentence numbers as values \r\n",
        "\r\n",
        "for i in zip(input_stanza_list, file_name_list):\r\n",
        "  print(i[1]) #Prints file name\r\n",
        "  found1 = 0\r\n",
        "  for counterr, j in enumerate(i[0]):\r\n",
        "    flag = check_punctuation(j) #This line checks if that sentence is a heading or not by using the punctuation heurisitic \r\n",
        "    if (flag==0): \r\n",
        "      if (found1): #found1 will be 1 when we had previously found a baseline heading. So in this block, we add all the sentences between the two headings(i.e. the baseline heading and the next heading found)\r\n",
        "        if (i[1] not in dict_of_baselines.keys()):\r\n",
        "          dict_of_baselines[i[1]] = [] #Make a new empty list in that key if it was not already present\r\n",
        "        dict_of_baselines[i[1]] = dict_of_baselines[i[1]] + list(range(found1+1, counterr+1)) #Add all the sentence numbers between the previous baseline heading and the current found heading\r\n",
        "        found1 = 0 #Again put it to its default value\r\n",
        "\r\n",
        "      split_list = j.split() \r\n",
        "      search_for_comp = re.search(regex, j)\r\n",
        "      search_for_baseline = re.search(regex4, j)\r\n",
        "      search_for_compr = re.search(regex5, j)\r\n",
        "      search_for_compu = re.search(regex6, j)\r\n",
        "      search_for_compl = re.search(regex7, j)\r\n",
        "      search_for_compo = re.search(regex8, j)\r\n",
        "      #The Heading is a Baseline Heading if it contains either \"comp\" or \"baseline\" and is not something having \"compr\", \"compu\", \"compl\", \"compo\".\r\n",
        "      if ((search_for_baseline) or (search_for_comp)) and (search_for_compr==None) and (search_for_compu==None) and (search_for_compl==None) and (search_for_compo==None):\r\n",
        "        print(\"This is a baseline heading\")\r\n",
        "        print(j)\r\n",
        "        c3 += 1\r\n",
        "        found1 = counterr #Store the sentence number of that Heading\r\n",
        "\r\n",
        "          \r\n",
        "print(c3)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jvOD-MMyKcv"
      },
      "source": [
        "#### For reference\r\n",
        "print(len(dict_of_baselines))\r\n",
        "print()\r\n",
        "print(dict_of_baselines[\"natural_language_inference/1\"])\r\n",
        "print()\r\n",
        "print(dict_of_baselines[\"face_alignment/4\"])\r\n",
        "print()\r\n",
        "print(dict_of_baselines[\"document_classification/18\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1zy8i8XzQun"
      },
      "source": [
        "#### Out of the selected baseline sentences, only those sentences are used which are contribution sentences(i.e. in sentences.txt file)\r\n",
        "dict_of_selected_lines_for_baseline = {}\r\n",
        "for i in zip(file_name_list, input_sent_num_list):\r\n",
        "  if (i[0] in dict_of_baselines.keys()):\r\n",
        "    dict_of_selected_lines_for_baseline[i[0]] = []\r\n",
        "    for j in i[1]: \r\n",
        "      if ((int)(j) in dict_of_baselines[i[0]]): #Main checking condition where the baseline sentence is included only if it is a contribution sentence\r\n",
        "        dict_of_selected_lines_for_baseline[i[0]] = dict_of_selected_lines_for_baseline[i[0]] + [(int)(j)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-dtCubAzQuz"
      },
      "source": [
        "#### For reference\r\n",
        "print(len(dict_of_selected_lines_for_baseline))\r\n",
        "print()\r\n",
        "print(dict_of_selected_lines_for_baseline[\"natural_language_inference/1\"])\r\n",
        "print()\r\n",
        "print(dict_of_selected_lines_for_baseline[\"document_classification/18\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7rbGMQyyh6S"
      },
      "source": [
        "##### Phrases arrangement in ascending order according to their sentence numbers and their starting character numbers \r\n",
        "import copy\r\n",
        "train_final_phrases_list = []\r\n",
        "for i in range(len(input_stanza_list)):\r\n",
        "  entity_list = [j.split('\\t') for j in input_entity_list[i]]\r\n",
        "  entity_list.sort(key=lambda x: (int(x[0]),int(x[1]))) \r\n",
        "  train_final_phrases_list = train_final_phrases_list + [entity_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0mNx0haysLK"
      },
      "source": [
        "#### For reference\r\n",
        "print(len(train_final_phrases_list))\r\n",
        "print()\r\n",
        "print(train_final_phrases_list[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trp-GyTHyzSy"
      },
      "source": [
        "#### Here we form the triples using the rule that all three consequetive triples in the same sentence are treated as a triple.\r\n",
        "baseline_predicted_triples = [] #Stores the triples formed for each file \r\n",
        "\r\n",
        "for j, name_of_file in enumerate(file_name_list):\r\n",
        "  temp_store_list = []\r\n",
        "  if name_of_file in dict_of_selected_lines_for_baseline.keys(): \r\n",
        "    for i in range(len(train_final_phrases_list[j])):\r\n",
        "      if (int)(train_final_phrases_list[j][i][0]) in dict_of_selected_lines_for_baseline[name_of_file]: #Form triples only for those files in which \"baselines\" sentences are found\r\n",
        "        try:\r\n",
        "          #Check if the sentence number is same for the three consequetive phrases\r\n",
        "          if((train_final_phrases_list[j][i][0]==train_final_phrases_list[j][i+1][0])  and (train_final_phrases_list[j][i][0]==train_final_phrases_list[j][i+2][0])):\r\n",
        "            temp_store_list = temp_store_list + [[train_final_phrases_list[j][i][3], train_final_phrases_list[j][i+1][3], train_final_phrases_list[j][i+2][3]]]\r\n",
        "        except Exception as e:\r\n",
        "          continue\r\n",
        "  baseline_predicted_triples = baseline_predicted_triples + [temp_store_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oaS_SOAyzS2"
      },
      "source": [
        "#### For reference\r\n",
        "print(len(baseline_predicted_triples))\r\n",
        "print()\r\n",
        "print([j for j in baseline_predicted_triples if len(j)!=0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4txnsRGzkdH"
      },
      "source": [
        "#### Write the formed triples only in those files which have Baselines information unit already predicted in them\r\n",
        "#### Note that we are not predicting Baselines Information unit.\r\n",
        "#### We are only forming triples for the files which have Baseline Information Unit already predicted in them (from the Information unit prediction step). \r\n",
        "for i in zip(file_name_list, baseline_predicted_triples):\r\n",
        "  #Check whether we have formed baseline triples for this file AND whether this file already contains Baseline Information Unit.\r\n",
        "  if i[0] in dict_of_selected_lines_for_baseline.keys() and (\"baselines.txt\" in os.listdir(input_dir+i[0]+\"/\"+\"triples/\")): \r\n",
        "    output_file = open(input_dir + i[0] + \"/\" + \"triples/\" + \"baselines.txt\",\"w\")\r\n",
        "    output_file.write(\"(Contribution||has||Baselines)\" + \"\\n\")\r\n",
        "    for j in i[1]:\r\n",
        "      output_file.write(\"(\" + j[0] + \"||\" + j[1] + \"||\" + j[2]  + \")\" + \"\\n\")     \r\n",
        "    output_file.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xlnc3mkg7rt"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "# **Ablation Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8He5tYGb82l7"
      },
      "source": [
        "#### Load the required packages\r\n",
        "import os\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWCWVST982mE"
      },
      "source": [
        "#### Point it to the directory where your Test Data is present\r\n",
        "input_dir = \"/content/drive/MyDrive/sub3_ph22/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i4jLOKM82mF"
      },
      "source": [
        "#### List all the folders present in the directory\r\n",
        "list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\r\n",
        "                   \"data-to-text_generation\",\"dependency_parsing\",\r\n",
        "                   \"document_classification\",\"entity_linking\",\r\n",
        "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\r\n",
        "                   \"natural_language_inference\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlAw9D1n82mG"
      },
      "source": [
        "#### Load the Test Data\r\n",
        "\r\n",
        "input_stanza_list = [] #Stores individual lines from Stanza_out.txt file\r\n",
        "input_stanza_len = [] #Stores the number of lines present in each Stanza_out.txt file \r\n",
        "input_sent_num_list = [] #Stores individual lines from sentences.txt file\r\n",
        "file_name_list = [] #Stores the name of individual files\r\n",
        "input_entity_list = [] #Stores the list of all phrases from entities.txt\r\n",
        "\r\n",
        "\r\n",
        "for fls in list_of_folders:\r\n",
        "  count=0\r\n",
        "  for i in os.listdir(input_dir + fls + '/'):\r\n",
        "    for files in os.listdir(input_dir + fls + '/' + i + \"/\"):\r\n",
        "      if files.endswith(\"Stanza-out.txt\"):\r\n",
        "        stanza_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        stanza_lines = stanza_file.read()\r\n",
        "        stanza_lines_list = list(filter(None,map(lambda x:x,stanza_lines.lower().splitlines()))) # filter empty strings and split into lines\r\n",
        "        input_stanza_len.append(len(stanza_lines_list))\r\n",
        "        input_stanza_list.append(stanza_lines_list)  \r\n",
        "      if files.endswith(\"sentences.txt\"):\r\n",
        "        sentence_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\r\n",
        "        input_sent_num_list.append(sentence_num_list)\r\n",
        "      if files.endswith(\"entities.txt\"):\r\n",
        "        entities_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        entities_list = list(filter(None,(entities_file.read()).splitlines())) # filter empty strings and split into lines\r\n",
        "        input_entity_list.append(entities_list)\r\n",
        "    count=count+1\r\n",
        "    temp_list_code_triple = []\r\n",
        "    \r\n",
        "    file_name_list.append(fls + '/' + str(i))\r\n",
        "  print(\"completed\",fls, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGNDfs1682mG"
      },
      "source": [
        "print(len(file_name_list))\r\n",
        "print()\r\n",
        "print(file_name_list[1])\r\n",
        "print()\r\n",
        "print(len(input_stanza_list))\r\n",
        "print()\r\n",
        "print(input_stanza_list[1])\r\n",
        "print()\r\n",
        "print(len(input_entity_list))\r\n",
        "print()\r\n",
        "print(input_entity_list[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYikLWKO82mH"
      },
      "source": [
        "#### Checks if that sentence has a punctuation or not.\r\n",
        "#### We extract out headings from stanza_out.txt file by checking if that sentence contains a punctuation or not.\r\n",
        "import string\r\n",
        "result = string.punctuation\r\n",
        "def check_punctuation(sentence):\r\n",
        "  flag = 0\r\n",
        "  for i in sentence: \r\n",
        "    if i in result:   \r\n",
        "        flag = 1\r\n",
        "        break\r\n",
        "  if flag == 1:     \r\n",
        "    return 1\r\n",
        "  else:\r\n",
        "    return 0         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb0eP4kYZMk3"
      },
      "source": [
        "#### This block finds out all the files and the corresponding sentence numbers which should belong to Ablation Analysis Information unit\r\n",
        "#### We store all the sentence numbers between an \"Ablation Analysis\" heading and the next consequetive heading (basically all sentences in the \"Ablation Analysis\" paragraph)\r\n",
        "\r\n",
        "regex = r\"ablation\" #Most of the Ablation Analysis headings have \"ablation\" as starting word.\r\n",
        "c3 = 0 #Keeps a count of all Ablation Analysis headings found\r\n",
        "\r\n",
        "dict_of_baselines = {} #Stores the filename as key and the eligible Ablation Analysis sentence numbers as values \r\n",
        "\r\n",
        "for i in zip(input_stanza_list, file_name_list):\r\n",
        "  print(i[1]) #Prints file name\r\n",
        "  found1 = 0\r\n",
        "  for counterr, j in enumerate(i[0]):\r\n",
        "    flag = check_punctuation(j) #This line checks if that sentence is a heading or not by using the punctuation heurisitic. \r\n",
        "    if (flag==0): \r\n",
        "      if (found1): #found1 will be 1 when we had previously found an Ablation Analysis heading. So in this block, we add all the sentences between the two headings(i.e. the Ablation Analysis heading and the next heading found)\r\n",
        "        if (i[1] not in dict_of_baselines.keys()):\r\n",
        "          dict_of_baselines[i[1]] = [] #Make a new empty list in that key if it was not already present\r\n",
        "        dict_of_baselines[i[1]] = dict_of_baselines[i[1]] + list(range(found1+1, counterr+1)) #Add all the sentence numbers between the previous Ablation Analysis heading and the current found heading\r\n",
        "        found1 = 0 #Again put it to its default value\r\n",
        "\r\n",
        "      split_list = j.split() \r\n",
        "      search_for_ablation = re.search(regex, j)\r\n",
        "      \r\n",
        "      #The Heading is an Ablation Analysis Heading if it contains \"ablation\" word in it.\r\n",
        "      if (search_for_ablation):\r\n",
        "        print(\"This is an Ablation Analysis heading\")\r\n",
        "        print(j)\r\n",
        "        c3 += 1\r\n",
        "        found1 = counterr #Store the sentence number of that Heading\r\n",
        "\r\n",
        "          \r\n",
        "print(c3)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unsA1-JVc99D"
      },
      "source": [
        "#### Out of the selected Ablation Analysis sentences, only those sentences are used which are contribution sentences(i.e. in sentences.txt file)\r\n",
        "dict_of_selected_lines_for_baseline = {}\r\n",
        "for i in zip(file_name_list, input_sent_num_list):\r\n",
        "  if (i[0] in dict_of_baselines.keys()):\r\n",
        "    dict_of_selected_lines_for_baseline[i[0]] = []\r\n",
        "    for j in i[1]: \r\n",
        "      if ((int)(j) in dict_of_baselines[i[0]]): #Main checking condition where the Ablation Analysis sentence is included only if it is a contribution sentence\r\n",
        "        dict_of_selected_lines_for_baseline[i[0]] = dict_of_selected_lines_for_baseline[i[0]] + [(int)(j)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdVzPhasdFlV"
      },
      "source": [
        "##### Phrases arrangement in ascending order according to their sentence numbers and their starting character numbers \r\n",
        "import copy\r\n",
        "train_final_phrases_list = []\r\n",
        "for i in range(len(input_stanza_list)):\r\n",
        "  entity_list = [j.split('\\t') for j in input_entity_list[i]]\r\n",
        "  entity_list.sort(key=lambda x: (int(x[0]),int(x[1]))) \r\n",
        "  train_final_phrases_list = train_final_phrases_list + [entity_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBkgOfm_dPWR"
      },
      "source": [
        "#### Here we form the triples using the rule that all three consequetive triples in the same sentence are treated as a triple.\r\n",
        "baseline_predicted_triples = [] #Stores the triples formed for each file \r\n",
        "\r\n",
        "for j, name_of_file in enumerate(file_name_list):\r\n",
        "  temp_store_list = []\r\n",
        "  if name_of_file in dict_of_selected_lines_for_baseline.keys(): \r\n",
        "    for i in range(len(train_final_phrases_list[j])):\r\n",
        "      if (int)(train_final_phrases_list[j][i][0]) in dict_of_selected_lines_for_baseline[name_of_file]: #Form triples only for those files in which \"Ablation Analysis\" sentences are found\r\n",
        "        try:\r\n",
        "          #Checks if the sentence number is same for the three consequetive phrases\r\n",
        "          if((train_final_phrases_list[j][i][0]==train_final_phrases_list[j][i+1][0])  and (train_final_phrases_list[j][i][0]==train_final_phrases_list[j][i+2][0])):\r\n",
        "            temp_store_list = temp_store_list + [[train_final_phrases_list[j][i][3], train_final_phrases_list[j][i+1][3], train_final_phrases_list[j][i+2][3]]]\r\n",
        "        except Exception as e:\r\n",
        "          continue\r\n",
        "  baseline_predicted_triples = baseline_predicted_triples + [temp_store_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmJnRwTtvAqE"
      },
      "source": [
        "#### Write the formed triples only in those files which have Ablation Analysis information unit already predicted in them\r\n",
        "#### Note that we are not predicting Ablation Analysis Information unit.\r\n",
        "#### We are only forming triples for the files which have Ablation Analysis Information Unit already predicted in them (from the Information unit prediction step).\r\n",
        "for i in zip(file_name_list, baseline_predicted_triples):\r\n",
        "  #Checks whether we have formed Ablation Analysis triples for this file AND whether this file already contains Ablation Analysis Information Unit.\r\n",
        "  if i[0] in dict_of_selected_lines_for_baseline.keys() and (\"ablation-analysis.txt\" in os.listdir(input_dir+i[0]+\"/\"+\"triples/\")):\r\n",
        "    output_file = open(input_dir + i[0] + \"/\" + \"triples/\" + \"ablation-analysis.txt\",\"w\")\r\n",
        "    output_file.write(\"(Contribution||has||Ablation analysis)\" + \"\\n\")\r\n",
        "    for j in i[1]:\r\n",
        "      output_file.write(\"(\" + j[0] + \"||\" + j[1] + \"||\" + j[2]  + \")\" + \"\\n\")     \r\n",
        "    output_file.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}