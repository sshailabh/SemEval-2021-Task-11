{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Research_Problem and Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re8T7-D7J2Wd"
      },
      "source": [
        "# **Code.txt** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qare3uLHK89l"
      },
      "source": [
        "#### Load the required packages\r\n",
        "import os\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEYc1tZlKPA2"
      },
      "source": [
        "#### Point it to the directory where your Test Data is present\r\n",
        "input_dir = \"/content/drive/MyDrive/sub3_ph22/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLLi78yKwqc"
      },
      "source": [
        "#### List all the folders present in the directory\r\n",
        "list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\r\n",
        "                   \"data-to-text_generation\",\"dependency_parsing\",\r\n",
        "                   \"document_classification\",\"entity_linking\",\r\n",
        "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\r\n",
        "                   \"natural_language_inference\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF7weUGLKPA8"
      },
      "source": [
        "#### Load the Test Data\r\n",
        "input_stanza_list = [] #Stores individual lines from Stanza_out.txt file \r\n",
        "input_stanza_len = [] #Stores the number of lines present in Stanza_out.txt file \r\n",
        "input_sent_num_list = [] #Stores individual lines from sentences.txt file\r\n",
        "file_name_list = [] #Stores the name of individual files\r\n",
        "\r\n",
        "for fls in list_of_folders:\r\n",
        "  count=0\r\n",
        "  for i in os.listdir(input_dir + fls + '/'):\r\n",
        "    for files in os.listdir(input_dir + fls + '/' + i + \"/\"):\r\n",
        "      if files.endswith(\"Stanza-out.txt\"):\r\n",
        "        stanza_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        stanza_lines = stanza_file.read()\r\n",
        "        stanza_lines_list = list(filter(None,map(lambda x:x,stanza_lines.splitlines()))) # filter empty strings and split into lines\r\n",
        "        input_stanza_len.append(len(stanza_lines_list))\r\n",
        "        input_stanza_list.append(stanza_lines_list)\r\n",
        "      if files.endswith(\"sentences.txt\"):\r\n",
        "        sentence_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\r\n",
        "        input_sent_num_list.append(list(map(int, sentence_num_list)))\r\n",
        "    count=count+1\r\n",
        "         \r\n",
        "    file_name_list.append(fls + '/' + str(i))\r\n",
        "  print(\"completed\",fls, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch58RJM5LvAL"
      },
      "source": [
        "print(len(input_sent_num_list))\r\n",
        "print()\r\n",
        "print(input_sent_num_list[1])\r\n",
        "print()\r\n",
        "print(len(file_name_list))\r\n",
        "print()\r\n",
        "print(file_name_list[1])\r\n",
        "print()\r\n",
        "print(len(input_stanza_list))\r\n",
        "print()\r\n",
        "print(input_stanza_list[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHnvGotWNY9Y"
      },
      "source": [
        "#### Returns a list of urls present in a string using a regex expression. In case no url is present, it returns an empty list. \r\n",
        "def find_url(string): \r\n",
        "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\r\n",
        "    url = re.findall(regex,string)       \r\n",
        "    return [x[0] for x in url] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mImoePk0M3S-"
      },
      "source": [
        "#### The main function to identify where the Code information unit should be present and what should be its triples\r\n",
        "\r\n",
        "for i in zip(input_sent_num_list, input_stanza_list, file_name_list):\r\n",
        "  for sen_num in i[0]:\r\n",
        "\r\n",
        "    # Get the contribution sentence corresponding to its sentence number in sentences.txt file\r\n",
        "    j = i[1][(int)(sen_num)-1]  \r\n",
        "\r\n",
        "    # Now we remove all the useless spaces which can be present in our url (eg. \"https:// wikipedia .com\" converted to \"https://wikipedia.com\")   \r\n",
        "    # temp1 removes all leading and trailing spaces around \"/\"\r\n",
        "    temp1 = re.sub(r'(?:(?<=\\/) | (?=\\/))','',j)\r\n",
        "    # temp2 removes all leading and trailing spaces around \":\"\r\n",
        "    temp2 = re.sub(r'(?:(?<=\\:) | (?=\\:))','',temp1)\r\n",
        "    # temp3 removes all leading and trailing spaces around \".\"\r\n",
        "    temp3 = re.sub(r'(?:(?<=\\.) | (?=\\.))','',temp2)\r\n",
        "    # temp4 removes all leading and trailing spaces around \"-\"\r\n",
        "    temp4 = re.sub(r'(?:(?<=\\-) | (?=\\-))','',temp3)\r\n",
        "    # Now temp4 is used as the final sentence in which urls have to be found\r\n",
        "\r\n",
        "    # list_urls contains the list of urls found in that sentence\r\n",
        "    list_urls = find_url(temp4)\r\n",
        "\r\n",
        "    # If some url is found in that sentence, then proceed forward\r\n",
        "    if (len(list_urls)!=0): \r\n",
        "      # Only those urls considered in Code Information unit whose corresponding sentence consists of words like \"our\", \"code\", \"codes\"\r\n",
        "      if (\"our\" in j.lower().split(\" \")) or (\"code\" in j.lower().split(\" \"))or (\"codes\" in j.lower().split(\" \")):\r\n",
        "\r\n",
        "        # Now we identify the starting and ending character number of the url in its respective sentence\r\n",
        "        user_iterator = re.finditer('http', j.lower())\r\n",
        "\r\n",
        "        # This list consists of the starting character number of the urls\r\n",
        "        user_list = [user.start() for user in user_iterator]  \r\n",
        "        \r\n",
        "        for k in zip(user_list, list_urls):\r\n",
        "          start = k[0]\r\n",
        "          end = start \r\n",
        "          length_of_link = len(k[1])\r\n",
        "          while (end<len(j))and(length_of_link!=0):\r\n",
        "            # we ignore all the spaces occuring in between the url\r\n",
        "            if (j[end]!=\" \"):\r\n",
        "              length_of_link = length_of_link - 1\r\n",
        "            # \"end\" stores the ending character number of the urls\r\n",
        "            end = end + 1\r\n",
        "\r\n",
        "          # Check the url in the original sentence using start and end character number   \r\n",
        "          print(j[start:end])\r\n",
        "          # Print the respective file name also\r\n",
        "          print(i[2])\r\n",
        "\r\n",
        "          # Write the extracted url along with its starting and ending character number, in the entities.txt file \r\n",
        "          output_file1 = open(input_dir + i[2] + \"/entities.txt\",\"a\")\r\n",
        "          output_file1.write(str(sen_num) + \"\\t\" + str(start) + \"\\t\" + str(end) + \"\\t\" + j[start:end] + \"\\n\")\r\n",
        "          output_file1.close() \r\n",
        "          # Make and write the extracted url in the Code.txt file \r\n",
        "          output_file2 = open(input_dir + i[2] + \"/triples/\" + \"code.txt\",\"a\")\r\n",
        "          output_file2.write(\"(Contribution||Code||\" + j[start:end] + \")\" + \"\\n\")\r\n",
        "          output_file2.close()  \r\n",
        "\r\n",
        "        print()      \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lx8lIx7htEu"
      },
      "source": [
        "# **Research_problem.txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkzEBxJ8iDYj"
      },
      "source": [
        "#### Load the required packages\r\n",
        "import os\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUFlqfVTh7cv"
      },
      "source": [
        "#### Point it to the directory where your Test Data is present\r\n",
        "input_dir = \"/content/drive/MyDrive/sub3_ph22/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw8gTcBSiGdP"
      },
      "source": [
        "#### List all the folders present in the directory\r\n",
        "list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\r\n",
        "                   \"data-to-text_generation\",\"dependency_parsing\",\r\n",
        "                   \"document_classification\",\"entity_linking\",\r\n",
        "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\r\n",
        "                   \"natural_language_inference\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNjQL5BKh2gy"
      },
      "source": [
        "#### Load Test Data\r\n",
        "file_name_list = [] #Stores the name of all files \r\n",
        "input_entity_list = [] #Stores the list of all phrases from entities.txt\r\n",
        "\r\n",
        "for fls in list_of_folders:\r\n",
        "  count=0\r\n",
        "  for i in os.listdir(input_dir + fls + '/'):\r\n",
        "    for files in os.listdir(input_dir + fls + '/' + i + \"/\"):\r\n",
        "      if files.endswith(\"entities.txt\"):\r\n",
        "        entities_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\r\n",
        "        entities_list = list(filter(None,(entities_file.read()).splitlines())) # filter empty strings and split into lines\r\n",
        "        input_entity_list.append(entities_list)  \r\n",
        "    count=count+1\r\n",
        "    \r\n",
        "    file_name_list.append(fls + '/' + str(i))\r\n",
        "    \r\n",
        "  print(\"completed\",fls, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v1wE3QhCH8y"
      },
      "source": [
        "#### Returns a list of urls present in a string using a regex expression. In case no url is present, it returns an empty list. \r\n",
        "def find_url(string): \r\n",
        "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\r\n",
        "    url = re.findall(regex,string)       \r\n",
        "    return [x[0] for x in url] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H1mxpdaioGa"
      },
      "source": [
        "for i in zip(file_name_list, input_entity_list):\r\n",
        "  # print the file name\r\n",
        "  print(i)\r\n",
        "  # iterate over the phrases stored in entities.txt file\r\n",
        "  for j in i[1]:\r\n",
        "    # split them\r\n",
        "    split_version = j.split('\\t')\r\n",
        "\r\n",
        "    # Open the empty Research-problem.txt file in that folder\r\n",
        "    output_file = open(input_dir + i[0] + \"/\" + \"triples/\" + \"research-problem.txt\",\"a\")\r\n",
        "    \r\n",
        "    # If the sentence number of that phrase is between \"0 to 30\", then only process further \r\n",
        "    if(0<(int)(split_version[0])<=30):\r\n",
        "\r\n",
        "      # Now identify if that phrase is the \"only phrase\" extracted from that sentence \r\n",
        "      count_of_line = 0\r\n",
        "      for kk in i[1]:\r\n",
        "        if(kk.split('\\t')[0]==split_version[0]):\r\n",
        "          count_of_line += 1\r\n",
        "          if(count_of_line == 2):\r\n",
        "            break\r\n",
        "            \r\n",
        "      # \"Count_of_line\" stores the number of phrases extracted from that sentence.      \r\n",
        "      if(count_of_line==1):      \r\n",
        "        print(split_version)\r\n",
        "\r\n",
        "        # It can happen that the single extracted phrase might be a url, so we check it here \r\n",
        "        list_urls = find_url(split_version[3])\r\n",
        "\r\n",
        "        # Include it only if it is not a url \r\n",
        "        if (len(list_urls)==0): \r\n",
        "          output_file.write(\"(Contribution||has research problem||\" + split_version[3] + \")\" + \"\\n\")\r\n",
        "    output_file.close()  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}