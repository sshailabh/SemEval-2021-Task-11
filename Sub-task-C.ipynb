{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7v26VGi47T2"
   },
   "source": [
    "#Task-3 w entities and w/o KG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKcGbcWfoUDv",
    "outputId": "b2b799ce-3648-488c-c70c-30de930572bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npAPf69Vp30L",
    "outputId": "760bda3b-4689-4a39-cb2d-c2459ffe63b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/evaluation': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%rm -r \"/content/evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NklYLU625Dku",
    "outputId": "9a4a78b9-1c34-4735-9a81-8444a8287600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/evaluation1'...\n",
      "remote: Enumerating objects: 490, done.\u001b[K\n",
      "remote: Counting objects: 100% (490/490), done.\u001b[K\n",
      "remote: Compressing objects: 100% (483/483), done.\u001b[K\n",
      "remote: Total 490 (delta 7), reused 488 (delta 5), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (490/490), 2.50 MiB | 6.20 MiB/s, done.\n",
      "Resolving deltas: 100% (7/7), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ncg-task/evaluation-phase1 \"/content/evaluation1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eH829Yl95Ldo",
    "outputId": "c1b90fc3-a042-41ec-b025-3a54033ccd26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/sent1'...\n",
      "remote: Enumerating objects: 334, done.\u001b[K\n",
      "remote: Counting objects: 100% (334/334), done.\u001b[K\n",
      "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
      "remote: Total 334 (delta 5), reused 332 (delta 3), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (334/334), 21.40 KiB | 5.35 MiB/s, done.\n",
      "Resolving deltas: 100% (5/5), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ncg-task/evaluation-phase2 \"/content/sent1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4wpRdpx5OAF",
    "outputId": "9436c764-6458-4b49-c81c-184cb4ec3efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/entity1'...\n",
      "remote: Enumerating objects: 326, done.\u001b[K\n",
      "remote: Counting objects: 100% (326/326), done.\u001b[K\n",
      "remote: Compressing objects: 100% (169/169), done.\u001b[K\n",
      "remote: Total 326 (delta 2), reused 326 (delta 2), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (326/326), 202.11 KiB | 1.14 MiB/s, done.\n",
      "Resolving deltas: 100% (2/2), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ncg-task/evaluation-phase2-part2 \"/content/entity1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWpyT1q75QSw",
    "outputId": "9f48a940-24d3-4f98-fd00-bab9cbc18f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed sent constituency_parsing 9\n",
      "completed sent coreference_resolution 10\n",
      "completed sent data-to-text_generation 7\n",
      "completed sent dependency_parsing 9\n",
      "completed sent document_classification 21\n",
      "completed sent entity_linking 17\n",
      "completed sent face_alignment 19\n",
      "completed sent face_detection 22\n",
      "completed sent hypernym_discovery 9\n",
      "completed sent natural_language_inference 32\n",
      "completed entity constituency_parsing\n",
      "completed entity coreference_resolution\n",
      "completed entity data-to-text_generation\n",
      "completed entity dependency_parsing\n",
      "completed entity document_classification\n",
      "completed entity entity_linking\n",
      "completed entity face_alignment\n",
      "completed entity face_detection\n",
      "completed entity hypernym_discovery\n",
      "completed entity natural_language_inference\n"
     ]
    }
   ],
   "source": [
    "# Test dataset reading\n",
    "import os\n",
    "from shutil import copyfile\n",
    "output_dir = \"/content/evaluation1/\"\n",
    "test1_input_dir = \"/content/sent1/\"\n",
    "test2_input_dir = \"/content/entity1/\"\n",
    "test_list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
    "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
    "                   \"document_classification\",\"entity_linking\",\n",
    "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
    "                   \"natural_language_inference\"]\n",
    "test1_file_name_list = []\n",
    "t_files = 0\n",
    "for fls in test_list_of_folders:\n",
    "  count=0\n",
    "  for i in os.listdir(test1_input_dir + fls + '/'):\n",
    "    count=count+1\n",
    "    for files in os.listdir(test1_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        copyfile(test1_input_dir + fls + '/' + str(i) + '/' + files, output_dir + fls + '/' + str(i) + '/' + files)\n",
    "        t_files=t_files+1\n",
    "    test1_file_name_list.append(fls + '/' + str(i))\n",
    "  print(\"completed sent\",fls,count)\n",
    "\n",
    "test2_file_name_list = []\n",
    "for fls in test_list_of_folders:\n",
    "  for i in os.listdir(test2_input_dir + fls + '/'):\n",
    "    for files in os.listdir(test2_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"entities.txt\"):\n",
    "        copyfile(test2_input_dir + fls + '/' + str(i) + '/' + files, output_dir + fls + '/' + str(i) + '/' + files)\n",
    "    test2_file_name_list.append(fls + '/' + str(i))\n",
    "  print(\"completed entity\",fls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAy4gXQT5Vy8"
   },
   "outputs": [],
   "source": [
    "%cp -R \"/content/evaluation1\" \"/content/drive/MyDrive/sub3_ph22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLdB4jl36NFK",
    "outputId": "31384e72-b94c-4ac0-bdea-e6e2dc25e65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed constituency_parsing 9\n",
      "completed coreference_resolution 19\n",
      "completed data-to-text_generation 26\n",
      "completed dependency_parsing 35\n",
      "completed document_classification 56\n",
      "completed entity_linking 73\n",
      "completed face_alignment 92\n",
      "completed face_detection 114\n",
      "completed hypernym_discovery 123\n",
      "completed natural_language_inference 155\n"
     ]
    }
   ],
   "source": [
    "test_input_dir = \"/content/drive/MyDrive/sub3_ph22/\"\n",
    "# Test dataset reading\n",
    "import os\n",
    "from shutil import copyfile\n",
    "test_list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
    "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
    "                   \"document_classification\",\"entity_linking\",\n",
    "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
    "                   \"natural_language_inference\"]\n",
    "t_files = 0\n",
    "for fls in test_list_of_folders:\n",
    "  for i in os.listdir(test_input_dir + fls + '/'):\n",
    "    for files in os.listdir(test_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"entities.txt\"):\n",
    "        t_files=t_files+1\n",
    "  print(\"completed\",fls,t_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG4lEjR-5_Jw"
   },
   "source": [
    "#Task-3 (entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfL3Hq9q5-b-",
    "outputId": "0fc6056a-fd80-4036-d951-2eb38da4ecc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fV30ui0O5gko",
    "outputId": "6ee2443d-85e4-40d4-f172-7487eb4157da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.8MB 8.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 37.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.9MB 57.5MB/s \n",
      "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "########### evaluating model\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "import random\n",
    "\n",
    "### Training label formation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "seed_val = 66\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166,
     "referenced_widgets": [
      "7def957200934c688c3984d65a042f18",
      "7b03c110ebe14542ad2c3f2eab0e8d23",
      "74252d88d06a44a99551c2c7f11098a9",
      "a292ce36f6504636bd83f7a6815b37b5",
      "f0b9258d2e3e4677990651dc85c6d380",
      "7ee9a58040734516a2d3cfe72b487d86",
      "da0ca5b647ef4cac9f7040ffdfc64509",
      "2e73bcabc7a24731b5d5c230e6a91f4d",
      "be0b351448c242048aeca38148ede6dc",
      "b23bb2d100484b3388fe5588cf33c61e",
      "552b66794a10470f8115c8978017009f",
      "c7437879651b44999e8d5c1519faf03b",
      "0605bc767b2a4bdda845f89ee8813e66",
      "2ef8f3cd00fe4836abe4daa601845d9b",
      "5c06cc0b4b7540b7bba0b10686761285",
      "8a52121f3e21446d9995a0eae5dfa9cb",
      "4c777914bf824e07b95f9ea6804df26f",
      "01678666028e4273afd75e2b045ce180",
      "b7253170d6ed4f2ea5f8e64a6c78ea12",
      "fd494f86c5544b509d534f0b8fe06735",
      "cb9fcf8f754d4aceba9281389785e4fe",
      "2b7b21f157604fa2b8286e21973ca647",
      "9175e21ac0e84129915d6fc5c3164e7b",
      "663e0e8bc0214b9bb23cc289b831fa67"
     ]
    },
    "id": "Q9Vm9lyQ6VUU",
    "outputId": "fc846b59-3e91-4300-9ddc-402fae33e0c2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7def957200934c688c3984d65a042f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0b351448c242048aeca38148ede6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442221694.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c777914bf824e07b95f9ea6804df26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=227845.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/content/drive/MyDrive/sub3_ph22/\"\n",
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "#self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "class SCIBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SCIBERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', output_hidden_states=True)\n",
    "        self.lstm = torch.nn.LSTM(768, 400, num_layers=2, batch_first = True, bidirectional=True, dropout=0.1)\n",
    "        self.l2 = torch.nn.Dropout(0.2)\n",
    "        self.l3 = torch.nn.Linear(800, 400)\n",
    "        self.l4 = torch.nn.Linear(400,100)\n",
    "        self.l5 = torch.nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, lengths):\n",
    "        encoded_layers = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)[2]\n",
    "        scibert_hidden_layer = encoded_layers[12]\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(scibert_hidden_layer, lengths, batch_first=True, enforce_sorted=False)) #enforce_sorted=False  #pack_padded_sequence(data and batch_sizes\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        output_hidden = self.l2(output_hidden)            # no size change\n",
    "        output_2 = self.l3(output_hidden)\n",
    "        output_2 = torch.nn.ReLU()(output_2)\n",
    "        output_4 = self.l4(output_2)\n",
    "        output_5 = self.l5(output_4)\n",
    "        return output_5\n",
    "\n",
    "model = SCIBERTClass()\n",
    "model.to(device)\n",
    "model = torch.load(\"/content/drive/MyDrive/phase-2-task3/15-entity-task3.pt\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMmAS9rM6VXU"
   },
   "outputs": [],
   "source": [
    "infoname_test_to_idx = {'approach.txt': 0, 'model.txt': 1, 'dataset.txt': 2, \n",
    "        'experimental-setup.txt': 3, 'hyperparameters.txt': 4,  'baselines.txt': 5, 'results.txt': 6, \n",
    "        'tasks.txt': 7, 'experiments.txt': 8, 'ablation-analysis.txt': 9}\n",
    "\n",
    "infoname_idx_to_test = {v: k for k, v in infoname_test_to_idx.items()}\n",
    "\n",
    "infoname_test_to_first_triplet = {'approach.txt': 'Approach', 'model.txt': 'Model', 'dataset.txt': 'Dataset', \n",
    "        'experimental-setup.txt': 'Experimental setup', 'hyperparameters.txt': 'Hyperparameters',  'baselines.txt': 'Baselines', 'results.txt': 'Results', \n",
    "        'tasks.txt': 'Tasks', 'experiments.txt': 'Experiments', 'ablation-analysis.txt': 'Ablation analysis'} #research problem and code has no first triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laBo01eu6VZt",
    "outputId": "f140cbfd-b5a8-4c42-c98a-b6476ee68985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output recurrent neural network grammars recurrent neural network grammars ( rnng ) rnngs manipulates inductive bias rnngs test linguistic hypotheses begin ablation study discover importance composition function augment rnng composition function novel gated attention mechanism incorporate interpretability using ga - rnng investigating role individual heads play phrasal representation nonterminal category labels play rnng stack strongest ablations outperforms `` full `` rnng three data structures ablating stack gives worst among new results seen language modeling stack - rnng achieves best performance ablating stack harmful modeling syntax without explicit composition provides little benefit sequential lstm language model stack - results best published ptb results phrasestructure dependency parsing gated attention rnng clear model outperforms baseline rnng three structures achieves competitive performance strongest , stack - , rnng variant headedness phrases model higher overlap conversion using collins head rules 49.8 uas stanford head rules 40.4 uas attention - based tree output high error rate ( ? 90 % ) dependent verb conversion accuracy better nouns ? 50 % error much better determiners 30 % particles 6 % respect collins head rules role nonterminal labels test data usual split ga - rnng achieves 94.2 % u - ga - rnng achieves 93.5 %\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output transition - based constituent parsing neural transition - based constituent parsing propose novel transition system constituent parsing mitigating issues bottom - top - systems finding compromise bottom - constituent information top - lookahead information https : //github.com/leoncrashcode/inorderparser reranking experiments bottom - system performs slightly better top - system inorder system outperforms bottom - top - system find bottom - parser top - parser similar results greedy setting english constituent results fully - supervise setting inorder parser outperforms state - - - art discrete parser state - - - art neural parsers chinese dependency results final model achieves best results among transitionbased parsing obtains comparable results state - - - art graph - based models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output class [[False  True False False False False  True False False False]]\n",
      "output syntactic constituency parsing constructed artificial dataset labelling large corpus berkeleyparser trained sequence - - sequence model attention small human - annotated parsing dataset second artificial dataset consisting high - confidence parse trees measured agreement two parsers trained sequence - - sequence model attention used model 3 lstm layers 256 units layer call lstm + training small dataset additionally used 2 dropout layers lstm 1 lstm 2 lstm 2 lstm 3 embedding layer 90k vocabulary initialized randomly using pre-trained word - vector embeddings pre-trained skip - gram embeddings size 512 using word2vec 10b - word corpus single attention model gets 88.3 ensemble 5 lstm + a+d models achieves 90.5 matching single - model berkeleyparser wsj trained large high - confidence corpus single lstm + model achieves 92.5 outperforms best single model best ensemble result ensemble 5 lstm+ models improves score 92.8 lstm + model trained wsj dataset produced malformed trees 25 1700 sentences development set ( 1.5 % cases ) full high - confidence dataset 14 sentences ( 0.8 % ) difference f 1 score sentences length upto 30 upto 70 1.3 berkeleyparser 1.7 baseline lstm 0.7 lstm + lstm + trained high - confidence corpus achieved f 1 score 95.7 qtb 84.6 web\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output parsing syntactic parsing deep learning syntactic parsing present neural - net parse reranker simple architecture three lstm layers 1,500 units trained truncated backpropagation time mini-batch size 20 step size 50 initialize starting states previous minibatch 's last hidden states forget gate bias initialized one rest model parameters sampled u ( ? 0.05 , 0.05 ) dropout applied non-recurrent connections gradients clipped norm bigger 20 learning rate 0.25 0.85 max epoch number use vanilla softmax entire vocabulary results single lstm - lm ( gs ) charniak ( gs ) reaches 93.6 ensemble eight lstm - lms ( gs ) charniak ( gs ) achieves new state art 93.8 f 1 trees converted stanford dependencies 5 uas las 95.9 % 94.1 %\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output constituency parsing introduce parser combines encoder built using self - attentive architecture decoder customized parsing present version model uses character lstm performs better lexical representationseven word embeddings removed results english ( wsj ) test score 93.55 f1 charlstm parser exceeds previous best numbers single - system parsers trained penn treebank parser augmented elmo word representations achieves new state - - - art score 95.13 f1 wsj test set multilingual ( spmrl ) development set results show addition word embeddings model uses character lstm mixed effect improves performance languages hurts others 8 9 languages test set result exceeds previous best - published numbers\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output constituency parsing update seq2seq approach stronger baseline constituency parsing smaller mini-batch size gradient clipping g provided better performance larger layer size , hidden state dimension , beam size little impact performance using subword split input token unit instead standard tokenized word unit potential improve performance subword information features promising approach leveraging subword information constituency parsing seq2seq approach achieved competitive level current top - notch methods rnng variants\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output grammars introduce recurrent neural network grammars new generative probabilistic model sentences explicitly models nested , hierarchical relationships among words phrases rnngs operate via recursive syntactic process reminiscent probabilistic context - free grammar generation give two variants algorithm parsing generation discriminative model use ancestor sampling obtain samples parse trees sentences present simple importance sampling algorithm uses samples discriminative parser solve inference problems generative model discriminative model used hidden dimensions 128 2 - layer lstms generative model used 256 dimensions 2 layer lstms models tuned dropout rate maximize validation set likelihood obtaining optimal rates 0.2 ( discriminative ) 0.3 ( generative ) sequential lstm baseline language model found optimal dropout rate 0.3 training used stochastic gradient descent learning rate 0.1\n",
      "output class [[False  True False False False False False False False False]]\n",
      "output pretraining self - attention networks pretraining bi-directional transformer model language model pretraining show even larger performance gains possible jointly pretraining directions large language - model - inspired self - attention cloze model bi-directional transformer architecture predicts every token training data introducing cloze - style training objective model predict center word given left - - right right - - left context representations model separately computes forward backward states equal contribution experimental setup cnn models use adaptive softmax output headband contains 60k frequent types dimensionality 1024 followed 160 k band dimensionality 256 learning rate linearly warmed 10 ? 7 1 16 k steps annealed using cosine learning rate schedule single phase 0.0001 run experiments dgx - 1 machines 8 nvidia v100 gpus machines interconnected infiniband use nccl2 library torch results glue models outperform uni-directional transformer cnn base model performs well stilts aggregate named entity recognition fine tuning gives biggest gain constituency parsing results found scaling bilm term factor 0.15 better performance shows cloze loss performs significantly better bilm loss combining two loss types improve cloze loss\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output constituency parsing neural constituency parsing present experiments isolate degree gain two state - - - art generative neural parsing models recurrent neural network grammar generative parser ( rg ) lstm language modeling generative parser ( lm ) beam - based search procedure augmented state space search generative models taking weighted average scores models selecting parse base parser 's candidate list augmenting candidate set rg decreases performance 93.45 f1 92.78 f1 score combination combining scores models improves using score either model alone strengthening model combination combining candidates scores three models obtain 93.94 f1 ensembling using ensembled rd models lower rescoring single rd model ptb setting ensembling score combination achieves best result 94.25\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "completed constituency_parsing 0\n",
      "output abstract anaphora resolution anaphora ( coreference ) resolution inspired mention - ranking model coreference resolution combines siamese net learning similarity sentences given anaphoric sentence candidate antecedent lstm - siamese net learns representations candidate anaphoric sentence shared space representations combined joint representation calculate score characterizes relation learned score select highest - scoring antecedent candidate given anaphoric sentence hence anaphor consider one anaphor time provide embedding context anaphor head anaphoric phrase characterize individual anaphorsimilar individuating multiply occurring predicates srl show model learns relation anaphor anaphoric sentence antecedent produces large amounts instances easily adaptable languages https : //github.com/amarasovic / neural-abstract-anaphora baselines report preceding sentence baseline ( ps bl ) chooses previous sentence antecedent tagbaseline ( tag bl ) randomly chooses candidate constituent tag label { , vp , root , sbar } glo word embeddings pre-trained gigaword wikipedia fine - tune vocabulary built words training data frequency { 3 , u ( 1 , 10 ) } oov words replaced unk token size lstms hidden states set { 100 , qlog - u ( 30 , 150 ) } initialized weight matrices lstms random orthogonal matrices first feed - forward layer size set value optimization trained model minibatches using adam learning rate 10 ? 4 maximal batch size 64 clip gradients global norm clipping value { 1.0 , u ( 1 , 100 ) } train 10 epochs used l 2 - regularization { 10 ? 5 , log - u ( 10 ? 7 , 10 ? 2 ) } dropout keep probability k p { 0.8 , u ( 0.5 , 1.0 ) } applied outputs lstms feed - forward layers optionally input k p ? u ( 0.8 , 1.0 ) terms @ 1 score mr - lstm outperforms kzh13 's results tag bl without even necessitating hp tuning observe hps tuned arrau - aa obtain results well beyond kzh13 ablated model variants perform worse full model large performance drop omitting syntactic information performance 68.10 @ 1 score indicates model learn without syntactic guidance arrau corpus mr - lstm successful resolving nominal pronominal anaphors shell noun resolution kzh13 's dataset mr - lstm achieved @ 1 scores range 76.09-93.14 best variant model achieves 51.89 @ 1 score nominal anaphors arrau - aa mr - lstm without context embedding ( ctx ) achieves comparable @ 2 score variant omits syntactic information better @ 3 - 4 scores\n",
      "output class [[False  True False  True False  True  True False False  True]]\n",
      "output coreference resolution fine - tune bert coreference resolution present two ways extending c 2f - coref model https : //github.com/mandarjoshi90/coref extend original tensorflow implementations c 2f - coref 3 bert fine tune models ontonotes english data 20 epochs using dropout 0.3 learning rates 1 10 ? 5 2 10 ? 4 linear decay bert parameters task parameters trained separate models max segment len 128 256 384 512 paragraph level : gap shows bert improves c 2 f - coref 9 % 11.5 % base large models document level : ontonotes shows bert - base offers improvement 0.9 % elmo - based c2 fcoref model bert - large improves c 2 f - coref much larger margin 3.9 % observe overlap variant offers improvement independent\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output coreference resolution provides entity - level representation simple intuitive manner facilitates end - - end optimization `` entity equalization `` approach posits entity represented via sum corresponding mention representations uses contextual embeddings input mention representations use bert embeddings using bert fully convolutional manner results baseline span - ranking model elmo input features second - order span representations achieves 73.0 % avg replacing elmo features bert features achieves 76 . 25 % average f1 removing second - order span - representations using bert features achieves 76.37 % f1 secondorder span representations entity equalization achieves 76 . 64 % average f1 set new state art coreference resolution improving previous state art 3.6 % average f1\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output coreference resolution train deep neural network build distributed representations pairs coreference clusters captures entity - level information large number learned , continuous features instead small number hand - crafted categorical ones using cluster - pair representations network learns combining two coreference clusters desirable test time builds coreference clusters incrementally starting mention cluster merging pair clusters step makes decisions novel easy - first cluster - ranking procedure combines strengths cluster - ranking ( rahman easy - first coreference algorithms using learning - - search algorithm inspired searn train neural network learn action available current state partially completed coreference clustering lead high - scoring coreference partition mention - ranking model surpasses previous systems cluster - ranking model improves results across languages evaluation metrics\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output coreference resolution explore using two variants reinforcement learning directly optimize coreference system coreference evaluation metrics modify max-margin coreference objective incorporating reward associated coreference decision loss 's slack rescaling test reinforce policy gradient algorithm model neural mention - ranking model results find reinforce slightly better heuristic loss reward rescaling performs significantly better languages reward - rescaled max - margin loss combines best worlds resulting superior performance\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output coreference resolution propose goal - directed endto - end deep reinforcement learning framework resolve coreference leverage neural architecture policy network includes learning span representation scoring potential entity mentions generating probability distribution possible coreference linking actions current mention antecedents sequence linking actions reward function used measure good generated coreference clusters directly related coreference evaluation metrics introduce entropy regularization term encourage exploration prevent policy prematurely converging bad local optimum update regularized policy network parameters based rewards associated sequences sampled actions computed whole input document experiments pretrain model around 200 k steps use learned parameters initialization set number sampled trajectories tune regularization parameter { 10 ? 5 , 10 ? 4 , 0.001 , 0.01 , 0.1 , 1 } set 10 ? 4 results base reinforced model improves average f 1 score around 2 points statistical significant t- test p < 0.05 using entropy regularization encourage exploration improve result 1 point introducing context - dependent elmo embedding base model boosts performance full model achieves state - - art performance 73.8 % f1 - score using elmo entropy regularization\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output coreference resolution introduce approximation higher - order inference uses span - ranking architecture iterative manner iteration antecedent distribution used attention mechanism optionally update existing span representations enabling later corefer alleviate computational challenges higher - order inference propose coarseto - fine approach learned single endto - end objective less accurate efficient coarse factor pairwise scoring function additional factor enables extra pruning step inference reduces number antecedents cheaply computes rough sketch likely antecedents applying expensive scoring function results baseline span - ranking model augmented elmo hyperparameter tuning achieves 72.3 f1 full approach achieves 73.0 f1 setting new state art coreference resolution despite using far less computation outperforms baseline observe much higher recall adopting coarse - - fine approach improvement including second - order inference\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output end - - end neural coreference resolution end - - end coreference resolution present neural coreference resolution model learned end - toend given gold mention clusters training end - - end neural model jointly learns spans entity mentions cluster reasons space spans maximum length directly optimizes marginal likelihood antecedent spans gold coreference clusters includes span - ranking model decides previous spans good antecedent core vector embeddings representing spans text document combine context - dependent boundary representations head - finding attention mechanism span attention component inspired parser - derived head - word matching features previous systems less susceptible cascading errors hyperparameters word embeddings fixed concatenation 300 - dimensional glove embeddings 50 - dimensional embeddings normalized unit vectors outof - vocabulary words represented vector zeros character cnn characters represented learned 8 - dimensional embeddings convolutions window sizes 3 , 4 , 5 characters consisting 50 filters hidden states lstms 200 dimensions feedforward neural network consists two hidden layers 150 dimensions rectified linear units use adam learning minibatch size 1 lstm weights initialized random orthonormal matrices apply 0.5 dropout word embeddings character cnn outputs 0.2 dropout hidden layers feature embeddings dropout masks shared across timesteps preserve long - distance information learning rate decayed 0.1 % every 100 steps trained 150 epochs early stopping code implemented tensor - flow results coreference results outperform previous systems metrics single model improves state - - - art average f1 1.5 5 - model ensemble improves 3.1 significant gains improvements recall distance spans width spans crucial signals coreference resolution contribute 3.8 f1 final result see contribution 0.9 f1 character - level modeling show 1.3 f1 degradation without attention mechanism finding task - specific heads keeping mention candidates detected rule - based system predicted parse trees degrades performance 1 f1 oracle mentions see improvement 17.5 f1\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output coreference resolution coreference prediction posit global context indeed necessary improvements coreference resolution learn representations mention clusters embedding sequentially using recurrent neural network manually defined cluster features learns global representation individual mentions present cluster mention - ranking style coreference system including recurrent neural network mention - ranking sub-system trained end - - end coreference task train model local classifier fixed context training use document - size minibatches minimize loss adagrad choose learning rates layer { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } set ha ( x n ) , h c ( x n ) , h ( ) r 200 hp ( x n , ) r 700 use single - layer lstm without `` peep - hole `` connections implemented element - rnn library regularization apply dropout rate 0.4 applying linear weights u 0.3 lstm states forming dot -product scores https : //github.com/swiseman/nn_coref system makes use gpu training trains two hours results see statistically significant improvement 0.8 co nll points previous state art highest f 1 scores three conll metrics rnn improves performance dramatic improve - ments non-anaphoric pronouns errors decreased significantly non-anaphoric nominal proper mentions rnn performance significantly better avg baseline barely improves mention - ranking\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output end - - end co -reference resolution end - - end co-reference resolution ( e2e - cr ) e2e - cr co-reference resolution propose cross - sentence encoder end - - end co-reference ( e2e - cr ) external memory block containing syntactic semantic information context sentences added standard lstm model context memory block proposed model encode input sentences batch calculate representations input words taking target sentences context sentences lstm modules applied model 200 output units asl calculate cross - sentence dependency using multilayer perceptron one hidden layer consisting 150 hidden units initial learning rate set 0.001 decays 0.001 % every 100 steps model optimized adam algorithm select 40 continuous sentences training results comparing baseline model achieved 67.2 % f1 score asl model improved performance 0.6 % achieved 67.8 % average f1 show models consider cross - sentence dependency significantly outperform baseline model encodes sentence input document indicated asl model better performance lsl model extracts context information attention mechanism instead simply viewing sentence - level embeddings\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "completed coreference_resolution 0\n",
      "output data - - text generation learns content plan input conditions content plan generate output document train end - - end using neural networks used one - layer pointer networks content planning two - layer lstms text generation input feeding employed text decoder applied dropout rate 0.3 models trained 25 epochs adagrad optimizer initial learning rate 0.15 learning rate decay selected { 0.5 , 0.97 } batch size 5 text decoding made use bptt set truncation size 100 set beam size 5 models implemented open nmt - py results ncp improves upon vanilla encoderdecoder models ( ed + jc , ed + cc ) irrespective copy mechanism achieves comparable scores either joint conditional copy mechanism indicates content planner brings performance improvements ncp + cc achieves best content selection content ordering scores terms bleu compared best reported system achieve absolute improvement approximately 12 % terms relation generation content selection precision improves 5 % recall 15 % content ordering increases 3 % bleu 1.5 points template - based system obtains low bleu cs precision scores high cs recall rg metrics\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output structured data text generation neural text generation graph - structured data graph structured data text generation results webnlg task model gcn encoder outperforms strong baseline employs lstm encoder .009 bleu points gcn model stable baseline standard deviation .004 vs . 010 gcn ec model outperforms pkuwriter .047 bleu points melbourne .014 bleu points sr11 deep task compare neural models upper bound results pipeline model stumba - tbdil model obtains .794 . 805 blue outperforming gcn - based model notice importance skip connections gcn layers residual dense connections lead similar results dense connections produce models bigger slightly less accurate residual connections\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output sequence - - sequence natural language generation natural language generation present neural ensemble natural language generator train test three large unaligned datasets restaurant , television , laptop domains explore novel ways represent mr inputs including novel methods delexicalizing slots values automatic slot alignment semantic reranker built ensemble model using seq2seq framework tensorflow individual lstm models use bidirectional lstm encoder 512 cells per layer cnn models use pooling encoder decoder models 4 - layer rnn decoder 512 lstm cells per layer attention e2e dataset automatic metric evaluation lstm cnn models clearly benefit additional pseudo - samples training set official e2e test set ensemble model performs comparably baseline model tv laptop datasets ensemble model performs competitively baseline tv dataset outperforms laptop dataset wide margin\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output data - - text generation transcribing structured data natural language descriptions data - - text propose new structured - data encoder assuming structures hierarchically captured focuses encoding data - structure model general structure data using two - level architecture first encoding entities basis elements encoding data structure basis entities introduce transformer encoder data - - text models ensure robust encoding element / entities comparison others integrate hierarchical attention mechanism compute hierarchical context fed decoder baselines wiseman standard encoder - decoder system copy mechanism li standard encoder - decoder delayed copy mechanism text first generated placeholders replaced salient records extracted table pointer network puduppully - plan acts two steps first standard encoder - decoder generates plan second standard encoder - decoder generates text plan puduppully - updt consists standard encoder - decoder added module aimed updating record representations generation process decoding step gated recurrent network computes records updated size record value embeddings hidden layers transformer encoders set 300 use dropout rate 0.5 trained batch size 64 train model fixed number 25 k updates average weights last 5 checkpoints every 1 k updates ensure stability across runs adam optimizer initial learning rate 0.001 reduced half every 10 k steps used beam search beam size 5 inference models implemented open nmt - py https : //github.com/kaijuml/data-to-text-hierarchical see lower results obtained flat scenario compared scenarios comparison scenario hierarchical - kv hierarchical -k shows omitting entirely record values attention mechanism effective hierarchical models achieve significantly better scores metrics compared flat architecture shows flat scenario obtains significant higher bleu score generates fluent descriptions accurate mentions included gold descriptions outperform two - step decoders li puduppully - plan bleu qualitative metrics\n",
      "output class [[False  True False False  True  True  True False False  True]]\n",
      "output neural data - - text generation data - - text generation propose explicit , symbolic , text planning stage output fed neural generation system text planner determines information structure expresses unambiguously sequence ordered trees plan neural generation system transform fluent , natural language text https : //github.com/amitmy/ chimera best melbourne end - - end system scored best categories automatic evaluation upf - forge classic grammar - based nlg system scored human evaluation developed end - - end neural baseline uses set encoder lstm decoder attention copy - attention mechanism neural checklist model results strongneural bestplan systems outperform webnlg participating systems automatic metrics\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output character - based data - - text generation natural language generation text generation present character - level sequence - - sequence model attention mechanism results completely neural end - - end architecture require delexicalization tokenization lowercasing produces vocabulary - free model inherently general depend specific domain 's set terms general alphabet character - wise copy mechanism consisting soft switch generation copy mode peculiar training procedure improves internal representation capabilities enhancing recall consists exchange encoder decoder rnns developed system using pytorch framework 2 , release 0.4.1 minimize negative log - likelihood loss using teacher forcing adam results first interesting result eda_cs always obtains higher metric values respect tgen hotel restaurant datasets e2e + tgen achieves three five metrics values approach eda_cs tl obtain better performance respect training eda_cs standard way hotel restaurant datasets eda_cs tl shows bleu increment least 14 % respect tgen 's score compared hotel restaurant datasets baseline model eda largely outperformed examined methods\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output text generation language generation interpretation builds learned rational speech acts ( rsa ) models canonical presentation rsa framework grounded reference resolution models speakers describe referents presence distractors models listeners resolve descriptors referents abstractive summarization pragmatic methods obtain improvements 0.2-0.5 rouge scores 0.2-1.8 meteor base 0 model distractor - based approach sd outperforming reconstructorbased approach r sd strong across metrics\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "completed data-to-text_generation 0\n",
      "output parsing natural language parsing transition - based parsing adapt training criterion explore parser states training data model learned use method dynamically chose optimal ( relative final attachment accuracy ) action given imperfect history interpolating algorithm states sampled model training data robust predictions test time experiments achieved dynamic oracle english 93.56 uas chinese score establishes state - - - art\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output joint pos tagging dependency parsing joint part - - speech ( pos ) tagging dependency parsing https : //github.com/datquocnguyen/jptdp dependency parsing present novel neural network - based model jointly learning pos tagging dependency paring joint model extends well - known bist graph - based dependency parser additional lower - level bilstm - based tagging component jptdp v 2.0 implemented using dynet v2.0 fixed random seed word embeddings initialized randomly pre-trained word vectors character pos tag embeddings randomly initialized apply dropout 67 % keep probability inputs bilstms mlps word dropout learn embedding unknown words optimize objective loss using adam initial learning rate 0.001 mini-batches use 100 - dimensional word embeddings 50 - dimensional character embeddings 100 dimensional pos tag embeddings fix number hidden nodes mlps 100 model produces competitive parsing results obtains uas score 94.51 % las score 92.87 % achieve 0.9 % lower parsing scores state - - - art dependency parser obtain state - - - art pos tagging accuracy 97.97 % test\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output neural network transition - based parsing neural network transition - based dependency parsing dependency parsing transition - based parsing combine representational power neural networks superior search enabled structured training inference train neural network model probability individual parse actions use activations layers neural network representation structured perceptron model trained beam search early updates generate large quantities high - confidence parse trees parsing unlabeled data two different parsers selecting sentences two parsers produced trees known tri-training used publicly available word2vec 2 tool learn cbow embeddings results reported accuracy 94.22 % uas model competitive highest reported accuries dependencies wsj\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output pos tagging dependency parsing https : //github.com/datquocnguyen/bioposdep investigate current stateof - - art ( sota ) approaches dependency parsing applied biomedical texts study impact parser choice biomedical event extraction three bilstm - crf - based models stanford - nndep jptdp stanford - biaffine employ 200 dimensional pre-trained word vectors traditional feature - based models marmot nlp4j - pos nlp4j - dep use original pure java implementations default hyperparameter settings bilstm - crf - based models use nadam run 50 epochs stanford - nndep select word cutoff { 1 , 2 } size hidden layer { 100 , 150 , 200 , 250 , 300 , 350 , 400 } jptdp use 50 - dimensional character embeddings fix initial learning rate 0.0005 number bilstm layers 2 select number lstm units layer { 100 , 150 , 200 , 250 , 300 } pos tagging results bilstm - crf mar - mot obtain lowest scores genia craft jptdp obtains similar score mar - mot genia bilstm - crf craft marmot obtains accuracy results 98.61 % 97.07 % genia craft bilstm - crf obtains accuracies 98.44 % ge - nia 97.25 % craft ptb cnn - based character - level word embeddings provided 0.1 % improvement bilstm - crf genia craft bilstm - crf character - level word embeddings obtains highest accuracy scores overall dependency parsing results genia among pre-trained models bllip obtains highest results pre-trained stanford - biaffine ( v1 ) model produces lower scores pre-trained stanford - nndep model genia pre-trained nndep biaffine models result significant performance differences pre-trained stanford tagger 98.37 % vs. retrained nlp4j - pos model 98.80 %\n",
      "output class [[False  True False False  True  True  True False False False]]\n",
      "output globally normalized transition - based neural networks globally normalized transition - based neural network demonstrate simple feed - forward networks without recurrence achieve comparable better accuracies lstms globally normalized uses transition system feature embeddings beam search maintaining multiple hypotheses introduce global normalization conditional random field ( crf ) objective overcome label bias problem use beam inference approximate partition function summing elements beam use early updates compute gradients based approximate global normalization perform full backpropagation training neural network parameters based crf loss experiments part speech tagging results globally normalized model significantly outperforms local model beam search locally normalized model suffers severe label bias issues using beam search locally normalized model help global normalization leads 7 % reduction relative error set character ngrams feature important increasing average accuracy conll '09 datasets 0.5 % absolute dependency parsing results model compares favorably 94.26 % las 92.41 % uas results significantly outperform lstm - based approaches\n",
      "output class [[False  True False False False False  True False False  True]]\n",
      "output give probabilistic interpretation ensemble parser viewing instance minimum bayes risk inference distilling ensemble single fog parser discriminative training defining new cost function derive cost possible attachment ensemble 's division votes use discriminative learning graphbased dependency parsing consider neural fog parser trained hamming cost strong benchmark outperforming many higherorder graph - based neural network models three datasets training distillation cost gives consistent improvements languages english comes close slower ensemble chinese achieves best published scores german best published uas scores\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output neural dependency parsing modify neural graphbased approach build network larger uses regularization replace traditional mlp - based attention mechanism affine label classifier biaffine ones choose optimize adam model gets nearly uas performance ptb - sd 3.3.0 current sota model sota uas performance ctb 5.1 7 sota performance conll 09 languages\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output dependency parsing centered around birnns represent word bilstm encoding use concatenation minimal set bilstm encodings feature function passed non-linear scoring function ( multi - layer perceptron ) bilstm trained rest parser learn good feature representation parsing problem using bilstm feature extractor two parsing architectures transition - based graph - based graphbased parser jointly train structured - prediction model top bilstm parsers implemented python using pycnn toolkit neural network training https : //github.com/elikip / bist -parser use lstm variant implemented pycnn optimize using adam optimizer parsers competitive using external embeddings first - order graph - based parser 2 features outperforms systems using external resources including third - order turboparser greedy transition based parser 4 features matches outperforms parsers including beam - based transition parser heavily engineered features moving simple ( 4 features ) extended ( 11 features ) feature set leads gains accuracy english chinese adding external word embeddings accuracy graph - based parser degrades\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output dependency parsing propose novel neural network architecture dependency parsing stackpointer networks ( stackptr ) stackptr transition - based architecture corresponding asymptotic efficiency maintains global view sentence proves essential achieving competitive accuracy stackptr parser pointer network backbone equipped internal stack maintain order head words tree structures stackptr parser performs parsing incremental , topdown , depth - first fashion generates arc assigning child headword top internal stack re-implemented graph - based deep biaffine ( biaf ) parser results uas las full variation stackptr decoding beam size 10 outperforms biaf chinese obtains competitive performance english german full model achieves best accuracy english chinese performs slightly worse + sib german lcm ucm stackptr significantly outperforms biaf languages ra slightly worse biaf re-implementation biaf obtains better performance original one model achieves state - - - art performance uas las chinese best uas english german performance competitive biaf significantly better models\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "completed dependency_parsing 0\n",
      "output text classification document classification hierarchical classification presents new approach hierarchical document classification call hierarchical deep learning text classification ( hdltex ) hdltex combines deep learning architectures allow specialized learning level document hierarchy using central processing units ( cpus ) graphical processing units ( gpus ) done xeon e5 ? 2640 ( 2.6 ghz ) 32 cores 64gb memory gpu cards n vidia quadro k620 n vidia tesla k20c implemented approaches python using compute unified device architecture ( cuda ) parallel computing platform application programming interface ( api ) model created n vidia used keras tensor flow libraries creating neural networks compare three conventional document classification approaches nave bayes two versions svm stacking svm three deep learning approaches dnn rnn cnn rnn outperforms others three w os data sets cnn performs secondbest three data sets svm term weighting third first two sets nave bayes much worse methods hdltex approaches stacked , deep learning architectures provide superior performance data set w os ? 11967 best accuracy obtained rnn first level classification dnn second level gives accuracies 94 % first level 92 % second level 86 % data set w os ? 46985 best scores achieved rnn level one level 2\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output text classification investigate modifications network proposed conneau et al . aim reducing number parameters , storage size latency used temporal depthwise separable convolution global average pooling techniques propose squeezed deep convolutional neural networks ( svdcnn ) requires significantly fewer parameters compared stateof - - art cnns svdcnn char - cnn network architecture implemented pytorch svdcnn experimental settings similar original vdcnn paper using dictionary embedding size 16 training performed sgd utilizing size batch 64 maximum 100 epochs use initial learning rate 0.01 momentum 0.9 weight decay 0.001 performed nvidia gtx 1060 gpu + intel core 7 4770s cpu network reduction obtained gap fc layers considering dataset four target classes comparing svdcnn vdcnn number parameters passed 12.59 0.02 million parameters representing reduction 99.84 % char - cnn proposed model 99.82 % smaller - depth model occupies 6 mb regarding accuracy results performance difference vdcnn svdcnn models varies 0.4 1.3 %\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output text classification https : //github.com/andyweizhao/capsule_text_ classification recent method called capsule network introduce iterative routing process decide credit attribution nodes lower higher layers three strategies stabilize dynamic routing process alleviate disturbance noise capsules may contain `` background `` information stop words words unrelated specific categories use 300 - dimensional word2vec vectors initialize embedding vectors conduct mini-batch size 50 ag 's news size 25 datasets adam optimization algorithm 1e - 3 learning rate several strong baseline methods including lstm / bi - lstm tree - structured lstm ( tree - lstm ) lstm regularized linguistic knowledge ( lr - lstm ) cnnrand / cnn - static / cnn - non-static ( kim , 2014 ) deep convolutional network ( vd - cnn ) character - level convolutional network ( cl - cnn ) results observe capsule networks achieve best results 4 6 benchmarks single - label multi - label text classification investigate capability capsule network multi-label text classification using single - label samples training data observe capsule networks substantial significant improvement terms four evaluation metrics strong baseline methods test sets reuters - multi-label reuters - full datasets larger improvement achieved reuters - multi - label dataset contains multi-label documents test set capsule network much stronger transferring capability conventional deep neural networks good results reuters - full indicate capsule network robust superiority competitors single - label documents connection strength visualization observe capsule networks correctly recognize cluster important phrases respect text categories\n",
      "output class [[False  True False False  True  True  True False False False]]\n",
      "output text classification propose new graph neural networkbased method text classification construct single large graph entire corpus contains words documents nodes model graph graph convolutional network ( gcn ) simple effective graph neural network captures high order neighborhoods information edge two word nodes built byword co-occurrence information word node document node built using word frequency word 's document frequency turn text classification problem anode classification problem https : //github . com/yao8839836/text_gcn baselines compare text gcn multiple stateof - - art text classification embedding methods tf - idf + lr bag - - words model term frequencyinverse document frequency weighting logistic regression used classifier cnn : convolutional neural network explored cnn -rand uses randomly initialized word embeddings cnn - non- static uses pre-trained word embeddings lstm uses last hidden state representation whole text bi- lstm bi-directional lstm used text classification pv - dbow paragraph vector model orders words text ignored used logistic regression classifier pv - dm paragraph vector model considers word order used logistic regression classifier pte predictive text embedding firstly learns word embedding based heterogeneous text network containing words , documents labels nodes averages word embeddings document embeddings text classification fast text treats average word / n- grams embeddings document embeddings feeds document embeddings linear classifier swem simple word embedding models employs simple pooling strategies operated word embeddings leam label - embedding attentive models embeds words labels joint space text classification utilizes label descriptions graph - cnn - c graph cnn model operates convolutions word embedding similarity graphs chebyshev filter used graph - cnn - graph - cnn - c using spline filter graph - cnn - f graph - cnn - c using fourier filter text gcn set embedding size first convolution layer 200 window size 20 set learning rate 0.02 baseline models using pre-trained word embeddings used 300 dimensional glo word embeddings text gcn performs best significantly outperforms baseline models p < 0.05 based student t- test four datasets pre-trained glo word embeddings provided cnn performs much better ohsumed 20 ng lstm - based models rely pre-trained word embeddings perform better documents shorter pv - dbow achieves comparable results strong baselines 20 ng ohsumed pv - dm performs worse pv - dbow graph - cnn models show competitive performances\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output supervised semi- supervised text categorization text categorization build general framework region embedding + pooling explore sophisticated region embedding via long short - term memory ( lstm ) supervised semi-supervised settings enable learning dependencies larger time lags feasible traditional recurrent networks lstm embed text regions variable ( possibly large ) sizes strategy simplify model including elimination word embedding layer used produce input lstm http : //riejohnson.com/cnn download.html experiments ( supervised ) optimization done sgd mini-batch size 50 100 momentum optionally rmsprop acceleration see one - hot bidirectional lstm pooling outperforms word - vector lstm ( wv - lstm ) datasets review non -lstm baseline methods three four datasets oh - 2 lstmp outperforms svm cnn rcv1 n-gram svm better bag - - word svm bow - cnn outperforms seq-cnn one - hot cnn works surprising well previous best performance 20ng 15.3 dl15 oh - 2 lstmp achieved 13.32 semi-supervised experiments pre-trained wv - lstm clearly outperformed supervised wv - lstm underperformed models region tv-embeddings wv - 2 lstmp using google news vectors performed relatively poorly review performance one - hot cnn one 200 - dim cnn tv-embedding comparable lstm two 100 - dim lstm tv-embeddings lstm rivals outperforms cnn imdb / elec underperforms rcv1 increasing dimensionality lstm tvembeddings 100 300 rcv1 obtain 8.62\n",
      "output class [[False  True False False  True  True  True False False False]]\n",
      "output text classification https : //github.com/wikipedia2vec/wikipedia2vec proposes neural attentive bagof - entities ( naboe ) model neural network model addresses text classification problem modeling semantics target documents using entities kb entity name document model detects entities referred represents document using weighted average embeddings entities weights computed using novel neural attention mechanism less ambiguous meaning relevant document attention mechanism compute weights jointly addressing entity linking entity salience detection tasks model trained using mini-batch sgd learning rate controlled adam mini-batch size set 32 size embeddings words entities set = 300 baselines bow based logistic regression classifier conventional binary bow features fts- brnn based bidirectional rnn gated recurrent units ( gru ) ntee state - - - art model uses multi - layer perceptron classifier features computed using embeddings words entities trained wikipedia results relative baselines models yielded enhanced performance datasets naboe - full model outperformed baseline models naboe-entity model outperformed baseline models terms measures 20ng dataset f 1 score r8 dataset\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output practical text classification train mlstm transformer language models large 40 gb text dataset transfer models two text classification problems binary sentiment multidimensional emotion classification based plutchik wheel emotions results binary sentiment tweets transformer gets close exceed state art sst dataset exceeds ml - stm elmo baseline watson google sentiment apis company tweets multi - label emotion tweets models outperform watson every emotion category sem eval tweets model achieved top macro-averaged f1 score among submission lower scores micro -average f1 transformer lstm find outperforms across plutchik categories models gets lower f 1 scores company tweets dataset equivalent se -m eval categories\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output cross - lingual document classification cross - lingual understanding ( xlu ) xlu focus two approaches domain adaptation first method based masked language model ( mlm ) pre-training using unlabeled target language corpora second method unsupervised data augmentation ( uda ) synthetic paraphrases generated unlabeled corpus model trained label consistency loss fine-tune ( ft ) fine - tuning pre-trained model source - domain training set fine - tune uda ( uda ) utilizes unlabeled data target domain optimizing uda loss function self - training based uda model ( uda + self ) first train ft model uda model choose teacher model used train new xlm student using unlabeled data u tgt target domain looking ft ( xlm ) results substantial gap model performance cross -lingual settings monolingual baselines uda algorithm mlm pre-training offer significant improvements utilizing unlabeled data sentiment classification task ft ( xlm ft ) model usnig mlm pre-training provides larger improvements compared uda method mlm method relatively resource intensive takes longer converge mldoc dataset size unlabeled samples limited uda method helpful observe self - training technique consistently improves teacher model offers best results xlm xlm ft based classifiers mldoc dataset self - training achieves best results comparing best cross - lingual results monolingual fine - tune baseline completely close performance gap utilizing unlabeled data target language framework reaches new state - - - art results improving vanilla xlm baselines 44 % average leveraging unlabeled data domains offer consistent improvement\n",
      "output class [[False  True False False False  True  True False False False]]\n",
      "output semi - supervised text classification adversarial virtual adversarial training text classification tasks text classification sequence models adversarial perturbations consist small modifications many real - valued inputs text classification input discrete represented series highdimensional one - hot vectors perturbation define continuous word embeddings instead discrete word inputs means regularizing text classifier stabilizing classification function used tensorflow gpus https : //github.com/tensorflow/models/tree/master/adversarial_text trained 100,000 steps applied gradient clipping norm set 1.0 parameters except word embeddings regularization recurrent language model applied dropout word embedding layer 0.5 dropout rate bidirectional lstm model used 512 hidden units lstm standard order reversed order sequences 256 dimensional word embeddings shared lstms saw cosine distance adversarial virtual adversarial training much smaller ones baseline random perturbation method shows test performance elec rcv1 datasets see proposed method improved test performance baseline method achieved state art performance unidirectional lstm model improves state art method method bidirectional lstm improves results rcv1 rotten tomatoes dataset adversarial training able improve baseline method adversarial virtual adversarial cost test performance virtual adversarial training worse baseline\n",
      "output class [[False  True False False False False False False False False]]\n",
      "output text classification introduce interaction mechanism ( wang jiang 2016 b ) capable incorporating word - level matching signals text classification based upon interaction mechanism devise explicit interaction model dubbed exam consists three main components word - level encoder interaction layer aggregation layer word - level encoder projects textual contents word - level representations interaction layer calculates matching scores words classes last layer aggregates matching scores predictions class experiments multi - class classification chose region embedding encoder exam region size 7 embedding size 128 used adam ( kingma ba 2014 ) optimizer initial learning rate 0.0001 batch size set 16 aggregation mlp set size hidden layer 2 times interaction feature length models implemented trained mxnet single nvidia titan xp baselines mainly three variants models based feature engineering char - based deep models word - based deep models models based feature engineering get worst results five datasets char - based models get highest scores two amazon datasets word - based baselines exceed variants three datasets lose two amazon datasets five baselines w.c region emb performs best see exam achieves best performance three datasets ag yah . a. dbp yah.a . exam improves best performance 1.1 % multi - label classification implemented baseline models exam mxnet used matrix trained word2vec initialize embedding layer embedding size 256 adopted gru encoder gru cell 1,024 hidden states accumulated mlp 60 hidden units applied adam optimize models nvidia titan xp batch size 1000 initial learning rate 0.001 validation set applied early - stopping avoid overfitting word - based models better char - based models kanshan - cup dataset models achieve state - - - art performance two different datasets\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output text classification propose task - oriented word embedding method denoted towe words ' contextual information task information inherently jointed model task information regularize distribution salient words adjust distribution words embedding space bow method represents document bag words weighting scheme tfidf word2 vec method neural network language method learns word embeddings maximizing conditional probability leveraging contextual information method performs better methods towe - sg method significantly outperforms baselines 20 new group 5 abstract group mr word embedding methods outperform basic bag - - words methods achieves better performance retrofit method outperforms twe method document - level sentence - level tasks\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output text classification scale baselines large corpus large output space show linear models rank constraint fast loss approximation train billion words within ten minutes sentiment analysis use 10 hidden units run fasttext 5 epochs learning rate selected validation set 0.05 , 0.1 , 0.25 , 0.5 adding bigram information improves performance 1 - 4 % accuracy slightly better char - cnn char - crnn bit worse vdcnn trigrams performance sogou goes 97.1 % using n-grams 5 leads best performance tag prediction consider frequency - based baseline predicts frequent tag compare tagspace ( weston et al . , 2014 ) tag prediction model results adding bigrams gives significant boost accuracy\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output text classification propose making use label embedding framework label - embedding attentive model ( leam ) improve text classification proposed leam implemented jointly embedding word label latent space text representations constructed directly using text - label compatibility label embedding framework label - attentive text representation informative downstream classification task leam learning procedure involves series basic algebraic operations use 300 - dimensional glo word embeddings word embeddings label embeddings - - vocabulary ( oov ) words initialized uniform distribution range [ ? 0.01 , 0.01 ] final classifier implemented mlp layer followed sigmoid softmax function train model 's parameters adam optimizer ( kingma ba , 2014 ) initial learning rate 0.001 minibatch size 100 dropout regularization employed final mlp layer dropout rate 0.5 model implemented using tensorflow trained gpu titan x https : //github.com/guoyinwang/leam compare logistic regression model bag - ofwords bidirectional gated recurrent unit ( bi - gru ) single - layer 1 convolutional network compare multi-label classification clinical text condensed memory networks ( c - memnn ) attentive lstm convolutional attention ( caml ) leam provides best auc score better f1 p @ 5 values methods except cnn cnn consistently outperforms basic bi - gru architecture logistic regression baseline performs worse deep learning architectures\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output text classification introduce new architecture short c - lstm combining cnn lstm model sentences design simple end - - end , unified architecture feeding output one - layer cnn lstm cnn constructed top pre-trained word vectors massive unlabeled text data learn higher - level representions n-grams learn sequential correlations higher - level suqence representations feature maps cnn organized sequential window features serve input lstm choose sequence - based input feeding neural network implement model based theano train model gpu use one convolutional layer one lstm layer trec number filters set 300 memory dimension set 300 word vector layer lstm layer dropped outwith probability 0.5 add l2 regularization factor 0.001 weights softmax layer tasks results sentiment classification achieve fourth best published result 5 - class classification task binary classification task achieve comparable results respect state - - - art ones question type classification result consistently outperforms published neural baseline models close state - - - art svm found single convolutional layer filter length 3 always outperforms cases shown single convolutional layer filter length 3 performs best among filter configurations multiple convolutional layers parallel shown filter configurations filter length 3 performs better without tri-gram filters\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output text classification develop deep architectures able learn hierarchical representations whole sentences jointly task use deep architectures many convolutional layers using 29 layers dictionary consists abcdefghijklmnopqrstuvwxyz0123456 789- , ; . ! ? : ' '' / | # $ % & * ' += < > ( ) [ ] { } special padding space unknown token input text padded fixed size 1014 character embedding size 16 training performed sgd using mini-batch size 128 initial learning rate 0.01 momentum 0.9 implementation done using torch 7 performed single nvidia k40 gpu use temporal batch norm without dropout deep architecture works well big data sets smallest depth use 9 convolutional layers see model performs better zhang 's convolutional baselines includes 6 convolutional layers different architecture important decrease classification error observed largest data set amazon full 3 million training samples small depth temporal max - pooling works best data sets\n",
      "output class [[False  True False  True False False False False  True False]]\n",
      "output text categorization call deep pyramid cnn ( dpcnn ) computation time per layer decreases exponentially pyramid shape converting discrete text continuous representation dpcnn architecture alternates convolution block downsampling layer network depth treated meta-parameter computational complexity bounded twice one convolution block dpcnn 15 weight layers first layer performs text region embedding stacking convolution blocks interleaved pooling layers stride 2 downsampling final pooling layer aggregates internal data document one vector use max pooling pooling layers results large data results five datasets dpcnn outperforms previous results small data results dpcnn performances 100 - dim unsupervised embed - dings turned good 300 - dim unsupervised embeddings shallowcnn rivals dpcnn\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output multilingual document classification cross - lingual document classification use data reuters corpus volume 2 define new cross - lingual document classification tasks eight different languages namely english french spanish italian german russian chinese japanese language define train , development test corpus zero - shot cross - lingual document classification classifiers based multicca embeddings perform well development corpus accuracies close exceeding 90 % system trained english transfered different languages scores best three seven languages de zh transfer accuracies quite low training classifiers languages english particular russian chinese japanese systems using multilingual sentence embeddings seem robust less language specific training german french leads better transfer performance training english crosslingual transfer different languages like chinese russian achieves remarkable results joint multilingual document classification leads important improvement languages comparison zero - shot targeted transfer learning\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output text categorization incorporate positioninvariance rnn propose novel model named disconnected recurrent neural network ( drnn ) maintain position - invariance utilize max pooling extract important information proposed model regarded special 1d cnn convolution kernels replaced recurrent units utilize 300d glo 840b vectors ( pennington et al . , 2014 ) pre-trained word embeddings use adadelta ( zeiler , 2012 ) optimize trainable parameters adadelta set 1 e ? 6 avoid gradient explosion problem apply gradient norm clipping batch size set 128 see deep cnn ( vdcnn ) performs well large datasets shows model achieves 10 - 50 % relative error reduction compared char - crnn drnn performs far better cnn model drnn achieves much better performance gru lstm\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output document classification question overly complex neural architectures focusing document classification starting large - scale reproducibility study several recent neural models find simple bi-directional lstm ( bilstm ) architecture appropriate regularization yields accuracy f 1 competitive exceed state art four standard benchmark datasets conduct large - scale reproducibility study involving han , xml - cnn , kimcnn , sgm compare neural approaches logistic regression ( lr ) support vector machines ( svms ) lr model trained using one - vs - rest multi-label objective svm trained linear kernel performed nvidia gtx 1080 rtx 2080 ti gpus pytorch 0.4.1 backend framework use scikitlearn 0.19.2 computing tf - idf vectors implementing lr svms see simple lstm reg model achieves state art reuters imdb establishing mean scores 87.0 52.8 f 1 score accuracy test sets reuters imdb observe lstm reg consistently improves upon performance lstm base across tasks lstm reg runs attain state - - theart test f 1 scores aapd non-neural lr svm baselines perform remarkably well reuters svm beats many neural baselines including non-regularized lstm base aapd svm ties beats models losing sgm compared svm lr baseline appears better suited single - label datasets imdb yelp 2014 achieves better accuracy svm\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output text classification proposes bidirectional long short - term memory networks two - dimensional max pooling blstm - 2dpooling capture features time - step dimension feature vector dimension first utilizes bidirectional long short - term memory networks ( blstm ) transform text vectors 2d max pooling operation obtain fixed - length vector applies 2d convolution ( blstm - 2dcnn ) capture meaningful features represent input text dimension word embeddings 300 hidden units lstm 300 use 100 convolutional filters window sizes ( 3 , 3 ) 2d pooling size ( 2 , 2 ) set mini-batch size 10 learning rate adadelta default value 1.0 regularization employ dropout operation dropout rate 0.5 word embeddings 0.2 blstm layer 0.4 penultimate layer use l 2 penalty coefficient 10 ? 5 parameters results blstm - 2dcnn model achieves excellent performance 4 6 tasks 52.4 % 89.5 % test accuracies sst - 1 sst - 2 blstm - 2dpooling performs worse state - - - art models blstm - cnn beats baselines sst - 1 sst - 2 trec datasets subj mr datasets blstm - 2dcnn gets second higher accuracies compared rcnn blstm - 2dcnn achieves comparable result renn proposed two models depend external language - specific features dependency parse trees dscnn blstm - 2dcnn outperforms five datasets\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output text classification treating text kind raw signal character level applying temporal ( one-dimensional ) convnets traditional methods bag - - words tfidf bag - - words model constructed selecting 50,000 frequent words training subset bag - - ngrams tfidf bag - - ngrams models constructed selecting 500,000 frequent n-grams ( 5 - grams ) training subset dataset bag - - means word embedding uses k-means word2vec learnt training subset dataset deep learning methods word - based convnets long - short term memory character - level convnets work text classification without need words traditional methods like n-grams tfidf remain strong candidates dataset size several hundreds thousands dataset goes scale several millions observe character - level convnets better conv nets may work well user - generated data choice alphabet makes difference semantics tasks may matter\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "completed document_classification 0\n",
      "output word sense disambiguation wsd aim modeling joint disambiguation target text whole terms sequence labeling problem design , analyze compare experimentally various neural architectures different complexities ranging single bidirectional long short - term memory sequence - tosequence approach architecture reflects particular way modeling disambiguation problem trained end - - end sense - annotated text sense labels learn single - words model training data without fine tuning explicit engineering local features used layer word embeddings pre-trained 8 english uk wac corpus initialization kept fixed training process architectures employed 2 layers bidirectional lstm 2048 hidden units 1024 units per direction report f1 - score - dividual test set obtained concatenation four test sets divided part - - speech tag considered context2 vec makes sense original implementation best configuration integrates word embeddings using exponential decay blstm seq2seq achieved results state - - - art statistically equivalent best supervised system benchmark performing par word experts tuned explicitly engineered features blstm models tended consistently outperform seq2seq counterparts english - words wsd worth noting rnn - based architectures outperformed classical supervised approaches dealing verbs blstm seq2seq outperformed ukb ims trained semcor well recent supervised approaches based distributional semantics neural architectures multilingual - words wsd f - score figures show bilingual multilingual models consistently outperformed mfs baseline achieved results competitive best participating systems task note overall f- score performance change substantially moving bilingual multilingual models\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output neural word sense disambiguation neural wsd wsd word sense disambiguation ( wsd ) taking advantage semantic relationships senses included wordnet hypernymy hyponymy meronymy antonymy based observation sense closest related senses share common idea concept bert used model named bert - largecased pytorch implementation consists vectors dimension 1024 trained book corpus english wikipedia transformer encoder layers 6 layers 8 attention heads hidden size 2048 dropout 0.1 observe systems use sense vocabulary compression hypernyms relations obtain scores overall equivalent systems use comparison works outperform systematically state art every task additional training corpus ( wngc ) use bert input embeddings major impact results lead scores state art using bert instead elmo glo improves score approximately 3 5 points every experiment adding wngc training data improves approximately 2 points using ensembles adds roughly another 1 point final f1 score compression method relations seems negatively impact results cases\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output word sense disambiguation word sense disambiguation ( wsd ) wsd show cwes utilized directly approach wsd task learn semantic capabilities cwes employ simple , yet interpretable approach wsd using k -nearest neighbor classification ( knn ) approach contextualized embeddings simple k nn elmo well bert embeddings beats state art lexical sample task bert outperforms others se - 3 task observe major performance drop approach two - words wsd tasks training data provided along test set similarity contextualized embeddings largely relies semantically structurally similar sentence contexts polysemic target words se - 2 se - 3 training datasets provide cwes word sense approach performs better growing number cwes cwes organize well spherically shaped form embedding space nearest neighbors k- optimization senseval - 2 senseval - 3 achieve new state - - - art result s7 - * achieve minor improvements higher k drastically lack behind state art senses cwe space investigate different cwe models encode information distinguishable senses vector space flair embeddings hardly allow distinguish clusters senses scattered across entire plot elmo embedding space major senses slightly separated different regions point cloud\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output neural word sense disambiguation word sense disambiguation ( wsd ) wsd propose novel model gas gloss - augmented wsd neural network variant memory network gas jointly encodes context glosses target word models semantic relationship context glosses memory module inner relationship glosses context accurately employ multiple passes operation within memory re-reading process adopt two memory updating mechanisms neural word sense disambiguation use pre-trained word embeddings 300 dimensions keep fixed training process employ 256 hidden units gloss module context module means n = 256 orthogonal initialization used weights lstm random uniform initialization range [ - 0.1 , 0.1 ] others assign gloss expansion depth k value 4 experiment number passes | | 1 5 framework finding | | = 3 performs best adam optimizer training process 0.001 initial learning rate avoid overfitting use dropout regularization set drop rate 0.5 training 100 epochs early stopping validation loss n't improve within last 10 epochs knowledge - based systems babelfy exploits semantic network structure babelnet builds unified graph - based architecture wsd entity linking supervised systems ims selects linear support vector machine ( svm ) classifier use set features surrounding target word within limited window pos tags local words local collocations ims +emb selects ims underlying framework makes use word embeddings features makes hard beat inmost wsd datasets neural - based systems bi- lstm leverages bidirectional lstm network shares model parameters among words bi-lstm +att.+ lex variant bi- lstm +att.+ lex+p os transfers wsd sequence learning task propose multi - task learning framework wsd pos tagging coarse - grained semantic labels ( lex ) english - words results gas gas ext achieves state - - theart performance concatenation test datasets find gas ext concatenation memory updating strategy achieves best results 70.6 concatenation four test datasets appropriate number passes boost performance avoid - fitting model multiple passes analysis shows multiple passes operation performs better one pass increasing number passes f1 - score increases number passes larger 3 f1- score stops increasing even decreases due over-fitting\n",
      "output class [[False  True False False  True  True  True False False False]]\n",
      "output neural word sense disambiguation word sense disambiguation ( wsd ) wsd propose method reducing vocabulary senses word net selecting minimal set senses required differentiating meaning every word https : //github.com/getalp/disambiguate showed vocabulary reduction method improves coverage supervised systems overall wordnet vocabulary see coverage improvement holds true evaluation tasks training sets difference scores obtained system using sense vocabulary reduction significant notice large gap semeval 2013 task semcor used alone training add wordnet gloss tagged training data obtain systematically state art results tasks except senseval 3 sense reduction method consistently improves decreases score every task result roughly without sense reduction ensembling efficient method wsd improves systematically results ensembles scores significantly higher applying vocabulary reduction algorithm\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output deep contextualized word representations deep contextualized word representation pre-trained word representations introduce new type deep contextualized word representation representations differ traditional word type embeddings token assigned representation function entire input sentence use vectors derived bidirectional lstm trained coupled lan - guage model ( lm ) objective large text corpus call elmo ( embeddings language models ) representations elmo representations deep function internal layers bilm learn linear combination vectors stacked input word end task markedly improves performance using top lstm layer using intrinsic evaluations show higher - level lstm states capture context - dependent aspects word meaning lowerlevel states model aspects syntax question answering adding elmo baseline model test set f 1 improved 4.7 % 81.1 % 85.8 % 24.9 % relative error reduction baseline improving overall single model state - - - art 1.4 % increase 4.7 % elmo significantly larger 1.8 % improvement adding cove baseline model textual entailment adding elmo esim model improves accuracy average 0.7 % across five random seeds semantic role labeling adding elmo improved average f 1 3.2 % 67.2 70.4 improving previous best ensemble result 1.6 % f 1 named entity extraction elmo enhanced bilstm - crf achieves 92 . 22 % f 1 averaged five runs\n",
      "output class [[False  True False False False  True  True False False False]]\n",
      "output word sense disambiguation word sense disambiguation ( wsd ) wsd modeling sequence words surrounding target word using represent words real valued vector representation i.e . word embeddings source code implemented using tensorflow embeddings initialized using set freely available 2 glo vectors trained wikipedia gigaword words included set initialized n ( 0 , 0.1 ) proposed model achieves top score se2 tied ims + adapted cw se3 see dropword consistently improves results se2 se3 randomizing order input words yields substantially worse result system makes use information pre-trained word embeddings\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output semi-supervised word sense disambiguation word sense disambiguation ( wsd ) wsd describe two novel wsd algorithms first based long short term memory ( lstm ) take account word order classifying present semi-supervised algorithm uses label propagation label unlabeled sentences based similarity labeled ones sem eval tasks proposed algorithms achieve highest - words f 1 scores except sem - eval 2013 unified wsd highest f 1 score nouns ( sem - eval - 7 coarse ) algorithms outperform unified wsd part - - speech tags word2 vec vectors vs. lstm performs similar ims + word2 vec ( : semcor ) shows lstm classifier outperforms word2 vec classifier across board sem cor vs. omsti lstm classifier trained omsti performs worse trained semcor svm classifier able learn model copes noise naive nearest neighbor classifiers learned model deal less well noisy labels noad eval lstm classifier frequent sense lstm outperforms word2vec 10 % overall words gains verbs adverbs change training data semcor ( masc ) trained classifier par noad trained classifier f1 score change language model capacity balance accuracy resource usage use second best lstm model ( h = 2048 p = 512 ) default semi-supervised wsd lp yield clear benefits using word2 vec language model see significant improvements 6.3 % increase semcor 7.3 % increase masc using lp lstm language model change seed data lp substantially improves classifier f1 training datasets semcor + noad masc + noad change graph density construct lp graph connecting two nodes affinity 95 % percentile f1 scores relatively stable percentile ranges 85 98 decrease percentile drops 80\n",
      "output class [[False  True False False False  True  True False False  True]]\n",
      "output named entity disambiguation named entity disambiguation ( ned ) ned propose method construct novel embedding jointly maps words entities continuous vector space similar words entities placed close one another vector space measure similarity pair items simply computing cosine similarity model based skip - gram model learns predict context word given target word consists following three models conventional skip - gram model learns predict neighboring words given target word text corpora kb graph model learns estimate neighboring entities given target entity link graph kb anchor context model learns predict neighboring words given target entity using anchors context words kb proposed embedding develop straightforward ned method computes two contexts using textual context similarity coherence ned method combines contexts several standard features using supervised machine learning method achieved enhanced performance conll tac 2010 datasets found choice candidate generation method considerably affected performance conll dataset outperformed state - - - art methods datasets\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output knowledge - based word sense disambiguation disambiguation wsd word sense disambiguation ( wsd ) propose novel knowledge - based wsd algorithm - word wsd task utilizes whole document context word model whole document wsd leverage formalism topic models especially latent dirichlet allocation ( lda ) method variant lda topic proportions document replaced synset proportions document use non-uniform prior synset distribution words model frequency words within synset model relationships synsets using logisticnormal prior drawing synset proportions document proposed method denoted wsd - tm outperforms state - - - art wsd system significant margin achieving overall f1 - score 66.9 compared moro14 's score 65.5 observe performance proposed model much worse best supervised system proposed system outperforms previous knowledgebased systems overall parts speech\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output word sense disambiguation word sense disambiguation ( wsd ) wsd develop supervised wsd model leverages bidirectional long short - term memory ( blstm ) network works neural sense vectors i.e . sense embeddings learned model training employs neural word vectors i.e . word embeddings learned unsupervised deep learning approach called glove context words - - models comparisons show single model sits among 5 top - performing algorithms shows results top - performing low - performing supervised algorithms within - - model comparisons reverse sequential follow information bidirectional lstm shuffle order context words replace bidirectional lstms two different fully - connected networks size 50 achieved results notably less 72.5 %\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output word sense disambiguation ( wsd ) wsd propose novel model gas gloss - augmented wsd neural network variant memory network gas jointly encodes context glosses target word models semantic relationship context glosses memory module measure inner relationship glosses context accurately employ multiple passes operation within memory re-reading process adopt two memory updating mechanisms neural word sense disambiguation use pre-trained word embeddings 300 dimensions keep fixed training process employ 256 hidden units gloss module context module means n = 256 orthogonal initialization used weights lstm random uniform initialization range [ - 0.1 , 0.1 ] assign gloss expansion depth k value 4 experiment number passes | | 1 5 framework adam optimizer training process 0.001 initial learning rate dropout regularization set drop rate 0.5 training runs 100 epochs early stopping validation loss n't improve within 10 epochs english - words results gas gas ext achieves state - - theart performance concatenation test datasets performs best test sets find gas ext concatenation memory updating strategy achieves best results 70.6 concatenation four test datasets find best model outperforms previous best neural network models every individual test set best model beat ims + emb se3 , se13 se15 test sets incorporating glosses neural wsd greatly improve performance extending original gloss boost results compared bi - lstm baseline proposed model greatly improves wsd task 2.2 % f1 - score gloss knowledge compared gas gas ext improve performance help extended glosses semantic relations\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output joint entity disambiguation entity disambiguation ( ed ) ed use deep learning learn basic features combinations scratch implemented torch framework entity embeddings use wikipedia ( feb 2014 ) corpus training entity vectors initialized randomly 0 mean normal distribution standard deviation 1 train entity vector entity 's wikipedia canonical description page 400 iterations use adagrad learning rate 0.3 choose embedding size = 300 pre-trained ( fixed ) word2 vec word vectors 8 , ? = 0.6 , ? = 0.1 window size 20 hyperlinks training takes 20 hours single titanx gpu 12 gb memory local global ed models trained aida - train ( multiple epochs ) validated aida - tested aida - b use adam learning rate 1e - 4 validation accuracy exceeds 90 % setting 1e - 5 regularize use early stopping stop learning validation accuracy increase 500 epochs training single gpu takes average 2 ms per mention 16 hours 1250 epochs aida - train obtain state art accuracy aida analyzed accuracy aida - b dataset shows method performs well harder cases http : //github.com/dalab/deep-ed\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output named entity disambiguation named entity disambiguation ( ned ) ned describe new contextualized embedding model words entities ned proposed model based bidirectional transformer encoder takes sequence words entities input text produces contextualized embedding word entity propose masked entity prediction new task aims train embedding model predicting randomly masked entities based words non-masked entities input text trained model using texts entity annotations retrieved wikipedia models outperformed previously proposed models using pseudo entity annotations boosted accuracy 0.3 % models achieved new state - - - art results four five datasets namely msnbc aquaint ace2004 wned - wiki performed competitive wned - clueweb dataset improved performance aquaint ace2004 datasets\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output entity linking entity linking ( el ) el perform entity mention detection entity disambiguation jointly single neural model makes whole process end - - end differentiable overcome noise data automatically learn features set contexts different granularity levels level granularity handled separate component model token - level component extracts higher - level features whole question context character - level component builds lower - level features candidate n-gram extract features knowledge base context candidate entity character - level features extracted entity label higher - level features based entities surrounding candidate entity knowledge graph aggregated used predict n-gram entity mention entity linked https : //github.com/ukplab/ starsem2018-entity-linking existing systems compare dbpedia spotlight used several qa systems represents strong baseline entity linking - mart system include heuristics baseline ranks candidate entities simplified vcg employ features cover frequency entity wikipedia edit distance label entity token n-gram number entities relations connected entity kb word overlap input question labels connected entities relations length n-gram vcg model shows overall f- score result better dbpedia spotlight baseline wide margin model achieves higher precision values compared approaches keep satisfactory level recall\n",
      "output class [[False  True False False False  True  True False False False]]\n",
      "output learning word embeddings context dependent representations phrases present relic ( representations entities learned context ) table independent entity embeddings trained match fixed length vector representations textual context entities seen apply relic entity typing entity linking trivia question answering train relic obtain data 2018 - 10 - 22 dump english wikipedia limit context sentence 128 tokens set entity embedding size = 300 train model using tensor flow entity linking adopt standard conll - aida training set alias table relic matches state art benchmark relic 's entity linking performance boosted even higher adoption commonly used entity linking features relic outperforms prior work 5 % training data entity - level fine typing show relic significantly outperforms prior results datasets typenet aggregate mention - level types train structured loss based typenet hierarchy still outperformed flat classifier binary labels effect masking clear masking mentions training beneficial entity typing tasks detrimental entity linking higher mask rate leads better performance low high - data situations mask rate 10 % relic nears optimum performance tasks - shot category completion due incompleteness figment typenet type systems relic 's performance approaching upper bound supervised tasks trivia question answering observe retrieve - - read approach taken orqa outperforms direct answer retrieval approach taken relic see relic 's much faster nearest neighbor lookup captures 80 % orqa 's performance relic outperforms 's reading comprehension baseline 20 points\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output learning distributed representations texts entities knowledge base propose neural text - entity encoder ( ntee ) neural network model jointly learn jointly learn distributed representations texts ( i.e . , sentences paragraphs ) kb entities distributed representations texts ( i.e . , sentences paragraphs ) kb entities every text kb model predict relevant entities places text relevant entities close continuous vector space use humanedited entity annotations obtained wikipedia supervised data relevant entities texts containing annotations kb entities conventionally used model semantics texts representative example explicit semantic analysis ( esa ) represents semantics text using sparse vector space dimension corresponds relevance score text entity esa shows text accurately represented using small set relevant entities placing texts entities vector space enables easily compute similarity texts entities https : //github.com/ studio-ousia /ntee bow using logistic regression ( lr ) classifier trained binary bow features bow - dt based bow baseline augmented feature set dependency relation indicators qanta based recursive neural network derive distributed representations questions fts - brnn based bidirectional recurrent neural network ( rnn ) gated recurrent units ( gru ) method compared state - - - art methods qanta fts - brnn clearly outperformed method mostly performed perfect terms predicting types target answers locations events people\n",
      "output class [[False  True False False False  True  True False False False]]\n",
      "completed entity_linking 0\n",
      "output robust face alignment face alignment proposing novel face alignment method dub deep alignment network ( dan ) based multistage neural network stage refines landmark positions estimated previous stage input stage algorithm face image normalized canonical pose image learned dense layer previous stage make use entire face image process face alignment additionally input stage landmark heatmap data augmentation total 10 images created input image training set training performed using theano 0.9.0 lasagne 0.2 optimization use adam stochastic optimization initial step size 0.001 mini batch size 64 validation use random subset 100 images training set python implementation runs 73 fps images processed parallel 45 fps images processed sequentially geforce gtx 1070 gpu failure rate reduction 60 % 300 w private test set 72 % 300w public test set 9 % improvement mean error challenging subset\n",
      "output class [[False  True False  True False False False False False False]]\n",
      "output face alignment 3d reconstruction utilize two network decoders instead two pca spaces shape texture model components design different networks shape texture multi - layer perceptron ( mlp ) shape convolutional neural network ( cnn ) texture decoder take shape texture representation input output dense 3 face face texture learn fitting algorithm nonlinear 3 dmm formulated cnn encoder encoder takes 2 face image input generates shape texture parameters two decoders estimate 3d face texture 3 face texture perfectly reconstruct input face fitting algorithm 3 dmm well learnt differentiable rendering layer generate reconstructed face fusing 3d face texture camera projection parameters estimated encoder endto - end learning scheme encoder two decoders learnt jointly minimize difference reconstructed face input face jointly learning 3 dmm model fitting encoder leverage large collection unconstrained 2d images without relying 3d scans show significantly improved shape texture representation power linear 3 dmm model optimized using adam optimizer initial learning rate 0.001 minimizing l 0 0.0002 minimizing l set following parameters q 53 , 215 u = v 128 l = l 160 representation power minimize reconstruction error image space rendering layer groundtruth nonlinear texture closer groundtruth linear model especially - - wild images nonlinear model significantly lower l 1 reconstruction error lin significantly smaller reconstruction error linear model 0.0196 vs. 0.0241 3d reconstruction obtain low error comparable optimization - based methods 3d face reconstruction achieve - par results garrido et al . surpassing regression methods using global image - based discriminator redundant global structure guaranteed rendering layer using global image - based discriminator cause severe artifacts resultant texture patchgan offers higher realism fewer artifacts\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output face alignment across large poses face alignment fit 3d dense face model image call 3d dense face alignment ( 3ddfa ) resolve fitting process ddfa propose cascaded convolutional neutral network ( cnn ) based regression method adopt cnn fit 3d face model specifically designed feature namely projected normalized coordinate code ( pncc ) weighted parameter distance cost ( wpdc ) proposed cost function first attempt solve 3d face alignment cnn enable training 3ddfa construct face database containing pairs 2d face images 3d face models propose face profiling algorithm synthesize 60 k + training samples across large poses synthesized samples simulate face appearances large poses boost performance prior proposed face alignment algorithms http : //www.cbsr.ia.ac.cn/users / xiangyuzhu/ error reduction cascade testing error reduced due initialization regeneration generic cascade process training testing errors converge fast 2 iterations initialization regeneration training error updated beginning iteration testing error continues descend performance different costs pdc well model fitting error converges unsatisfied result wpdc explicitly models priority parameter adaptively optimizes parameter weights leading best result large pose face alignment aflw protocol methods benefits substantially face profiling dealing large poses 3ddfa reaches state art 2d methods beyond medium poses minimum standard deviation 3ddfa demonstrates robustness pose variations performance 3ddfa improved sdm landmark refinement 3d face alignment aflw2000-3d methods despite ground truth bounding boxes performance standard deviation obviously reduced considering landmarks common challenging full tspm ddfa demonstrates competitive performance common set state - - - art performance challenging set\n",
      "output class [[False  True False False False  True  True False False False]]\n",
      "output joint 3d face reconstruction dense face alignment single image 3 face reconstruction single 2d image 3 face reconstruction single 2d image 3 face reconstruction overcome intrinsic limitation existing 3 face recovery models propose novel learning method leverages 2d `` - - wild `` face images supervise facilitate 3d face model learning design novel self - supervised learning method train 3 face model weak supervision 2d images model recover 2d landmarks predicted 3d ones via direct 3d - - 2d projection proposed method exploits cycle - consistency 2d landmark predictions facilitate overall learning procedure method exploits self - critic learning proposed 2 dasl implemented pytorch sgd optimizer cnn regressor learning rate beginning 5 10 ? 5 decays exponentially discriminator uses adam optimizer fixed learning rate 1 10 ? 4 use two - stage strategy train model first stage train model using overall loss l second stage fine - tune model using vertex distance cost dense face alignment 2 dasl achieves lowest nme evaluation 2 3d coordinates among methods 3 dmm - based methods 3 ddfa defa method outperforms large margin 68 spare landmarks dense coordinates method achieves lowest mean nme two datasets lowest nme across poses aflw2000 - 3d 2dasl performs better prnet reducing nme 0.09 0.08 aflw2000 - 3d aflw - lfpa large poses ( 60 90 ) 2 dasl achieves 0.2 lower nme prnet 3 face reconstruction 3d reconstruction results 2 dasl outperforms 3 ddfa 0.39 2.29 defa adding weights central points facial landmarks reduces nme 0.09 0.23 two stages self - critic learning used nme increases 0.04/0.18 / without weight mask self - supervision scheme reduce nme 0.1 weight mask used 0.23 weight mask removed found flms input accelerate convergence training process\n",
      "output class [[False  True False False  True False  True False False  True]]\n",
      "output facial alignment robust face recognition analysis localization facial features observe fairly accurate 3 models generated using simple mean shape deformed input image relatively low computational cost compared approaches network implemented caffe framework new layer consisting 3d tps transformation module camera projection module bilinear sampler module adopt two architectures alexnet vgg - 16 pre-trained models shared feature extraction networks use convolution layers pre-trained models initialize alexnet architecture freeze first layer vgg - 16 architecture first 4 layers frozen 2d landmark regression implemented attaching additional layers top last convolution layer\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output robust facial landmark localisation facial landmark localisation face alignment novel loss function namely wing loss improve deep neural network training capability small medium range errors data augmentation strategy i.e . pose - based data balancing compensates low frequency occurrence samples large - - plane head rotations training set two - stage facial landmark localisation framework performance boosting used matlab 2017a mat - convnet toolbox training testing networks conducted server running ubuntu 16.04 intel xeon e5-2667 v4 cpu 256 gb ram 4 nvidia geforce gtx titan x ( pascal ) cards use one gpu card use measuring run time set weight decay 5 10 ? 4 momentum 0.9 batch size 8 network training model trained 120 k iterations standard relu function used nonlinear activation max pooling stride 2 used downsize feature maps convolutional layer used 3 3 kernels stride 1 proposed pdb strategy number bins k set 17 aflw 9 300w cnn - 6 input image size 64 64 3 reduced learning rate 3 10 ? 6 3 10 ? 8 l2 loss 3 10 ? 5 3 10 ? 7 loss functions parameters wing loss set tow = 10 = 2 cnn - 7 input image size 128 128 3 reduced learning rate 1 10 ? 6 1 10 ? 8 l2 loss 1 10 ? 5 1 10 ? 7 loss functions perform data augmentation randomly rotated training image [ ? 30 , 30 ] degrees cnn - 6 [ ? 10 , 10 ] degrees cnn - 7 bounding box perturbation applied random translations upper-left bottom - right corners face bounding box within 5 % bounding set state - - - art approaches including sdm ert rcpr cfss lbf grf ccl dac - csr tr - drn randomly injected gaussian blur ( ? = 1 ) training image probability 50 % aflw cnn - 6/7 network outperforms approaches simply switching loss function l2 l1 smooth l1 performance method improved significantly use newly proposed wing loss function improves accuracy 300 w two - stage landmark localisation framework pdb strategy newly proposed wing loss function outperforms stateof - - art algorithms 300 w dataset error reduced almost 20 % compared current best result reported rar algorithm\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output facial landmarks detection develop self - iterative regression ( sir ) framework means powerful representation convolutional neural network ( cnn ) train one regressor learn descent directions coarse fine stages obtain discriminative landmarks features proposed landmarks - attention network ( lan ) focuses appearance around landmarks first concurrently extracts local landmarks ' features obtains holistic increment significantly reduces dimension final feature layer number model parameters perform experiments based machine core i7 - 5930 k cpu 32 gb memory gtx 1080 gpu 8g video memory detected faces resized 256 256 location patch size 57 57 cnn structure rectified linear unit ( relu ) adopted activation function optimizer adadelta ( zeiler 2012 ) approach learning rate set 0.1 weight decay set 1 e ? training cnn requires around 2 days nme results shows sir performs comparatively rar outperform existing methods challenging ibug subset method achieves robust performance large pose , expression illumination environment sir method outperform state - - - art methods according ced curve\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output dense face alignment face alignment learn cnn fit 3 face model face image tackle first challenge limited landmark labeling employ additional constraints include contour constraint contour predicted shape match detected 2 face boundary sift constraint sift key points detected two face images individual map vertexes 3d face model constraints integrated cnn training additional loss function terms end - - end training results enhanced cnn 3 face model fitting second challenge leveraging multiple datasets 3d face model fitting approach inherent advantage handling multiple training databases dense face alignment train network use 20 , 10 , 10 epochs stage 1 3 set initial global learning rate 1 e ? 3 reduce learning rate factor 10 training error approaches plateau minibatch size 32 weight decay 0.005 leak factor leaky relu 0.01 aflw - lfpa method outperforms best methods large margin 17.8 % improvement aflw2000 - 3d method shows large improvement images yaw angle [ 60 , 90 ] method improves performance 28 % 7.93 5.68 ijb - dataset method reaches higher accuracy 300w dataset method second best method challenging set performance method comparable methods designed near - frontal datasets accuracy method aflw2000 - 3d consistently improves adding datasets aflw - pifa dataset method achieves 9.5 % 20 % relative improvement utilizing datasets stage 2 stage 3 first stage including datasets second third stages 26 % relative improvement achieve nme 3.86 % comparing lfc + spc lfc + cfc performances shows cfc helpful spc using constraints achieves best performance result shows images nme - lp 5 % 15 % spc helpful\n",
      "output class [[False  True False False  True  True  True False False  True]]\n",
      "output joint 3d face reconstruction dense alignment https : //github.com/yadiraf/prnet 3 face reconstruction face alignment propose end - - end method called position map regression network ( prn ) jointly predict dense alignment reconstruct 3 face shape design uv position map 2d image recording 3d coordinates complete facial point cloud keeping semantic meaning uv place train simple encoder - decoder network weighted loss focuses discriminative region regress uv position map single 2 facial image optimization use adam optimizer learning rate begins 0.0001 decays half 5 epochs batch size set 16 training codes implemented tensorflow 3d face alignment result slightly outperforms state - - - art method 3d - fan calculating per distance 2d coordinates considering depth value performance discrepancy method 3d - fan increases method robust change pose datasets examples aflw2000 - 3 dataset show predictions accurate ground truth outperforms best methods large margin 27 % 2 3d coordinates network trained without using weight mask worst performance compared two settings adding weights specific regions 68 facial landmarks central face region weight ratio shows considerable improvement 68 points datasets weight ratio\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output large pose 3d face reconstruction single image 3 face reconstruction bypasses many difficulties encountered 3d face reconstruction using novel volumetric representation 3d facial geometry appropriate cnn architecture trained regress directly 2 facial image corresponding 3d volume architectures trained end - - end using rmsprop initial learning rate 10 ? 4 lowered 40 epochs 10 ? 5 training random augmentation applied input sample ( face image ) corresponding target ( 3d volume ) applied - plane rotation r translation z , scale 20 % cases input target flipped horizontally input samples adjusted colour scaling rgb channel case vrn - guided landmark detection module trained regress gaussians standard deviation approximately 3 pixels ( ? = 1 ) volumetric regression networks largely outperform 3ddfa eos datasets vrns perform well across whole spectrum facial poses expressions occlusions best performing vrn guided detected landmarks cost higher computational complexity vrn - multitask always perform particularly better plain vrn effect pose performance method decreases pose increases effect expression performance variation across different expressions quite minor effect gaussian size guidance performance 3d reconstruction dropped negligible amount\n",
      "output class [[False  True False False  True False  True False False  True]]\n",
      "output facial landmark detection propose novel semantic alignment method reduces ' semantic ambiguity ' intrinsi-cally model ' real ' ground - truth latent variable optimize optimized ' real ' ground - truth supervises landmark detection network training probabilistic model search ' real ' ground - truth train landmark detection network end - - end way probabilistic model prior model constrain latent variable close observations ' real ' ground truth likelihood model reduce pearson chi-square distance expected predicted distributions ' real ' ground - truth heatmap generated hourglass architecture represents confidence pixel confidence distribution used model predicted distribution likelihood global heatmap correction unit ( ghcu ) maintains global face shape constraint recovers unconfidently predicted landmarks caused challenging factors occlusions low resolution images perform data augmentation randomly sample angle rotation bounding box scale gaussian distribution use four - stage stacked hourglass network backbone trained optimizer rmsprop training roughly converged model human annotations initial learning rate 2.5 10 ? 4 decayed 2.5 10 ? 6 120 epochs semantic alignment initial learning rate 2.5 10 ? 6 divided 5 , 2 2 epoch 30 , 60 90 set batch size 10 network training models trained pytorch 2 titan x gpus 300 w see hgs semantic alignment ( hgs + sa ) greatly outperform hourglass ( hgs ) 4.37 % vs 5.04 % terms nme full set showing great effectiveness semantic alignment ( sa ) adding ghcu see hgs + sa + ghcu slightly outperforms hgs + sa normalize - plane - rotation training preprocessing network achieve state art performance challenge set full set 6.38 % 4.02 % challenge set significantly outperform state art method 6.38 % hgs + sa +ghcu + norm vs 6.98 % lab aflw hgs + sa outperforms hgs 1.62 % vs 1.95 % observed hgs + sa + ghcu works better hgs + sa 300 - vw see hgs + sa greatly outperforms hgs compared hgs + sa hgs + sa + ghcu reduce error rate ( rmse ) 18 % category 3 test set 300 - vw semantic alignment consistently improve performance subset sets ghcu effective challenge data set ( category 3 ) 8.15 % vs 9.91 %\n",
      "output class [[False  True False False  True  True  True False False False]]\n",
      "output 3d morphable model regression presents method training regression network removes need supervised training data reliance inverse rendering reproduce image pixels network learns minimize loss based facial identity features produced face recognition network vgg - face google 's facenet exploit invariance apply loss matches identity features input photograph synthetic rendering predicted face alleviate fooling problem applying three novel losses batch distribution loss match statistics training batch morphable model loopback loss ensure regression network correctly reinterpret output multi-view identity loss combines features multiple , independent views predicted shape train 3d shape texture regression network using face recognition network morphable face model dataset unlabeled face images show 3d face results improve accuracy previous work neutral pose reconstruction micc results improved absolute error ground truth 20 - 25 % stable across changing environments face recognition results method achieves average similarity rendering photo 0.403 mofa test method 's results closer - person distribution differentperson distribution distance gt distribution - person lfw distribution low mean\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output reconstructing facial shape single images 3 facial shape estimation single images propose methodology learning statistical texture model `` - -wild `` facial images full correspondence statistical shape prior exhibits identity expression variations show learn featurebased texture models 3 dmms advantage using `` - -wild `` feature - based texture model fitting strategy gets simplified 3d shape recovery classic model struggles fit `` - -wild `` conditions present test set performs worst texture - free linear model better itw model able recover facial shapes due ideal feature basis `` - -wild `` conditions wide variety expression , identity , lighting occlusion conditions model able robustly reconstruct realistic 3 facial shape stands scrutiny quantitative normal recovery itw slightly outperforms imm\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output facial landmark detection face alignment https : //github.com/thesouthfrog/stylealign propose new framework augment training facial landmark detection without using extra knowledge map face images space structure style guarantee disentanglement two spaces design conditional variational auto - encoder model kullback - leiber ( kl ) divergence loss skip connections incorporated compact representation style structure perform visual style translation existing facial geometry given existing facial structure , faces glasses , poor quality , blur strong lighting rerendered corresponding style train facial landmark detectors general robust system recognize facial geometry training images cropped resized 256 256 using provided bounding boxes use 6 residual encoder blocks downsampling input feature maps batch normalization removed better synthetic results training disentangling step use adam initial learning rate 0.01 descends linearly 0.0001 augmentation detectors first augment landmark map k random styles sampled face images detector architecture simple baseline network based resnet - 18 changing output dimension last fc layers landmark wflw light - weight res - 18 improved 13.8 % utilizing stronger baseline model achieves 4.39 % nme style - augmented training outperforms state - - art entries large margin method brings 15.9 % improvement san model 9 % boost lab 5.27 % nme 4.76 % additional `` style- augmented `` synthetic training samples model based simple backbone outperforms previous state - - - art methods 300w yields 1.8 % 3.1 % improvement lab san manifest consistent benefit using `` style - augmented `` strategy common cross - dataset evaluation cofw model performs best 4.43 % mean error 2.82 % failure rate aflw exploiting style information boosts landmark detectors large - scale training set method improves san baseline terms nme full set 6.94 % 6.01 % visual comparison shows hidden face part better modeled strategy disentanglement style structure key influences quality style - augmented samples style - augmented synthetic images improve detectors ' performance large margin improvement even larger number training images quite small evaluate method adding number random sampled styles k annotated landmarks resnet - 50 baseline adding number augmented styles model continue gaining improvement\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output face alignment https : //wywu.github.io/projects/lab/lab.html facial landmark detection use well - defined facial boundaries represent geometric structure human face represent facial structure using 13 boundary lines boundary - aware face alignment algorithm contains two stages first estimate facial boundary heatmaps regress landmarks boundary heatmaps explore relationship facial boundaries landmarks introduce adversarial learning ideas using landmark - based boundary effectiveness discriminator boundary heatmap estimator landmark regressor boundary effectiveness discriminator jointly learned end - - end manner used stacked hourglass structure estimate facial boundary heatmap model structure facial boundaries message passing increase robustness occlusion boundary heatmaps serve structure cue guide feature learning landmark regressor models trained caffe 4 titan x gpus 300w method performs best among state - - - art methods wflw reasonable performance obtained cross - dataset evaluation cofw aflw model outperforms previous results large margin achieve 4.62 % mean error 2.17 % failure rate failure rate significantly reduced 3.75 % indicates robustness method handle occlusions method uses boundary information achieves 29 % , 32 % 29 % relative performance improve- ment baseline method ( `` lab without boundary `` ) cofw - 29 aflw - full aflw - frontal observed boundary map ( `` bm `` ) effective one boundary information fusion one key steps algorithm final model fuses boundary information four levels improves mean error 7.12 % 6.13 % performance improved consistently fusing boundary heatmaps levels comparison `` bl + hg `` `` bl + hg/ b `` indicates effectiveness boundary information fusion rather network structure changes `` bl + hg `` `` bl + cl `` indicates effectiveness using hourglass structure design message passing plays vital role heatmap quality improvement severe occlusions happen adversarial learning improves quality effectiveness boundary heatmaps\n",
      "output class [[False  True False False False False  True False False  True]]\n",
      "output face alignment wild face alignment introduce deep convolutional cascade face alignment ( decafa ) decafa composed several stages produce landmark - wise attention maps relatively heterogeneous annotation markups attention maps refined successive stages use 1 4 stages contains 12 3 3 convolutional layers 64 ? 64 ? 128 ? 128 ? 256 ? 256 channels downsampling portion input images resized 128 128 grayscale images prior processed network convolution followed batch normalization layer relu activation bilinear image upsampling followed 3 3 convolutional layers whole architecture trained using adam optimizer 5e ? 4 learning rate momentum 0.9 learning rate annealing power 0.9 apply 400000 updates batch size 8 database accuracy steadily increases add stages saturates third lfpw helen coarsely annotated data ( 5 landmarks ) significantly helps fine - grained landmark localization reinjecting whole input image significantly improves accuracy challenging data 300 w - challenging wflw - pose f 4 - equation ( 7 ) f 3 fusion ( cascaded models ) using local + global information rivals basic deep approach f 1 - equation ( 4 ) f 5 - equation fusion uses local global cues best significant margin chaining transfer layers better using independant transfer layers celeb decafa sets new state - - - art three databases\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output robust face alignment https : //github.com/protossw512/adaptivewingloss face alignment facial landmark localization propose new loss function adaptive wing loss significantly improve quality heatmap regression results translation invariance convolution operation bottom - top - cnn structures stacked hourglass ( hg ) capture coordinate information encode model full coordinate information information boundaries predicted previous hg module encode boundary coordinates add sub-task boundary prediction concatenating additional boundary channel ground truth heatmap jointly trained channels use rm - sprop initial learning rate 1 10 ? 4 set momentum 0 weight decay 1 10 ? 5 train 240 epoches learning rate reduced 1 10 ? 5 1 10 ? 6 80 160 epoches data augmentation performed random rotation 50 translation 25 px flipping 50 % rescaling 15 % random gaussian blur noise occlusion used 300w method achieve state - - - art performance challenge subset ( ibug dataset ) able outperform wing significant margin 300 w private test dataset outperform previous state - - theart variant metrics including nme , auc fr measured 8 % nme 10 % nme wflw method achieves best results every subset outperform previous state - - - art ap - approach fails 2.84 % images\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output face alignment https : //github.com/zhiwenshao/mcnet-extension propose novel deep learning framework named multi - center learning ( mcl ) exploit strong correlations among landmarks network uses multiple shape prediction layers predict locations landmarks shape prediction layer emphasizes detection cluster landmarks weighting loss landmark challenging landmarks focused firstly cluster landmarks optimized model assembling method integrate multiple shape prediction layers one shape prediction layer entire framework reinforces learning process landmark low model complexity uniform scaling translation different extents face bounding boxes conducted training samples augmented horizontal flip jpeg compression train mcl using open source deep learning framework caffe input face patch 50 50 grayscale image pixel value normalized [ ? 1 , 1 ) subtracting 128 multiplying 0.0078125 complex model labeling pattern facial landmarks type solver sgd mini-batch size 64 momentum 0.9 weight decay 0.0005 maximum learning iterations pre-training finetuning step 1810 4 610 4 initial learning rates pre-training fine - tuning step 0.02 0.001 learning rate multiplied factor 0.3 every 3 10 4 iterations state - - - art methods including esr sdm cascaded cnn rcpr cfan lbf c gprt cfss tcdcn alr cft rfld recnet rar fld + pde method mcl outperforms state - - - art methods aflw dataset relative error reduction 3.93 % achieved compared recnet challenging images aflw cofw mcl demonstrates superior capability handling severe occlusions complex variations pose , expression , illumination observed mcl achieves competitive performance three benchmarks average running speed deep learning methods detecting 68 facial landmarks global average pooling vs. full connection seen bm performs better ibug cofw worse aflw pre-bm demonstrates global average pooling advantageous complex problems facial landmarks robustness weighting 0.4 achieves good performance analysis shape prediction layers compared wm left eye model right eye model reduce alignment errors corresponding clusters assembled improve detection accuracy landmarks left eye right eye basis wm improve localization precision clusters integration weighting fine - tuning multi - center fine - tuning accuracy superior simplified especially challenging ibug seen weighting simplified improves slightly cofw fails search better solution ibug observed higher accuracy stronger robustness bm wm localization accuracy facial landmarks cluster improved d. mcl partially occluded faces correlations among different facial parts useful face alignment partially occluded faces processing testing faces occlusions mean error results wm increase results landmarks left eye cluster remaining landmarks clusters become worse slightly wm perform well occluded left eyes mean error 6.60 6.50 e. weighting fine - tuning state - - - art frameworks seen mean error -dan reduced 7.97 7.81 using proposed weighting fine - tuning\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output face alignment present 3 dde ( 3d deeply - initialized ensemble ) regressor robust efficient face alignment algorithm based coarse - - fine cascade erts initialized robustly fitting 3 face model probability maps produced cnn ert implicitly imposes prior face shape solution coarse - - fine structure tackles combinatorial explosion parts deformation improve initialization using ransac - like procedure increases robustness presence occlusions introduced early stopping better data augmentation techniques increasing regularization training ert cnn use adam stochastic optimization ? 1 = 0.9 , ? 2 = 0.999 = 1 e ? 8 train convergence initial learning rate 0.001 validation error levels 10 epochs multiply learning rate decay = 0.05 apply batch normalization convolution layers contain 68 filters describe required landmark features gaussian filter ? = 33 output probability maps stabilize initialization , g 0 coarse - - fine ert gradient boosting algorithm requires maximum = 20 stages k = 50 regression trees per stage depth trees set 4 number tests choose best split parameters set 200 resize image set face size 160160 pixels generate z = 25 initializations robust soft posit scheme g 0 augment shapes face training image create set , sa least n = 60000 samples train cascade avoid overfitting use shrinkage factor 0.1 subsampling factor 0.5 using nvidia geforce gtx 1080 ti ( 11 gb ) gpu dual intel xeon silver 4114 cpu 2.20 ghz 210 cores 20 threads 128 gb ram batch size 32 images dde better providing public implementation literature able improve large margin ert methods rcpr , ert c gprt outperform rcn dan lab compete regularization using cascade ert obtains best overall performance indoor outdoor subsets private competition full subset 300w public test set challenging subset 300w public competition shn gets better results 3dde combined cascaded ert 3d initialization key achieve top overall performance 3d initialization achieve good performance presence large face rotations large receptive fields cnns specially helpful challenging situations pose occlusion subsets coarse - - fine strategy cascaded ert provides significative local improvements difficult cases\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "completed face_alignment 0\n",
      "output face detection propose light fast face detector ( lffd ) edge devices considerably balancing accuracy running efficiency inspired one - stage multi-scale object detection method ssd enlightens face detectors one characteristics ssd pre-defined anchor boxes manually designed detection branch initialize parameters xavier method train network scratch optimization method sgd 0.9 momentum zero weight decay batch size 32 initial learning rate 0.1 1,500,000 iterations reduce learning rate multiplying 0.1 iteration 600,000 1,000,000 1,200,000 1,400,000 training time 5 days two nvidia gtx 1080 ti method implemented using mxnet dsfd ( resnet152 backbone ) pyramid box ( vgg16 backbone ) s3 fd ( vgg16 backbone ) ssh ( vgg16 backbone ) faceboxes fddb dataset dsfd , pyramid box , s3fd ssh achieve high accuracy marginal gaps proposed lffd gains slightly lower accuracy first four methods outperforms faceboxes lffd superior detecting unconstrained faces wider face dataset performance drop evident dsfd pyramidbox s3fd ssh pyramid box obtains best results hard parts performance ssh hard parts decreased dramatically due neglect tiny faces faceboxes get desirable results medium hard parts proposed method lffd consistently outperforms face - boxes gaps state art methods lffd better ssh uses vgg16 backbone hard parts\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output face detection propose detailed design faster rcnn method named fdnet1.0 face detection achieves decent performance deformable layer fewer channels attached backbone network produce `` thin `` feature map fed full connected layer building efficient yet accurate two - stage detector testing time find comparable mean average precision ( ap ) achieved top - ranked proposals ( e.g . , 6000 ) directly selected without nms rpn stage wider face dataset multi-scale training testing strategy applied work single nvidia tesla k80 used training testing mini batch size set 1 considering memory consumption resnet_v1_101 trained imagenet - 128w used faster rcnn feature extraction aspect ratios 1 1.5 2 scales 16 2 32 2 64 2 128 2 256 2 512 2 carefully designed capture better locations faces rpn stage number filters rpn layer set 512 batch size rpn r - cnn assigned 256 128 initial learning rate set 1e - 3 decrease 1e - 4 20w iterations weight decay momentum set 1e - 4 0.9 fdnet1.0 wins two 1st places easy set 95.9 % medium set 94.5 % one 2nd place hard set 87.9 % validation set\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output face detection https : //github.com/paddlepaddle/models/tree/develop/ use semi-supervised solution generate approximate labels contextual parts related faces series anchors called pyramidanchors invented easily added general anchor - based architectures investigate performance feature pyramid networks ( fpn ) modify low - level feature pyramid network ( lfpn ) join mutually helpful features introduce context - sensitive prediction module ( cpm ) incorporate context information around target face wider deeper network propose max - - layer prediction module improve capability classification network training strategy named data - anchor - sampling make adjustment distribution training dataset fddb dataset pyramidbox achieves state - ofart performance wider face dataset pyramidbox outperforms others across three subsets 0.961 easy 0.950 medium 0.889 hard validation set 0.956 easy 0.946 medium 0.887 hard testing set\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output face detection modify popular one - stage retinanet method perform face detection namely ainnoface employing two - step classification regression detection applying intersection union ( iou ) loss function regression revisiting data augmentation based data - anchor - sampling training utilizing max - operation robuster classification using multi-scale testing strategy inference backbone network proposed ainnoface detector initialized pretrained model imagenet dataset use `` xavier `` method randomly initialize parameters newly added convolutional layers stochastic gradient descent ( sgd ) algorithm fine - tune model 0.9 momentum 0.0001 weight decay batch size 32 warmup strategy applied gradually ramp learning rate 0.0003125 0.01 first 5 epochs switches regular learning rate schedule dividing 10 100 120 epochs ending 130 epochs full training testing codes built pytorch library face detector sets new state - - - art results based ap score across three subsets validation testing subsets 97.0 % ( easy ) 96.1 % ( medium ) 91.8 % ( hard ) validation subset 96.5 % ( easy ) 95.7 % ( medium ) 91.2 % ( hard ) testing subset results outperform compared state - - - art methods demonstrate superiority ainnoface detector\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output high performance face detection face detection investigate effects two - step classification regression different levels detection layers propose novel face detection framework named selective refinement network ( srn ) selectively applies two - step classification regression specific levels detection layers network structure srn consists two key modules named selective two - step classification ( stc ) module selective two - step regression ( str ) module stc applied filter simple negative samples low levels detection layers contains 88.9 % samples design receptive field enhancement ( rfe ) provide diverse receptive fields better capture extreme - pose faces loss function srn sum stc loss str loss backbone network initialized pretrained resnet - 50 model parameters newly added convolution layers initialized `` xavier `` method fine - tune srn model using sgd 0.9 momentum 0.0001 weight decay batch size 32 set learning rate 10 ? 2 first 100 epochs decay 10 ? 3 10 ? 4 another 20 10 epochs implement srn using py - torch library afw dataset srn outperforms state - - - art methods top ap score ( 99.87 % ) pascal face dataset srn achieves state - - - art results improving 4.99 % ap score compared second best method stn fddb dataset srn sets new state - - - art performance i.e . 98.8 % true positive rate number false positives equal 1000 wider face dataset find srn performs favourably state - - - art based average precision ( ap ) across three subsets\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output face detection introduce large - scale face detection dataset called wider face consists 32 , 203 images 393 , 703 labeled faces 10 times larger current largest face detection dataset vary largely appearance pose scale quantify different types errors annotate multiple attributes occlusion pose event categories allows depth analysis existing algorithms select vj acf dpm faceness overall faceness outperforms methods three subsets dpm acf marginal second third easy set average precision ( ap ) methods 60 % performance drops 10 % medium set hard set even challenging scale results small scale abysmal none algorithms able achieve 12 % ap occlusion partial occlusion performance drops significantly maximum ap 26.5 % achieved faceness pose best performance achieved faceness recall 20 %\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output detecting faces image face detection propose new multi-scale face detector extremely tiny size ( extd ) main discovery share network generating feature - map design backbone network reduces size feature map half sharing significantly reduce number parameters enables model use layers generate low - level feature maps detecting small faces proposed iterative architecture makes network observe features various scale faces various layer locations offer abundant semantic information network without adding additional parameters baseline framework follows fpn - like structures applied ssd - like architecture https : //github.com/clovaai tested different activation functions relu prelu leaky - relu negative slope leaky - relu set 0.25 identical initial negative slope prelu 138 times lighter model size 28.3 , 19.2 , 11 times lighter madds compared sota face detectors pyra - midbox dsfd best model extd - fpn - 64 - prelu achieved lower results margin pyramidbox proposed model wider face hard case 3.4 % ap gap dsfd tremendously heavier 5.0 % ssd - based variations got lower map results fpn - based variants compared s3fd version trained mo - bile facenet backbone network proposed ssd variants achieved comparable better detection performance see method achieved higher performance wider face hard dataset different channel width fpn based architecture achieved better detection performance compared ssd based architecture tested ssd based architecture prelu outperformed leaky - relu larger margin using fpn structure\n",
      "output class [[False  True False False  True  True  True False False False]]\n",
      "output face detection landmark localization pose estimation gender recognition present novel framework based cnns simultaneous face detection facial landmarks localization head pose estimation gender recognition design cnn architecture learn common features exploit synergy refer set intermediate layer features hyperfeatures construct separate fusion - cnn fuse hyperfeatures learn tasks train simultaneously using multiple loss functions deep cnn combined fusion - cnn learned together end -toend fashion face detection hyperface hf - resnet outperform reported academic commercial detectors afw pascal datasets hyperface achieves high mean average precision ( ap ) 97.9 % 92.46 % afw pascal datasets hf - resnet improves ap 99.4 % 96.2 % hyperface performance comparable dp2mfd faceness fddb dataset ap 90.1 % show multitask cnns multitask face hyperface outperform r - cnn face wide margin landmarks localization shows hyperface performs consistently accurate overall pose angles find r - cnn fiducial multitask face attain similar performance significantly improves performance hyperface afw aflw datasets observe hyperface achieves comparable nme 10.88 hf - resnet achieves state - - theart result ibug nme 8.18 pose estimation hyperface hf - resnet outperform existing methods large margin hf - resnet improves performance roll pitch yaw gender recognition lfwa dataset method outperforms panda facetracer hf - resnet achieves state - - - art results celeba lfwa datasets hyperface linear bounding box regression traditional nms achieves ap 94 %\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output finding tiny faces object recognition detecting small objects finding small faces face detection train separate detectors tuned different scales ( aspect ratios ) train run scale - specific detectors multitask fashion make use features defined multiple layers single ( deep ) feature hierarchy extend features fine - tuned networks objects novel sizes employ simply strategy resize images test - time interpolation decimation pyramid find interpolating lowest layer particularly crucial finding small objects final approach delicate mixture scale - specific detectors used scale - invariant fashion demonstrate convolutional deep features extracted multiple layers effective `` foveal `` descriptors capture high - resolution detail coarse low - resolution cues across large receptive field show highresolution components foveal descriptors extracted lower convolutional layers crucial accurate localization wider face hybrid - resolution model ( hr ) achieves state - - - art performance difficulty levels reduces error `` hard `` set 2x fddb - - - box detector ( hr ) outperforms published results discrete score post - hoc regressor detector achieves state - - - art performance continuous score regressor trained 10 - fold cross validation\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output face localisation wild face localisation https : //github.com/deepinsight/insightface/tree/master/retinaface based single - stage design propose novel pixel - wise face localisation method named reti- naface employs multi-task learning strategy simultaneously predict face score face box five facial landmarks 3d position correspondence facial pixel training details train retinaface using sgd optimiser momentum 0.9 weight decay 0.0005 batch size 8 four nvidia tesla p40 ( 24gb ) gpus learning rate starts 10 ? 3 rising 10 ? 2 5 epochs training process terminates 80 epochs testing details testing wider face employ flip well multi-scale box voting applied union set predicted face boxes using iou threshold 0.4 adding branch five facial landmark regression significantly improves face box ap 0.408 % map 0.775 % hard subset adding dense regression branch increases face box ap easy medium subsets slightly deteriorates results hard subset learning landmark dense regression jointly enables improvement compared adding regression face box accuracy retinaface produces best ap subsets validation test sets 96.9 % easy 96.1 % medium 91.8 % hard validation set 96.3 % easy 95.6 % medium 91.4 % hard test set compared recent best performed method reti - na face sets new impressive record 91.4 % v.s . 90.3 % hard subset successfully finds 900 faces threshold 0.5 reported 1 , 151 faces five facial landmark accuracy retinaface significantly decreases normalised mean errors ( nme ) 2.72 % 2.21 % compared mtcnn compared mtcnn retinaface significantly decreases failure rate 26.31 % 9.37 % dense facial landmark accuracy dense regression results retinaface comparable state - - - art methods observe five facial landmarks regression alleviate training difficulty dense regression branch significantly improve dense regression results using single - stage features retinaface predict dense correspondence parameters much harder employing ( region interest ) roi features mesh decoder face recognition accuracy cfp - fp demonstrate reti - na face boost arcface 's verification accuracy 98.37 % 99.49 %\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output face detection propose three novel techniques introduce feature enhance module ( fem ) enhance discriminability robustness features combines advantages fpn pyramidbox receptive field block ( rfb ) rfbnet design progressive anchor loss ( pal ) uses progressive anchor sizes different levels different shots assign smaller anchor sizes first shot use larger sizes second shot propose improved anchor matching ( iam ) integrates anchor partition strategy anchor-based data augmentation better match anchors ground truth faces provides better initialization regressor proposed network dual shot face detector ( dsfd ) backbone networks initialized pretrained vgg / resnet image net newly added convolution layers ' parameters initialized ' xavier ' method use sgd 0.9 momentum 0.0005 weight decay fine - tune dsfd model batch size set 16 learning rate set 10 ? 3 first 40 k steps decay 10 ? 4 10 ? 5 two 10 k steps inference first shot 's outputs ignored second shot predicts top 5 k high confident detections non-maximum suppression applied jaccard overlap 0.3 produce top 750 high confident bounding boxes per image bounding box coordinates round top left coordinates roundup width height expand detection bounding box https : //github.com/tencentyouturesearch/facedetection-dsfd wider face dataset feature enhance module first shows feature enhance module improve vgg16 - based fssd 92.6 % , 90.2 % , 79.1 % 93.0 % , 91.4 % , 84.6 % progressive anchor loss second res50 - based fssd shows progressive anchor loss improve using fem 95.0 % , 94.1 % , 88.0 % 95.3 % , 94.4 % , 88.6 % improved anchor matching shows improved anchor matching improve res101 based fssd using fem 95.8 % , 95.1 % , 89.7 % 96.1 % , 95.2 % , 90.0 % wider face dataset dsfd achieves best performance among state - - - art face detectors based average precision ( ap ) across three subsets 96.6 % easy 95.7 % medium 90.4 % hard validation set 96.0 % easy 95.3 % medium 90.0 % hard test set fddb dataset dsfd achieves state - - - art performance discontinuous continuous roc curves i.e . 99.1 % 86.2 % number false positives equals 1 , 000 adding additional annotations unlabeled faces false positives model reduced outperform methods\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output efficient face detection real - word face detection propose new cascade convolutional neural network trained end - - end first stage multi-task region proposal network ( rpn ) simultaneously proposes candidate face regions along associated facial landmarks jointly conduct face detection face alignment network calculated original resolution better leverage discriminative information aligned candidate face region fed second - stage network rcnn verification keep k face candidate regions top responses local neighborhood rpn concatenate feature maps two cascaded networks together form architecture trained end - - end canonical positions facial landmarks aligned face image predicted facial landmarks candidate face region jointly defines transform candidate face region end - - end training training first - stage rpn predict facial landmarks supervised annotated facial landmarks true face regions call supervised transformer network region - - interest ( roi ) convolution scheme make run-time supervised transformer network efficient first uses conventional boosting cascade obtain set face candidate areas irregular binary roi mask dnn operations including convolution relu pooling concatenation processed inside roi mask significantly reduce computation multi-task rpn , supervised transformer , feature combination bring 1 % , 1 % , 2 % recall improvement remove anyone part cause recall drop found nms include much noisy low confidence candidates non - top k suppression close using candidates achieved consistently better results nms number candidates fddb dataset compare public methods afw pascal faces datasets compare deformable part based methods e.g . structure model tree parts model cascade - based methods e.g . headhunter commercial system e.g . face.com face ++ picasa\n",
      "output class [[False  True False False False  True  True False False  True]]\n",
      "output face detection https : //github.com/sfzhang15/faceboxes develop state - - - art face detector real - time speed cpu propose novel face detector named faceboxes contains single fully convolutional neural network trained end - - end proposed method lightweight yet powerful network structure consists rapidly digested convolutional layers ( rdcl ) multiple scale convolutional layers ( mscl ) rdcl designed enable faceboxes achieve real - time speed cpu mscl aims enriching receptive fields discretizing anchors different layers handle various scales faces new anchor densification strategy make different types anchors density input image filter boxes confidence threshold 0.05 keep top 400 boxes applying nms perform nms jaccard overlap 0.3 keep top 200 boxes measure speed using titan x ( pascal ) cudnn v 5.1 intel xeon e5-2660v3 @ 2.60 ghz faceboxes anchor densification strategy crucial anchor densification strategy used increase density small anchors improve recall rate small faces see ap fddb reduced 96.0 % 94.9 % ablating anchor densification strategy rdcl efficient accuracy - preserving afw dataset faceboxes outperforms large margin pascal face dataset method significantly outperforms methods commercial face detectors fddb dataset faceboxes achieves state - - - art performance outperforms large margin discontinuous continuous roc curves\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output object detection multiscale object detection github.com/sciencefans/rsa-for-object-detection propose recurrent scale approximation ( rsa , see ) unit rsa unit plugged specific depths network fed initial feature map largest scale unit convolves input recurrent manner generate prediction feature map half size input feed network input one scale approximate rest features smaller scales learnable rsa unit first scale - forecast network globally predict scales novel image compute feature pyramids certain set scales based prediction second landmark retracing network retraces location regressed landmarks preceding layers generates confidence score landmark based landmark feature set final score identifying face within anchor revised lrn network three components incorporated unified cnn framework trained end - - end structure model shallow version resnet use model scale - forecast network lrn numbers channels set half original resnet model consideration time efficiency train scale - forecast network use output predicted scales launch rsa unit lrn ratio positive negative 1 : 1 experiments batch size 4 base learning rate set 0.001 decrease 6 % every 10,000 iterations maximum training iteration 1,000,000 stochastic gradient descent optimizer scale - forecast network vital importance computational cost accuracy networks observe trained scale network recalls almost 99 % x = 1 deeper rsa branched worse feature approximation smaller scales handle scales features deep cnn model branched depth network minimum operation component means scaleforecast network used maximum operation indicates amount faces appear scales computation happens layer res2 b acceptable error rate 3.44 % times recurrent operation increase error rate goes due cumulative effect rolling predictions\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output fast object detection fast multi-scale object detection object detection proposes unified multi-scale deep cnn denoted multi -scale cnn ( ms - cnn ) fast object detection network consists two sub-networks object proposal network accurate detection network learned end - - end share computations ease inconsistency sizes objects receptive fields object detection performed multiple output layers focusing objects within certain scale ranges complimentary detectors different output layers combined form strong multi-scale detector performance ms - cnn detector evaluated kitti caltech pedestrian benchmarks https : //github.com/zhaoweicai/mscnn times reported implementation single cpu core ( 2.40 ghz ) intel xeon e5 - 2630 server 64 gb ram nvidia titan gpu used cnn computations ms - cnn set new record detection pedestrians cyclists led nontrivial margin recent sdp + rpn used scale depen - dent pooling ms - cnn caltech pedestrian benchmark state - - art performance performs well small occluded objects outperforming deepparts\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output detecting faces face detection develop face detector top r - fcn elaborate design details size design anchors rois introduce position - sensitive average pooling generate embedding features enhancing discrimination eliminate effect non-uniformed contribution facial part apply multi-scale training testing strategy - line hard example mining ( ohem ) technique integrated network boosting learning hard examples training hyper - parameters similar face r - cnn initialize network pre-trained weights 101 - layer resnet trained image net freeze general kernels pre-trained model throughout entire training process keep essential feature extractor trained imagenet terms rpn stage face r - fcn enumerates multiple configurations anchor combine range multiple scales aspect ratios construct multi-scale anchors rpn r - fcn learned jointly softmax loss smooth l1 loss non- maximum suppression ( nms ) adopted regularizing anchors certain iou scores set 256 size rpn mini-batch 128 r - fcn utilize multi-scale training input image resized bilinear interpolation various scales ( say , 1024 1200 ) testing stage multi-scale testing scale image image pyramid better detecting tiny general faces proposed approach consistently wins 1st place across three subsets validation set test set wider face significantly outperforms existing results wider face hard subset approach superior prior best - performing one clear margin fddb face r - fcn consistently achieves impressive performance terms discrete roc curve continuous roc curve discrete roc curve superior prior best - performing method obtain best true positive rate discrete roc curve 1000/2000 false positives 98.49 % /99.07 % shows superior performance prior methods across three subsets easy , medium hard validation test sets true positive rate 98 . 99 % discrete roc curve 1000 false positives 99 . 42 % 2000 false positives new state - - - art among published methods\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output multi-view face detection face detection human face detection adopt variant channel features called aggregate channel features extracted directly pixel values subsampled channels aggregate channel features breakthrough bottleneck vj framework make deep - round investigation specific feature parameters concerning channel types feature pool size subsampling method feature scale find multi-scaling feature representation enriches representation capacity aggregate channel features different combinations channel types impact performance greatly multi-view detection proven good match afw multi-scale detector achieves ap value 96.8 % outperforming academic methods large margin comes commercial systems better face.com almost equal face ++ google picasa discrete score evaluation metric afw detector achieves 83.7 % little better yan et al using continuous score takes overlap ratio score method gets 61.9 % true positive rate 1 fppi multiscale version surpassing methods output rectangular detections notable margin detector using single - scale features performs little worse benefit faster detection speed\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output object detection images object detection propose weakly supervised multimodal annotation segmentation ( wsma - seg ) approach uses segmentation models achieve accurate robust object detection without nms training phase wsma - seg first converts weakly supervised bounding box annotations detection tasks multi-channel segmentation - like masks called multimodal annotations segmentation model trained using multimodal annotations labels learn multimodal heatmaps training images testing phase resulting heatmaps given test image converted instance - aware segmentation map based pixel - level logic operation contour tracing operation generate contours objects using segmentation map bounding boxes objects created circumscribed quadrilaterals corresponding contours rebar head detection proposed method stack = 2 , base = 40 , depth = 5 achieved best performance among solutions terms f1 score compared state - - - art baselines wsma - seg much simpler effective efficient wider face detection wider face results much lower detection accuracy compared face detection datasets show proposed wsma - seg outperforms state - - - art baselines three categories reaching 94.70 , 93.41 , 87.23 easy , medium , hard categories ms coco detection wsma - seg approach outperforms state - - - art baselines terms metrics including ap ap ar ar metrics performance proposed approach close best baselines proves proposed wsma - seg approach generally achieves accurate robust object detection state - - - art approaches without nms\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output unconstrained face detection robust face detection wild face detection presents advanced cnn based approach named contextual multi - scale region - based cnn ( cms - rcnn ) handle problem face detection digital face images collected numerous challenging conditions designed region - based cnn architecture allows network simultaneously look multi-scale features explicitly look outside facial regions potential body regions helps synchronize global semantic features high level layers localization features low level layers facial representation cms - rcnn method introduces multi - scale region proposal network ( ms - rpn ) generate set region candidates contextual multi - scale convolution neural network ( cms - cnn ) inference region candidates facial regions cms - rcnn implemented caffe deep learning framework first 5 sets convolution layers architecture deep vgg - 16 model training parameters initialized pre-trained vgg - 16 ms - rpn ' conv3 ' , ' conv4 ' , ' conv5 ' synchronized size concatenation applied ' conv3 ' followed pooling layer perform - sampling ' conv3 ' , ' conv4 ' , ' conv5 ' normalized along channel axis learnable re-weighting scale concatenated ensure training convergence initial re-weighting scale needs carefully set set initial scale ' conv3 ' , ' conv4 ' , ' conv5 ' 66.84 , 94.52 , 94.52 cms - cnn roi pooling layer ensure pooled feature maps size features pooled ' conv3 ' , ' conv4 ' , ' conv5 ' initialized scale 57.75 , 81.67 , 81.67 face body pipelines ms - rpn cms - cnn share parameters convolution layers computation done resulting higher efficiency shrink channel size concatenated feature map 11 convolution layer employed wider face dataset achieves best average precision level faces i.e . ap 0.902 easy 0.874 medium 0.643 hard outperforms second best baseline 26.0 % easy 37.4 % medium 60.8 % hard fddb face database method achieves best recall rate database proposed cms - rcnn approach outperforms published face detection methods achieves high recall rate comparing methods\n",
      "output class [[ True False False False False False  True False False False]]\n",
      "output face detection automatic face recognition unconstrained face detection problem simple pixel - level feature called normalized pixel difference ( npd ) npd computed ratio difference two pixel intensity values sum values npd feature several desirable properties scale invariance boundedness ability reconstruct original image show npd features obtained lookup table resulting face detection template scaled multiscale face detection propose deep quadratic tree learning method construct single soft - cascade adaboost classifier handle complex face manifolds arbitrary pose occlusion conditions different types faces automatically divided different leaves tree classifier complex face manifold high dimensional space partitioned learning process `` divide conquer `` strategy tackle unconstrained face detection single classifier without pre-labeling views training set face images resulting face detector robust variations pose , occlusion , illumination blur low image resolution http : //www.cbsr.ia.ac.cn/users/scliao/ projects / npdface / used detection template 24 24 pixels set maximum depth tree classifiers learned eight eight npd features evaluated tree classifier soft cascade training set threshold exit minimal score positive samples final detector contains 1,226 deep quadratic trees 46,401 npd features trained near frontal face detector using proposed npd features classic cascade regression trees ( cart ) depth four subset training data including 12,102 face images 12,315 nonface images detection template 20 20 pixels detector cascade contains 15 stages stage target false accept rate 0.5 detection rate 0.999 fddb database observed proposed method outperforms baseline methods proposed npd face detector second best one fp = 0 discrete metric third best one continuous metric proposed npd detector among top performers discrete metric joint cascade algorithm competitive terms accuracy speed genki database proposed npd face detector significantly outperforms viola - jones pittpatt face detectors cmu - mit database compared viola - jones frontal face detector npd detector performs better number false positives fp < 50 slightly worse viola - jones higher fps\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output face detection alignment unconstrained environment face detection face alignment propose new framework integrate two tasks using unified cascaded cnns multi-task learning proposed cnns consist three stages first stage produces candidate windows shallow cnn refines windows reject large number non-faces windows complex cnn uses powerful cnn refine result output facial landmarks positions compare compare face detector alignment face detector alignment state - - - art methods state - - - art methods face detection data set benchmark ( fddb ) face detection data set benchmark ( fddb ) wider face wider face annotated facial landmarks wild ( aflw ) benchmark annotated facial landmarks wild ( aflw ) benchmark effectiveness online hard sample mining clear hard sample mining beneficial performance improvement joint detection alignment suggests joint landmarks localization task learning beneficial face classification bounding box regression tasks face detection consistently outperforms previous approaches large margin benchmarks face alignment outperforms state - - - art methods margin\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output robust face detection face detection propose mine hard examples image level parallel anchor level dynamically assign difficulty scores training images learning process determine whether image well - detected useful training fully utilize images perfectly detected better facilitate learning process improve detection quality exclusively exploiting small faces detector efficient specially designed leveraging small faces training use imagenet pretrained vgg16 model initialize network backbone newly introduced layers randomly initialized gaussian initialization train model itersize 2 46 k iterations learning rate 0.004 another 14 k iterations smaller learning rate 0.0004 training use 4 gpus compute gradient update weight synchronized sgd momentum first two blocks vgg16 frozen training rest layers vgg16 set double learning rate resize testing image short side contains 100 , 300 , 600 , 1000 1400 pixels evaluation wider face dataset follow testing strategies used pyra - midbox horizontal flip bounding - box voting show precision - recall ( pr ) curve average precision ( ap ) model compared many state - - - arts method achieves best performance hard subset outperforms current state - - - art large margin performance medium subset comparable recent state - - - art performance easy subset bit worse discontinuous roc curve method achieves state - - - art performance tpr = 98.7 % given 1000 false positives pr curve method achieves new state - - - art performance ap = 99.0 state - - - art almost perfect performance ap 99.60 model single detection feature map performs better one three detection feature maps despite shallower structure fewer parameters anchors improve performance hard subset significantly without involving complex network architecture dh boost performance shows effectiveness designing larger convolution larger anchors combining dh together improve towards state - - - art performance photometric distortion cropping contribute robust face detector extra small scales detect easy faces\n",
      "output class [[False  True False False  True False  True False False  True]]\n",
      "completed face_detection 0\n",
      "output neural hypernym discovery hypernym discovery hypernym detection introduce neural network architecture concerned task empirically study various neural networks model distributed representations words phrases leverage unambiguous vector representation via term embedding take advantage deep neural networks discover hypernym relationships terms model implemented using theano diagonal variant ada - grad used neural network training tune hyper - parameters following range values learning rate { 1 e ? 3 , 1 e ? 2 } dropout probability { 0.1 , 0.2 } cnn filter width { 2 , 3 , 4 } hidden dimension neural models 200 batch size set 20 word embedding sense embedding sizes set 300 models trained single gpu ( nvidia gtx 980 ti ) roughly 1.5h general - purpose subtask english 0.5h domain - specific domain - specific ones medical music convolution recurrent gated mechanisms cnn - based ( cnn , rcnn ) rnn ( gru , lstm ) based neural networks helpful modeling semantic connections words phrase guide networks discover hypernym relationships observe cnn - based network performance better rnn - based investigate performance neural models specific domains conduct experiments medical medicine subtask outperform term embedding compared word embedding sense embedding shows much poorer result\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output hypernym discovery combines path - based technique distributional technique via concatenating two feature vectors feature vector constructed using dependency parser obtained using term embeddings using concatenated vector create binary supervised classifier model based support vector machine ( svm ) algorithm model predicts term candidate hypernym hypernym related results three corpora system performs better stju system mfh system english corpora shows system performs well discovering new hypernyms defined gold hypernyms candidate hypernym extraction ( che ) coverage english testing terms 950 ( 63 % )\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output unsupervised hypernym discovery hypernym discovery adapt team focused two specialised domain english subtasks developing unsupervised system builds word embeddings supplied reference corpora unsupervised approach expectation generalise easily unseen hypernym - hyponym pairs results official submission ranked eleven eighteen medical domain subtask mean average precision ( map ) 8.13 first place among unsupervised systems music industry domain subtask system ranked 13th 16 places map 1.88 ranking 4th among unsupervised systems\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output hypernym discovery hypernym detection word2vec produce word embeddings skip - gram model ( - cbow 0 ) used embedding dimension set 300 ( - size 300 ) based projection learning best f - value validation set 0.68 best cluster number 2 threshold ( 17.7 , 17.3 ) projection learning method performs well task9 nn performance evaluated using either cross validation test data much worse typical hypernym prediction task submissions 1st spanish 2nd italian 6th english ranked metric map compared results got cross validation performance evaluated test data dropped significantly english map dropped 4 % italian map dropped 8 % increased margin spanish map increased 3.6 %\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output hypernym discovery developed crim team exploits combination two approaches unsupervised , pattern - based approach supervised , projection learning approach hybrid system ranked 1st three sub - tasks submitted runs scores much higher strongest baselines likely improve scores 2a 2b compare runs 1 2 hybrid system see data augmentation improved scores slightly 1a 2b increased several points 2a cross-evaluation results better supervised baseline computed using normal evaluation setup training system general - purpose data produced better results domain - specific test set strong , supervised baseline trained domain - specific data unsupervised system outperformed unsupervised systems supervised baseline 2a run 1 best 3 test sets use hybrid system carried simple ablation tests subtask 1 show 2 techniques namely subsampling multitask learning harmed system 's performance test set 1 fine - tuning word embeddings training seems keys success approach note supervised model prone overfitting\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output hypernym discovery domain adaptation taxonomy learning propose taxoembed hypernym detection algorithm based sense embeddings applied construction lexical taxonomies designed discover hypernymic relations exploiting linear transformations embedding spaces best configuration considers manually curated pairs wikidata hypernymy relations kb integrates several open information extraction ( oie ) systems method uses large semantic network reference sense inventory perform jointly hypernym extraction dis ambiguation www.taln.upf.edu/taxoembed compare number taxonomy learning information extraction systems namely wibi defie defie automaic oie system relying syntactic structure pre-dis ambiguated definitions taxoembed comparison systems yago wibi achieve best results taxoem - bed based solely distributional information performed competitively detecting new hypernyms compared defie improving recall domains surpassing yago technical areas like biology health model perform particularly well media physics\n",
      "output class [[False False False False False False  True False False False]]\n",
      "output hypernymy detection recognize hypernymy perform extensive evaluation various unsupervised distributional measures hypernymy detection using several distributional semantic models differ context type feature weighting analyze performance measures different settings suggest principled way select suitable measure context type feature weighting compare unsupervised measures state - - - art supervised methods experiments comparing unsupervised measures show preference syntactic context - types ( dep joint ) feature weighting consistency raw frequency appears successful hypernymy detection new slqs variants top list perform well discriminating hypernyms symmetric relations antonymy synonymy coordination comparison state - - - art supervised methods performance embeddingbased classifiers almost perfect best performance achieved using concatenation method either glove dependency - based embeddings unsupervised measures perform worse embedding - based classifiers\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output hypernym discovery apply sparse feature pairs hypernym extraction sparse representation related hypernymy formal concept analysis ( fca ) hierarchical sparse coding trees describe order variables enter model take non - zero values exploiting correspondence variable tree hypernym hierarchy offers natural choice submission attribute pairs achieved first place categories ( 1b ) italian ( entities ) ( 1c ) spanish entities ( 2b ) music entities\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output detecting hypernymy relations presents apollo team 's system hypernym discovery participated task 9 semeval 2018 based unsupervised machine learning rule - based system exploits syntactic dependency paths generalize hearst - style lexical patterns results relations fruitful outperform random strategy lower scores obtained multiword expressions\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "completed hypernym_discovery 0\n",
      "output text understanding text comprehension called attention sum reader ( reader ) tailor - made leverage fact answer word context document compute vector embedding individual word context whole document ( contextual embedding ) using dot product question embedding contextual embedding occurrence candidate answer document train model used stochastic gradient descent adam update rule learning rate 0.001 0.0005 weights gru networks initialized random orthogonal matrices biases initialized zero used gradient clipping threshold 10 batches size 32 single model little bit worse performance simultaneously published models ensemble models outperforms models use pre-trained word embeddings cnn dataset single model best validation accuracy achieves test accuracy 69.5 % average performance top 20 % models 69.9 % even 0.5 % better single best - validation model fusing multiple models gives significant increase accuracy cnn daily mail datasets named entity prediction best single model accuracy 68.6 % performs 2 % absolute better memnn self supervision averaging ensemble performs 4 % absolute better best previous result common noun prediction single models 0.4 % absolute better mem nn ensemble improves performance 69 % 6 % absolute better memnn\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output dynamic self - attention dynamic self - attention ( dsa ) dsa propose new self - attention mechanism sentence embedding namely dynamic self - attention ( dsa ) modify dynamic routing functions self - attention dynamic weight vector dsa stacked cnn dense connection achieves new state - - - art results among sentence encoding methods stanford natural language inference ( snli ) dataset least number parameters obtaining comparative results stanford sentiment treebank ( sst ) dataset outperforms recent models terms time efficiency due simplicity highly parallelized computations natural language inference results entailment , contradiction neutral considers semantic relationship snli used benchmark evaluating performance sentence encoder tradeoffs terms parameters learning time per epoch multiple dsa outperforms models large margin ( + 1.1 % ) comparison baseline single dsa shows better performance self - attention ( + 2.2 % ) implementation baseline selfattention stacked cnn dense connection shows better performance ( + 0.4 % ) stacked bilstm sentiment analysis results single dsa outperforms baseline models sst - 2 dataset achieves comparative results sst - 5 verifies effectiveness dynamic weight vector sst dataset marginal differences performance dsa previous self - attentive models found\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output sentence similarity learning sentence similarity propose novel model decomposing composing lexical semantics sentences given sentence pair model represents word low -dimensional vector calculates semantic matching vector word based words sentence based semantic matching vector word vector decomposed two components similar component dissimilar component use similar components words represent similar parts sentence pair dissimilar components every word model dissimilar parts explicitly two - channel cnn operation performed compose similar dissimilar components feature vector composed feature vector utilized predict sentence similarity qasent dataset adding word overlap features two sentences performance improved significantly see model got best map among previous work comparable mrr dos wiki qa dataset best performance acquired bigram cnn model combining word overlap features msrp dataset model obtained comparable performance without using sparse features extra annotated resources specific training strategies\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output parameter re-initialization optimal parameter initialization adapting weight initialization optimization dynamics specific learning task hand bayesian perspective improved weight initialization leads accurate posterior better generalization ability adaptive prior implemented via markov chain monte carlo ( mcmc ) methods incorporate adaptive initialization neural network training use cyclical batch size schedules control noise ( temperature ) sgd studying different cyclical annealing strategies wide range problems language results language modeling challenging problem due complex long - range interactions distant words help us avoid overfitting snapshot ensembling enables even greater performance best performing cbs schedules result significant improvements perplexity ( 7.91 ) baseline schedules offer reductions number sgd training iterations ( 33 % ) cbs schedules outperform baseline schedule cbs schedules yield large performance improvements models like e1 exhibit smaller disparities training testing performance image classification results training curves cbs schedules exhibit aforementioned cyclical spikes training loss testing accuracy observe cbs achieves similar performance baseline cbs - 15 see 90.71 % training accuracy 56 . 44 % testing accuracy larger improvement offered cbs convolutional models cifar - 10 combining cbs - 15 c2 improves accuracy 94.82 % applying snapshot ensembling c3 trained cbs - 15 - 2 leads improved accuracy 93 . 56 % compared 92.58 % ensembling resnet50 imagenet snapshots last two cycles performance increases 76.401 % 75.336 %\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output large - scale reading comprehension machine reading comprehension rc reading comprehension ( rc ) propose two novel strategies improve memory - handling capability mitigating information distortion extend memory controller residual connection alleviate information distortion expand gated recurrent unit ( gru ) dense connection conveys enriched features next layer containing original well transformed information build model sonnet implement memory interface nltk used tokenizing words memory controller use four read heads one write head memory size set 100 36 initialized 0 hidden vector dimension l set 200 use adadelta ( zeiler , 2012 ) optimizer learning rate 0.5 batch size set 20 triviaqa 30 squad quasar - exponential moving average weights decaying factor 0.001 model require memory existing methods single gpu enough train model within reasonable amount time lengthy - document cases trivi aqa quasar - model outperforms published results short - document case squad achieve best results assume concatenation layer outputs debs helps memory controller store contextual representations using debs places improves performance memory controller debs gives largest performance margin\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output reasoning multiple mentions using coreference coreference - based reasoning given input sequence coreference clusters extracted external system introduce term update equations gated recurrent units ( gru ) hidden states propagated along coreference chains original sequence parallel compare coref - gru layer regular gru layer incorporating recent model reading comprehension babi ai tasks see clear improvements using c - gru layers gru layers comparing qrn baseline found c - gru significantly worse task 15 ( basic deduction ) c - gru significantly better qrn task 16 ( basic induction ) wikihop dataset see higher performance c - gru model low data regime better generalization throughout training curve three settings\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output natural language comprehension machine comprehension text machine comprehension unstructured , real - world text machine comprehension propose deep , end - - end , neural comprehension model call epireader factors two components first component extracts small set potential answers based shallow comparison question supporting text call extractor second component reranks proposed answers based deeper semantic comparisons text call reasoner semantic comparisons implemented reasoner based concept recognizing textual entailment ( rte ) known natural language inference extractor serves important function filtering large set potential answers small , tractable set likely candidates thorough testing follows form pointer network uses differentiable attention mechanism indicate words text potentially answer question used ( ) question answering attention sum reader outputs small set answer candidates along estimated probabilities correctness reasoner forms hypotheses inserting candidate answers question estimates concordance hypothesis sentence supporting text use estimates measure evidence hypothesis aggregate evidence overall sentences combine reasoner 's evidence extractor 's probability estimates produce final ranking answer candidates train model used stochastic gradient descent adam optimizer ( kingma ba , 2014 ) initial learning rate 0.001 word embeddings initialized randomly drawing uniform distribution used batches 32 examples early stopping patience 2 epochs model implement theano using keras framework models used regularization 0.001 , ? = 50 , ? = 0.04 epireader achieves state - - - art performance across board datasets cnn score 2.2 % higher test cbt - cn single model scores 4.0 % higher previous best reader improvement cbt - ne modest 1.1 % looking closely cbt - ne results found validation test accuracies relatively high variance late epochs training\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output text based reasoning text - based question answering https : //github.com/juung/rmn `` relation memory network `` ( rmn ) find complex relation lot information given uses mlp find relevant information new generalization simply erase information already used rmn inherits rn 's mlp - based output feature map memory network architecture babi story - based qa dataset embedding component story question 32 unit word - lookup embeddings lstm attention component use 2 hop rmn regularization use batch normalization mlps softmax output optimized cross - entropy loss function using adam optimizer learning rate 2 e ? 4 babi dialog dataset trained full dialog scripts every model response answer previous dialog history sentences memorized last user utterance question selects probable response 4,212 candidates ranked set bot utterances training , validation test sets babi dialog without match type rn rmn outperform previous memory - augmented models normal oov tasks converted rmn 's attention component inner product based attention results revealed error rate increased 11.3 % number unnecessary object pairs created rn increases processing time decreases accuracy match type feature models rmn significantly improved performance except task 3 compared plain condition rmn yields error rate 25.1 % memn2n gme n2n task 3\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output natural language inference recognizing textual entailment ( rte ) explore multi-step inference strategies nli maintains state iteratively refines predictions spacy tool used tokenize dataset pytorch used implement models fix word embedding 300 - dimensional glove word vectors character encoding use concatenation multi-filter convolutional neural nets windows 1 , 3 , 5 hidden size 50 , 100 , 150 lexicon embeddings =600 - dimensions embedding - - vocabulary zeroed hidden size lstm contextual encoding layer , memory generation layer set 128 input size output layer 1024 ( 128 * 2 * 4 ) projection size attention layer set 256 speedup training use weight normalization dropout rate 0.2 dropout mask fixed time steps lstm mini - batch size set 32 optimizer adamax learning rate initialized 0.002 decreased 0.5 10 epochs shows multi-step model consistently outperforms single - step model dev set four datasets terms accuracy scitail dataset san outperforms single - step model scitail dataset san outperforms gpt comparing single - step baseline proposed model obtains + 2.8 improvement sc - itail test set 94.0 vs 91.2 + 2.1 improvement scitail dev set 96.1 vs 93.9 model outperforms best system repeval 2017 except `` conditional `` `` tense difference `` categories find san works extremely well `` active / passive `` `` paraphrase `` categories chen 's model biggest improvement san `` antonym `` category challenging `` long sentence `` `` quantity / time `` categories san 's result substantially better previous systems\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output question answering question answering ( qa ) qa employ connected graph depict paraphrase relation sentences pi task propose multi-task sentence - encoding model solves paraphrase identification task sentence intent classification task simultaneously semantic retrieval framework integrates encoding - based sentence matching model approximate nearest neighbor search technology find similar question quickly available questions quora dataset use glove - 840b - 300d vector pre-trained word embedding character embedding randomly initialized 150 hidden size bigru set 300 set 0.8 multi - task loss function dropout layer applied output attentive pooling layer dropout rate 0.1 adam optimizer used optimize trainable weights learning rate set 4e - 4 batch size set 200 sgd optimizer learning rate 1e - 3 find better local optimum esim enhanced sequential inference model interaction - based model natural language inference bimpm bilateral multi- perspective matching model interaction - based sentence matching model superior performance sse shortcut - stacked sentence encoder encodingbased sentence - matching model enhances multi - layer bilstm short - cut connections diin densely interactive inference network interaction - based model natural language inference ( nli ) quora dataset bimpm esim models without sentence interaction information close diin state - - - art interaction - based model lcqmc dataset show model outperforms state - - art models bq dataset model outperforms state - - - art models large margin reaching 83 . 62 % recording state - - - art performance tcs dataset msem model achieves best performance study contribution aru component accuracy decreases drop 88.25 % attentive pooling better max pooling remove highway network accuracy drop 88.36 % character - level embedding accuracy drop 88.26 %\n",
      "output class [[False  True False False  True False  True False False  True]]\n",
      "output text semantic matching modelling relevance / similarity pair texts adopt deep fusion strategy model strong interactions two sentences text matching regarded modelling interaction two texts recursive matching way propose deep fusion long short - term memory neural networks ( df - lstms ) model interactions recursively df - lstms consist two interconnected conditional lstms models apiece text influence another output vector df - lstms fed task - specific output layer compute match - ing score neural bag - - words ( nbow ) sequence represented sum embeddings words concatenated fed mlp single lstm two sequences encoded single lstm parallel lstms two sequences first encoded two lstms separately attention lstms two sequences encoded lstms attention mechanism word - - word attention lstms improved strategy attention lstms see proposed model shows superiority task outperforms stateof - - arts methods metrics ( p @ 1 ( 5 ) p @ 1 ( 10 ) ) large margin strong interaction models attention lstms df - lstms consistently outperform weak interaction models nbow parallel lstms large margin\n",
      "output class [[False  True False False False  True  True False False False]]\n",
      "output memory networks present novel recurrent neural network ( rnn ) architecture recurrence reads possibly large external memory multiple times outputting symbol considered continuous form memory network continuity model trained end - - end input - output pairs seen version rnnsearch multiple computational steps per output symbol mini-batch update 2 norm whole gradient parameters measured 5 use learning rate annealing schedule validation cost decreased one epoch learning rate scaled factor 1.5 weights initialized using n ( 0 , 0.05 ) batch size set 128 penn tree dataset repeat training 10 times different random initializations pick one smallest validation cost memnn strongly supervised + ng + nl memory networks approach memnn- wsh weakly supervised heuristic version memnn lstm standard lstm model trained using question / answer pairs\n",
      "output class [[False  True False False False False False False  True False]]\n",
      "output natural language understand - ing natural language understanding ( nlu ) nlu present general language understanding evaluation ( glue ) benchmark collection nlu tasks including question answering sentiment analysis textual entailment glue place constraints model architecture beyond ability process single - sentence sentence - pair inputs make corresponding predictions glue tasks training data plentiful others limited fails match genre test set four datasets feature privately - held test data used ensure benchmark fairly https : //github.com/nyu-mll/glue-baselines https : //github.com/jsalt18-sentence-repl/jiant simplest baseline architecture based sentence - - vector encoders find multi-task training yields better overall scores single - task training amongst models using attention elmo see consistent improvement using elmo embeddings place glove cove embeddings particularly single - sentence tasks among pre-trained sentence representation models observe fairly consistent gains moving cbow skip - thought infersent gensen relative models trained directly glue tasks infersent competitive gensen outperforms two best task sentence representation models substantially underperform cola compared models trained sts - b models trained directly task lag significantly behind performance best sentence representation model wnli model exceeds - frequent - class guessing ( 65.1 % ) rte aggregate best baselines room improvement\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output text understanding introduce neural tree indexers ( nti ) class tree structured models nlp tasks nti takes sequence tokens produces representation constructing full n-ary tree bottom - fashion node nti associated one node transformation functions leaf node mapping non-leaf node composition functions propose different variants node composition function attention tree nti models sequential leaf node transformer lstm chosen nti network forms sequence - tree hybrid model taking advantage conditional compositional powers sequential recursive models natural language inference best score 87.3 % accuracy obtained full tree matching nti model nti - slstm improved performance sequential lstm encoder approximately 2 % node - - node attention models improve performance answer sentence selection deep lstm lstm attention models outperform previous best result large margin nearly 5 - 6 % nasm improves result sets strong baseline combining variational autoencoder soft attention nti model exceeds nasm approximately 0.4 % map sentence classification nti - slstm model performed slightly worse transformed input lstm leaf node function achieved best performance task\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output natural language inference recognize entailment contradiction recognizing entailment contradiction natural language inference ( nli ) nli propose tbcnn - pair neural model recognize entailment contradiction two sentences lever- age newly proposed tbcnn model capture structural information sentences tbcnn robust sequential convolution terms word order distortion pooling layer aggregates information along tree serving semantic compositonality two sentences ' information combined several heuristic matching layers including concatenation element - wise product difference neural layers including embeddings set 300 dimensions word embeddings pretrained word2vec english wikipedia corpus fined tuned training apart model parameters penalty 310 ? 4 dropout chosen validation granularity 0.1 initial learning rate set 1 power decay applied used stochastic gradient descent batch size 50 tbcnn sentence pair model followed simple concatenation alone outperforms existing sentence encoding - based approaches without pretraining including feature - rich method model variant using element - wise product alone significantly worse concatenation element - wise difference combining different matching heuristics improves result tbcnn - pair model concatenation , element - wise product difference yields highest performance 82.1 % applying element - wise product improves accuracy another 0.5 % full tbcnn - pair model outperforms existing sentence encoding - based approaches - cluding 1024d gated recurrent unit ( gru ) - based rnn `` skip - thought `` pretraining\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output reading comprehension cloze - style reading comprehension read comprehend human languages present novel neural network architecture called attention - - attention model place another attention mechanism existing document - level attention model automatically generate attended attention various document - level attentions embedding layer embedding weights randomly initialized uniformed distribution interval [ ? 0.05 , 0.05 ] hidden layer internal weights grus initialized random orthogonal matrices adopted adam optimizer weight updating initial learning rate 0.001 set gradient clipping threshold 5 used batched training strategy 32 samples re-ranking step generate 5 - best list baseline neural network model language model features trained training proportion dataset 8 - gram wordbased setting kneser - ney smoothing trained srilm toolkit implementation done theano ( theano development team , 2016 ) keras models trained tesla k40 gpu aoa reader outperforms state - - - art systems large margin 2.3 % 2.0 % absolute improvements epireader cbtest ne cn test sets adding additional features re-ranking step another significant boost 2.0 % 3.7 % ao reader cbtest ne / cn test sets single model stay par previous best ensemble system absolute improvement 0.9 % beyond best ensemble model ( iterative attention ) cbtest ne validation set ensemble model aoa reader shows significant improvements previous best ensemble models large margin setup new state - - - art system significant boost performance 4.1 % 3.7 % improvements made cnn validation test set cas reader found ne cn category benefit lot re-ranking features ne category performance mainly boosted lm local feature\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output scaling memory - augmented neural networks memory augmented neural networks present mann named sam ( sparse access memory ) thresholding memory modifications sparse subset using efficient data structures content - based read operations model optimal space time respect memory size learning sparse memory access shows sparse models learn comparable efficiency dense models learn effectively priority sort associative recall scaling curriculum tasks sam advance models associative recall task advance curriculum sequences greater 4000 question answering babi tasks manns except ntm able learn solutions comparable previous best results failing 2 tasks sdnc solve 1 tasks best reported result babi ntm perform poorly learning real world data sam outperformed models manns able perform much better chance\n",
      "output class [[False  True False False False False False False  True False]]\n",
      "output baseline natural questions describe bert - based model natural questions jointly predict short long answers single model using pipeline approach split document multiple training instances using overlapping windows tokens aggressively downsample null instances training time create balanced training set use `` [ cls ] `` token training time predict null instances rank spans inference time difference span score `` [ cls ] `` score initialized model bert model finetuned sq u ad 1.1 trained model minimizing loss l adam optimizer batch size 8 tuned number epochs initial learning rate finetuning found training 1 epoch initial learning rate 3 10 ? 5 single tesla p100 gpu bert model nq performs dramatically better models presented original nq paper model closes gap f 1 score achieved original baseline systems super - annotator upper bound 30 % long answer nq task 50 % short answer nq task\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output story cloze test achieve high accuracy fully neural approach involving single feedforward network pre-trained skip - thought embeddings considering last sentence context outperforms models consider full context approach using skip - thought embeddings sentences story feed - forward neural network training model provided validation set considering two endings last sentence prompt use cross-entropy loss sgd learning rate 0.01 training save model every 3000 iterations 3 - layer feed - forward neural network trained validation set summing skip - thought embeddings last sentence ( ls ) story prompt ending gives best accuracy ( 76.5 % ) comparing val - ls- skip val - ls - glo using skip - thought embeddings sentences vs. glove word embeddings confirm success approach lies sizable boost accuracy use pretrained skip - thought embeddings model trained using last sentence ( ls ) story context higher accuracy compared model uses gru encode full context ( fc ) encodes entire context\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output question answering question answering ( qa ) qa introduce dynamic coattention network ( dcn ) end - - end neural network question answering model consists coattentive encoder captures interactions question document dynamic pointing decoder alternates estimating start end answer span preprocess corpus use tokenizer stanford corenlp use glo word vectors pretrained 840b common crawl corpus limit vocabulary words present common crawl corpus set embeddings - - vocabulary words zero max sequence length 600 training hidden state size 200 recurrent units maxout layers linear layers lstms randomly initialized parameters initial state zero sentinel vectors randomly initialized optimized training dynamic decoder set maximum number iterations 4 use maxout pool size 16 dropout regularize network training optimize model using adam models implemented trained chainer dcn capability estimate start end points answer span multiple times model explore local maxima corresponding multiple plausible answers\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output matching natural language sentences semantic matching matching two potentially heterogenous language objects natural language sentences complicated structures sequential hierarchical successful sentence - matching algorithm capture internal structures sentences rich patterns interactions propose deep neural network models adapt convolutional strategy natural language sentences naturally host hierarchical composition simple - - comprehensive fusion matching patterns convolutional architecture generic requiring prior knowledge natural language putting constraints matching tasks experiment : sentence completion arc - ii outperforms arc - senna + mlp performs fairly well experiment iii : paraphrase identification generic matching models perform reasonably well achieving accuracy f1 score close best performer 2008\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output conversational question - swering conversational question answering ( cqa ) machine reading comprehension ( mrc ) cqa propose sdnet contextual attention - based deep neural network task conversational question answering apply inter-attention self - attention passage question obtain effective understanding passage dialogue history sdnet leverages latest breakthrough nlp bert contextual embedding employed weighted sum bert layer outputs locked bert parameters prepend previous rounds questions answers current question incorporate contextual information significantly better results baseline models single sdnet model improves overall f 1 1.6 % compared previous state - - art model coqa flow qa ensemble sdnet model improves overall f 1 score 2.7 % achieve 80 % f 1 score - domain datasets ( 80.7 % ) sdnet overpasses one baseline models second epoch achieves state - - - art results 8 epochs\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output natural language inference recognizing text entailment recognizing text entailment ( rte ) rte proposed unified deep learning framework recognizing textual entailment basic model based building bil - stm models premises hypothesis basic mean pooling encoder roughly form intuition sentence talking introduced simple effective input strategy get ride words hypothesis premise training objective model cross - entropy loss use minibatch sgd rmsprop ( tieleman hinton , 2012 ) optimization batch size 128 dropout layer applied output network dropout rate set 0.25 used pretrained 300d glove 840b vectors initialize word embedding - - vocabulary words training set randomly initialized sampling values uniformly ( 0.05 , 0.05 ) observed attention given nones , verbs adjectives importance attention mechanism helps re-weight words according\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output reading comprehension reading comprehension ( rc ) rc develop novel context zoom - network ( conznet ) rc tasks skip irrelevant parts document generate answer using relevant regions text conznet architecture consists two phases first phase identify relevant regions text employing reinforcement learning algorithm second phase based encoder - decoder architecture comprehends identified regions text generates answer using residual self - attention network encoder rnnbased sequence generator along pointer network decoder method ability select relevant regions text based question also regions related decoder combines span prediction sequence generation allows decoder copy words relevant regions text generate words fixed vocabulary compare model reported models seq2seq asr bidaf multi-range reasoning unit ( mru ) implemented two baseline models context zoom layer model implemented using python tensorflow weights model initialized glorot initialization biases initialized zeros use 300 dimensional word vectors glove 840 billion pre-trained vectors initialize word embeddings words appear glove initialized sampling uniform random distribution apply dropout layers keep probability 0.8 number hidden units set 100 trained model adadelta ( zeiler , 2012 ) optimizer 50 epochs initial learning rate 0.1 minibatch size 32 performance model gradually dropped sample size 7 onwards shows evidence relevant sentences sufficient answer question model improved dramatically sample sizes 3 5 compared sample size 1 self - attention mechanism context zoom layer important component identify related relevant sentences\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output tracking world state simple story understanding scenario new kind memory - augmented neural network uses distributed memory processor architecture recurrent entity network ( entnet ) consists fixed number dynamic memory cells containing vector key w j vector value ( content ) h j cell associated `` processor `` simple gated recurrent network may update cell value given input entnet seen bank gated rnns parameters hidden states correspond latent concepts attributes describe laws world hidden state updated new information relevant concept received keys used addressing / gating mechanism correspond concepts entities modified learning inference synthetic world model task memn2n set number hops equal ? 2 embedding dimension = 20 entnet embedding dimension = 20 5 memory slots lstm 50 hidden units resulted significantly parameters two models models trained adam initial learning rates set grid search { 0.1 , 0.01 , 0.001 } divided 2 every 10,000 updates memn2n worst performance degrades quickly length sequence increases lstm performs better loses accuracy length sequence increases childre n 's book test ( cbt ) methods limited memory lstms perform well frequent , syntax based words prepositions verbs\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output multi - document reading comprehension machine reading comprehension ( mrc ) mrc propose deep cascade model combines advantages methods coarse - - fine manner deep cascade model designed keep balance effectiveness efficiency simple features ranking functions used select candidate set relevant contents filtering irrelevant documents paragraphs selected paragraphs passed attention - based deep mrc model extracting actual answer span word level better support answer extraction introduce document extraction paragraph extraction two auxiliary tasks helps narrow entire search space jointly optimize three tasks unified deep mrc model shares common bottom layers cascaded structure enables models perform coarse - - fine pruning different stages better models learnt effectively efficiently overall framework model consists document retrieval paragraph retrieval answer extraction first module takes question collection raw documents input module subsequent stage consumes output previous stage prunes documents , paragraphs answer spans given question ranking function used preliminary filter discard irrelevant documents paragraphs extraction function deal auxiliary document paragraph extraction tasks jointly optimized final answer extraction module better extraction performance choose k = 4 n = 2 good performance evaluating dev set multi-task deep attention framework adopt adam optimizer training mini-batch size 32 initial learning rate 0.0005 use glove 300 dimensional word embeddings triviaqa train word2 vec word embeddings whole dureader corpus dureader word embeddings fixed training hidden size lstm set 150 triviaqa 128 dureader task - specific hyper - parameters set ? 1 = ? 2 = 0.5 regularization parameter set small value 0.01 models trained nvidia tesla m40 gpu cudnn lstm cell tensorflow 1.3 adopting deep cascade learning framework proposed model outperforms previous state - - - art methods evident margin shared lstm plays important role answer extraction among multiple documents helps normalize content probability score multiple documents answers extracted different documents directly compared keep ranking order document ranking component incorporating manual features performance improved slightly preliminary cascade ranking multi-task answer extraction strategy vital final performance serve good trade - pure pipeline method fully joint learning method jointly training three extraction tasks provide great benefits\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output machine reading comprehension unanswerable questions machine reading comprehension ( mrc ) mrc three sub - tasks answer pointer - answer pointer answer verifier propose u - net incorporate three sub - tasks unified model answer pointer predict - didate answer span question -answer pointer avoid selecting text span question answer answer verifier determine probability unanswerability question candidate answer information introduce universal node process question context passage single contiguous sequence tokens greatly improves conciseness u - net universal node acts question passage learn question answerable use spacy process question passage obtain tokens pos tags ner tags lemmas tags 12 dimensions embed pos tags ner tags 3 binary features exact match lower - case match lemma match question passage 100 - dim glove pretrained word embeddings 1024 - dim elmo embeddings lstm blocks bi-directional one single layer set hidden layer dimension 125 attention layer dimension 250 added dropout layer overall modeling layers including embedding layer dropout rate 0.3 adam optimizer learning rate 0.002 model f 1 score 74.0 em score 70.3 development set f 1 score 72.6 em score 69.2 test set outperforms previous approaches comparing best - performing systems model simple architecture end - - end model among end - - end models achieve best f1 scores node u shared called universal learns information interaction question passage shared performance slightly degraded sharing bilstm effective method improve quality encoder removing plausible answer pointer performance dropped answer verifier performance dropped greatly second block ( multi - level attention ) u - net split output encoded presentation pass self - attention layer\n",
      "output class [[False  True False  True False False  True False False  True]]\n",
      "output neural - network - based question answering machine comprehension ( mc ) question answering ( qa ) take closer look modeling questions end - - end neural network framework introduced syntactic information help encode questions viewed modelled different types questions information shared among adaptation problem test models stanford question answering dataset ( squad ) squad dataset consists 100,000 questions annotated crowdsourcing workers selected set wikipedia articles use pre-trained 300 - glove 840b vectors initialize word embeddings - - vocabulary ( oov ) words initialized randomly gaussian samples charcnn filter length 1 , 3 , 5 50 dimensions cluster number k discriminative block 100 adam method used optimization first momentum set 0.9 second 0.999 initial learning rate 0.0004 batch size 32 hidden states grus , treelstms 500 dimensions word - level embedding w 300 dimensions set max length document 500 explicit question - type dimension et 50 apply dropout encoder layer aggregation layer dropout rate 0.5 model achieves 68.73 % em score 77.39 % f1 score ranked among state art single models baseline model using q- code achieved 68.00 % 77.36 % em f 1 scores added explicit question type - code baseline model performance improved slightly 68.16 % ( em ) 77.58 % ( f1 ) used treelstm introduce syntactic parses question representation understanding shows improvement observed 78.38 % f1 score whole development set\n",
      "output class [[False  True False  True False False  True False False False]]\n",
      "output machine comprehension question answering introduce general framework phasecond use multiple attention layers self - attention model multi-hops architecture used captures question - aware passage representations refines results using domains machine translation jointly align translate words , question - passage attention models machine comprehension question answering calculate alignment matrix corresponding question passage word pairs em result baseline iterative aligner lower rnet rnet shows performance different number layers question - passage attention phase self - attention phase question - passage attention phase using single layer n't degrade performance default setting two layers resulting different conclusion multiple stacking layers needed allow evidence fully propagated passage\n",
      "output class [[False  True False False False False  True False False False]]\n",
      "output neural natural language inference natural language inference neural - network - based inference natural language inference ( nli ) nli recognizing textual entailment ( rte ) enrich neural - network - based nli models external knowledge coattention local inference collection inference composition components show proposed model improves state - - - art nli models achieve better performances snli multinli datasets advantage using external knowledge significant size training data restricted dimension hidden states lstms word embeddings 300 word embeddings initialized 300d glove 840b - - vocabulary words initialized randomly adam ( kingma ba , 2014 ) used optimization initial learning rate 0.0004 mini - batch size set 32 proposed model namely knowledge - based inference model ( kim ) enriches esim external knowledge obtains accuracy 88.6 % difference esim kim statistically significant one - tailed paired t- test 99 % significance level multinli dataset baseline esim achieves 76.8 % 75.8 % - domain cross - domain test set extend esim external knowledge achieve significant gains 77.2 % 76.4 %\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output answer open-domain questions answering factoid questions open - domain setting open - domain question answering answer question retrieve relevant articles among 5 million items scan identify answer term machine reading scale ( mrs ) treats wikipedia collection articles approach generic could switched collections documents books daily updated newspapers single knowledge source forces model precise searching answer use 3 - layer bidirectional lstms h = 128 hidden units paragraph question encoding apply stanford corenlp toolkit tokenization generating lemma partof - speech named entity tags training examples sorted length paragraph divided minibatches 32 examples adamax optimization dropout p = 0.3 applied word embeddings hidden units lstms system ( single model ) achieve 70.0 % exact match 79.0 % f 1 scores test set surpasses published results without aligned question embedding feature system achieve f1 77 % remove f aligned f exact match performance drops dramatically\n",
      "output class [[False  True False False  True False  True False False False]]\n",
      "output machine reading machine reading comprehension address lack real natural language training data introducing novel approach building supervised reading comprehension data set observe summary paraphrase sentences associated documents readily converted context - query - answer triples using simple entity detection anonymisation algorithms collected two new corpora roughly million news stories associated queries cnn daily mail websites demonstrate efficacy new corpora building novel deep learning models reading comprehension attention - based models outperform pure lstm - based approaches word distance benchmark relatively strong performance word distance benchmark relative frame - semantic benchmark perform better neural models impatient attentive readers outperforming models deep lstm reader performs surprisingly well\n",
      "output class [[False False False False False False  True False False False]]\n",
      "completed natural_language_inference 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#####Test model\n",
    "#test_input_dir = \"/content/drive/My Drive/Dev_v2/\"\n",
    "test_input_dir = \"/content/drive/MyDrive/sub3_ph22/\"\n",
    "# test_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
    "#          \"relation-classification\", \"text-classification\"]\n",
    "test_list_of_folders = list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
    "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
    "                   \"document_classification\",\"entity_linking\",\n",
    "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
    "                   \"natural_language_inference\"]\n",
    "test_input_stanza_list = []\n",
    "test_input_sent_num_list = []\n",
    "test_file_name_list = []\n",
    "test_input_entity_list = []\n",
    "test_input_triple_list = []\n",
    "test_input_unit_name = []\n",
    "test_total_phrases = 0\n",
    "test_triplet_list = []\n",
    "test_total_triplets = 0\n",
    "for fls in test_list_of_folders:\n",
    "  for i in os.listdir(test_input_dir + fls + '/'):\n",
    "    triple_list = []\n",
    "    unit_name = []\n",
    "    triplet_lines_list = []\n",
    "    for files in os.listdir(test_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None, map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        test_input_stanza_list.append(stanza_lines_list)\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        sentence_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
    "        test_input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
    "      if files.endswith(\"entities.txt\"):\n",
    "        entities_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        entities_list = list(filter(None,map(lambda x:x,entities_file.read().splitlines()))) # filter empty strings and split into lines\n",
    "        test_input_entity_list.append(entities_list)\n",
    "    ### start embedd function for the info unit prediction\n",
    "    entity_phrase_only = [j.lower().split('\\t') for j in entities_list]\n",
    "    entity_phrase_only.sort(key=lambda x: (int(x[0]),int(x[1])))\n",
    "    entity_ph_only_string = ' '.join([j[3] for j in entity_phrase_only])\n",
    "    word_tokens = word_tokenize(entity_ph_only_string)\n",
    "    filter_ph_only_string = ' '.join([w for w in word_tokens if not w in stop_words])\n",
    "    print(\"output\", filter_ph_only_string)\n",
    "    output_info_folder = os.path.join(test_input_dir,fls,str(i),'triples')\n",
    "    if not os.path.exists(output_info_folder):\n",
    "      os.makedirs(output_info_folder)\n",
    "    with torch.no_grad():\n",
    "      test_input = tokenizer.encode_plus(filter_ph_only_string.lower(), None, add_special_tokens=True, \n",
    "                                         max_length= MAX_LEN, pad_to_max_length=True, \n",
    "                                         return_token_type_ids=True, truncation=True, \n",
    "                                         return_length = True,return_tensors=\"pt\")\n",
    "      test_ids = test_input['input_ids'].to(device, dtype = torch.long)\n",
    "      test_mask = test_input['attention_mask'].to(device, dtype = torch.long)\n",
    "      test_token_type_ids = test_input['token_type_ids'].to(device, dtype = torch.long)\n",
    "      test_lengths = test_input['length']#.squeeze(dim=1)#.to(device, dtype = torch.long)\n",
    "      #print(\"shape\",test_ids.shape, test_mask.shape, test_token_type_ids.shape, test_lengths)\n",
    "      outputs = model(test_ids, test_mask, test_token_type_ids, test_lengths)\n",
    "      output_idx = np.array(torch.sigmoid(outputs).cpu().detach().numpy().tolist()) >=0.5\n",
    "      print(\"output class\", output_idx)\n",
    "      # for i in range(output_idx.shape[1]):\n",
    "      #   if output_idx[0][i] == True:\n",
    "      #     output_file=open(os.path.join(test_input_dir,fls,str(i),'triples',infoname_idx_to_test[i]),\"a\")\n",
    "      #     output_file.write('(Contribution||has||'+infoname_test_to_first_triplet[infoname_idx_to_test[i]]+')'+\"\\n\")\n",
    "  print(\"completed\",fls,test_total_phrases)\n",
    "    ### Training label formation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #   t_entity_list = [j.split('\\t') for j in entities_list]\n",
    "  #   t_entity_list.sort(key=lambda x: (int(x[0]),int(x[1]))) #reverse = True)\n",
    "  #   t_triplet_formed = []\n",
    "  #   for t5 in range(len(t_entity_list)):\n",
    "  #     try:\n",
    "  #       if((t_entity_list[t5][0]==t_entity_list[t5+1][0])  and (t_entity_list[t5][0]==t_entity_list[t5+2][0])):\n",
    "  #         t_triplet_formed = t_triplet_formed + [[t_entity_list[t5][3], t_entity_list[t5+1][3], t_entity_list[t5+2][3]]]\n",
    "  #     except Exception as e:\n",
    "  #       continue\n",
    "  #   ###triplet end\n",
    "  #   t_triplet_formed_string = [' '.join(x) for x in t_triplet_formed]\n",
    "  #   #print(\"size\",len(t_triplet_formed_string))\n",
    "  #   output_info_folder = os.path.join(test_input_dir,fls,str(i),'triples')\n",
    "  #   if not os.path.exists(output_info_folder):\n",
    "  #     os.makedirs(output_info_folder)\n",
    "  #   with torch.no_grad():\n",
    "  #     for t6 in range(len(t_triplet_formed_string)): #lowercase the string input to classifier\n",
    "  #       test_input = tokenizer.encode_plus(t_triplet_formed_string[t6].lower(), None, add_special_tokens=True, \n",
    "  #                                          max_length= MAX_LEN, pad_to_max_length=True, \n",
    "  #                                          return_token_type_ids=True, truncation=True, \n",
    "  #                                          return_length = True,return_tensors=\"pt\")\n",
    "  #       test_ids = test_input['input_ids'].to(device, dtype = torch.long)\n",
    "  #       test_mask = test_input['attention_mask'].to(device, dtype = torch.long)\n",
    "  #       test_token_type_ids = test_input['token_type_ids'].to(device, dtype = torch.long)\n",
    "  #       test_lengths = test_input['length']#.squeeze(dim=1)#.to(device, dtype = torch.long)\n",
    "  #       #print(\"shape\",test_ids.shape, test_mask.shape, test_token_type_ids.shape, test_lengths)\n",
    "  #       outputs = model(test_ids, test_mask, test_token_type_ids, test_lengths)\n",
    "  #       _, output_idx = torch.max(outputs, dim =1)\n",
    "  #       t_output_class = output_idx.cpu().detach().numpy().tolist()\n",
    "  #       #print(t_output_class,'(' + '||'.join(t_triplet_formed[0])+')')\n",
    "        \n",
    "  #       #print(\"hbas\",t_output_class[0])\n",
    "  #       #for t7 in range(len(t_output_class)):\n",
    "  #         #print(infoname_test_to_idx[t_output_class[t7]])\n",
    "  #       output_file=open(os.path.join(output_info_folder, infoname_idx_to_test[t_output_class[0]]),\"a\")\n",
    "  #       output_file.write('(' + '||'.join(t_triplet_formed[t6])+')'+\"\\n\")\n",
    "  #         ####Info classification end\n",
    "  #         #### end embedd function for test\n",
    "  #     test_file_name_list.append(fls + '/' + str(i))\n",
    "  #   #if len(os.listdir(os.path.join(test_input_dir,fls,str(i),'triples'))) !=0:\n",
    "  #   for file_add in os.listdir(test_input_dir + fls + '/' + str(i) + '/triples'):\n",
    "  #     if file_add in list(infoname_test_to_first_triplet.keys()):\n",
    "  #       output_file=open(os.path.join(test_input_dir,fls,str(i),'triples',file_add),\"a\")\n",
    "  #       output_file.write('(Contribution||has||'+infoname_test_to_first_triplet[file_add]+')'+\"\\n\")\n",
    "  # print(\"completed\",fls,test_total_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DVrXtLb6VcY",
    "outputId": "3ca411a7-9b6f-4024-eb57-f53cce6e78e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed constituency_parsing 0\n",
      "completed coreference_resolution 0\n",
      "completed data-to-text_generation 0\n",
      "completed dependency_parsing 0\n",
      "completed document_classification 0\n",
      "completed entity_linking 0\n",
      "completed face_alignment 0\n",
      "completed face_detection 0\n",
      "completed hypernym_discovery 0\n",
      "completed natural_language_inference 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#####Test model\n",
    "#test_input_dir = \"/content/drive/My Drive/Dev_v2/\"\n",
    "test_input_dir = \"/content/drive/MyDrive/sub3_ph22/\"\n",
    "# test_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
    "#          \"relation-classification\", \"text-classification\"]\n",
    "test_list_of_folders = list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
    "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
    "                   \"document_classification\",\"entity_linking\",\n",
    "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
    "                   \"natural_language_inference\"]\n",
    "test_input_stanza_list = []\n",
    "test_input_sent_num_list = []\n",
    "test_file_name_list = []\n",
    "test_input_entity_list = []\n",
    "test_input_triple_list = []\n",
    "test_input_unit_name = []\n",
    "test_total_phrases = 0\n",
    "test_triplet_list = []\n",
    "test_total_triplets = 0\n",
    "for fls in test_list_of_folders:\n",
    "  for i in os.listdir(test_input_dir + fls + '/'):\n",
    "    triple_list = []\n",
    "    unit_name = []\n",
    "    triplet_lines_list = []\n",
    "    for files in os.listdir(test_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None, map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        test_input_stanza_list.append(stanza_lines_list)\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        sentence_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
    "        test_input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
    "      if files.endswith(\"entities.txt\"):\n",
    "        entities_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        entities_list = list(filter(None,map(lambda x:x,entities_file.read().splitlines()))) # filter empty strings and split into lines\n",
    "        test_input_entity_list.append(entities_list)\n",
    "    ### start embedd function for the info unit prediction\n",
    "    entity_phrase_only = [j.lower().split('\\t') for j in entities_list]\n",
    "    entity_phrase_only.sort(key=lambda x: (int(x[0]),int(x[1])))\n",
    "    entity_ph_only_string = ' '.join([j[3] for j in entity_phrase_only])\n",
    "    word_tokens = word_tokenize(entity_ph_only_string)\n",
    "    filter_ph_only_string = ' '.join([w for w in word_tokens if not w in stop_words])\n",
    "    #print(\"output\", filter_ph_only_string)\n",
    "    output_info_folder = os.path.join(test_input_dir,fls,str(i),'triples')\n",
    "    if not os.path.exists(output_info_folder):\n",
    "      os.makedirs(output_info_folder)\n",
    "    with torch.no_grad():\n",
    "      test_input = tokenizer.encode_plus(filter_ph_only_string.lower(), None, add_special_tokens=True, \n",
    "                                         max_length= MAX_LEN, pad_to_max_length=True, \n",
    "                                         return_token_type_ids=True, truncation=True, \n",
    "                                         return_length = True,return_tensors=\"pt\")\n",
    "      test_ids = test_input['input_ids'].to(device, dtype = torch.long)\n",
    "      test_mask = test_input['attention_mask'].to(device, dtype = torch.long)\n",
    "      test_token_type_ids = test_input['token_type_ids'].to(device, dtype = torch.long)\n",
    "      test_lengths = test_input['length']#.squeeze(dim=1)#.to(device, dtype = torch.long)\n",
    "      #print(\"shape\",test_ids.shape, test_mask.shape, test_token_type_ids.shape, test_lengths)\n",
    "      outputs = model(test_ids, test_mask, test_token_type_ids, test_lengths)\n",
    "      output_idx = np.array(torch.sigmoid(outputs).cpu().detach().numpy().tolist()) >=0.5\n",
    "      #print(\"output class\", output_idx)\n",
    "      for i in range(output_idx.shape[1]):\n",
    "        if output_idx[0][i] == True:\n",
    "          #print(infoname_idx_to_test[i])\n",
    "          output_file=open(os.path.join(output_info_folder,infoname_idx_to_test[i]),\"w\")\n",
    "          output_file.write('(Contribution||has||'+infoname_test_to_first_triplet[infoname_idx_to_test[i]]+')'+\"\\n\")\n",
    "  print(\"completed\",fls,test_total_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neDBTmw96VfC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIrX-GYao_HZ"
   },
   "source": [
    "#TASK-3 using KG-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fh_fhUCRWtty",
    "outputId": "3d17ee27-68d4-4944-d179-4a81bcff7385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f83KPTB2W0EZ"
   },
   "outputs": [],
   "source": [
    "infoname_to_idx = {'approach.txt': 0, 'model.txt': 1, 'dataset.txt': 2, \n",
    "        'experimental-setup.txt': 3, 'hyperparameters.txt': 4,  'baselines.txt': 5, 'results.txt': 6, \n",
    "        'tasks.txt': 7, 'experiments.txt': 8, 'ablation-analysis.txt': 9}\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRf-ViSBW0tm",
    "outputId": "ce416740-c36f-433b-a62d-e35b3bfcef2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/content/train' already exists and is not an empty directory.\n",
      "fatal: destination path '/content/valid' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ncg-task/training-data.git \"/content/train\"\n",
    "!git clone https://github.com/ncg-task/trial-data.git \"/content/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXtXyrzsW0wm",
    "outputId": "292a1f94-5b5d-447a-b904-2bae21b25bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed phrase_grounding 96\n",
      "completed question_answering 717\n",
      "completed sentence_compression 955\n",
      "completed question_similarity 997\n",
      "completed question_generation 1072\n",
      "completed sentence_classification 1354\n",
      "completed text_summarization 2341\n",
      "completed paraphrase_generation 2507\n",
      "completed smile_recognition 2557\n",
      "completed prosody_prediction 2655\n",
      "completed natural_language_inference 10016\n",
      "completed semantic_parsing 10180\n",
      "completed sentiment_analysis 14348\n",
      "completed temporal_information_extraction 14432\n",
      "completed topic_models 14473\n",
      "completed relation_extraction 15531\n",
      "completed semantic_role_labeling 15836\n",
      "completed query_wellformedness 15866\n",
      "completed negation_scope_resolution 15952\n",
      "completed sarcasm_detection 16077\n",
      "completed passage_re-ranking 16194\n",
      "completed text-to-speech_synthesis 16372\n",
      "completed text_generation 16771\n",
      "completed part-of-speech_tagging 17231\n"
     ]
    }
   ],
   "source": [
    "# Loading the Training Data\n",
    "input_dir = \"/content/train/\"\n",
    "import os\n",
    "import torch\n",
    "# list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
    "#                   \"relation-classification\",\"text-classification\"]\n",
    "list_of_folders = ['phrase_grounding', 'question_answering',\n",
    " 'sentence_compression', 'question_similarity',\n",
    " 'question_generation', 'sentence_classification',\n",
    " 'text_summarization', 'paraphrase_generation',\n",
    " 'smile_recognition', 'prosody_prediction',\n",
    " 'natural_language_inference', 'semantic_parsing',\n",
    " 'sentiment_analysis', 'temporal_information_extraction',\n",
    " 'topic_models', 'relation_extraction',\n",
    " 'semantic_role_labeling', 'query_wellformedness',\n",
    " 'negation_scope_resolution', 'sarcasm_detection',\n",
    " 'passage_re-ranking', 'text-to-speech_synthesis',\n",
    " 'text_generation', 'part-of-speech_tagging']\n",
    "input_stanza_list = []\n",
    "input_sent_num_list = []\n",
    "input_entity_list = []\n",
    "file_name_list = []\n",
    "input_triplet_list = []\n",
    "total_triplets = 0\n",
    "for fls in list_of_folders:\n",
    "  for i in os.listdir(input_dir + fls + '/'):\n",
    "    triplet_lines_list =[]\n",
    "    for files in os.listdir(input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None,map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        input_stanza_list.append(stanza_lines_list)\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        sentence_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
    "        input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
    "      if files.endswith(\"entities.txt\"):\n",
    "        entities_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        entities_list = list(filter(None,map(lambda x:x.lower(),entities_file.read().splitlines()))) # filter empty strings and split into lines\n",
    "        input_entity_list.append(entities_list)\n",
    "    file_name_list.append(fls + '/' + str(i))\n",
    "    for files in os.listdir(input_dir + fls + '/' + str(i) + '/triples'):\n",
    "      #unit_name.append(files)\n",
    "      info_unit_file = open(input_dir + fls + '/' + str(i) + '/triples/' + files, \"r\")\n",
    "      if (files != 'research-problem.txt') and (files!='code.txt'):\n",
    "        # info_unit_list = list(filter(None,map(lambda x:x.lower(), info_unit_file.read().lower().splitlines()))) # filter empty strings and split into lines\n",
    "        # triple_list.extend(info_unit_list[1:])  # a list for each article of triples (triples of all info units of a article)\n",
    "        # list_unit_corres_triples = [infoname_to_idx[files]]*len(info_unit_list[1:]) # list for assigning information units corresponding to each triple\n",
    "        # unit_name.extend(list_unit_corres_triples)\n",
    "      ####### start insert code\n",
    "        triplet_file = open(input_dir + fls + '/' + str(i) + '/' + 'triples' + '/' + files)\n",
    "        triplet_lines = (triplet_file.read()).lower()\n",
    "        triplet_lines_string = list(filter(None,triplet_lines.splitlines()))\n",
    "        triplets_broken_into_three = list(map(lambda x: x[1:-1].split(\"||\")[1], triplet_lines_string)) \n",
    "        triplet_lines_list = triplets_broken_into_three[1:] + triplet_lines_list\n",
    "    input_triplet_list.append(triplet_lines_list)\n",
    "    total_triplets += len(triplet_lines_list)\n",
    "    ###### finish insertion\n",
    "  print(\"completed\",fls,total_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1BTYiuqZAQu",
    "outputId": "622275d8-0b4a-4c6a-b145-347d08c7cfda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'has',\n",
       " 'has',\n",
       " 'compared to',\n",
       " 'on',\n",
       " 'on',\n",
       " 'has',\n",
       " 'prefers',\n",
       " 'is',\n",
       " 'to achieve',\n",
       " 'that consist of',\n",
       " 'has',\n",
       " 'maintains',\n",
       " 'across',\n",
       " 'shows',\n",
       " 'of encoding',\n",
       " 'has',\n",
       " 'outperforms',\n",
       " 'by',\n",
       " 'has',\n",
       " 'delivers',\n",
       " 'see that',\n",
       " 'offers',\n",
       " 'on',\n",
       " 'on',\n",
       " 'has',\n",
       " 'outperform',\n",
       " 'across',\n",
       " 'has',\n",
       " 'showing',\n",
       " 'adapt',\n",
       " 'to process and score',\n",
       " 'for',\n",
       " 'construct',\n",
       " 'has',\n",
       " 'deterministically converted to',\n",
       " 'to extract',\n",
       " 'from',\n",
       " 'has',\n",
       " 'to',\n",
       " 'of',\n",
       " 'has',\n",
       " 'to process',\n",
       " 'has',\n",
       " 'use',\n",
       " 'to encode',\n",
       " 'in',\n",
       " 'has',\n",
       " 'include',\n",
       " 'directly computes',\n",
       " 'as',\n",
       " 'of',\n",
       " 'that does not use']"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_triplet_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2Du0PhBgI8x"
   },
   "outputs": [],
   "source": [
    "##train label formation\n",
    "train_entity_label=[]\n",
    "train_entity_ph = []\n",
    "for i in range(len(file_name_list)):\n",
    "  t1_label=[0]*len(input_entity_list[i])\n",
    "  entity_phr = [j.lower().split('\\t') for j in input_entity_list[i]]\n",
    "  entity_phr.sort(key=lambda x: (int(x[0]),int(x[1])))\n",
    "  entity_ph_only = [j[3] for j in entity_phr]\n",
    "  train_entity_ph.append(entity_ph_only)\n",
    "  for k in range(len(entity_ph_only)):\n",
    "    if entity_ph_only[k] in input_triplet_list[i]:\n",
    "      t1_label[k]=1\n",
    "  train_entity_label.append(t1_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THQxi9WDo-a0",
    "outputId": "07140ac6-7e33-4edd-dc49-e25d3e921572"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_entity_label[1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skt8nVdSo-eG",
    "outputId": "4214dda0-4b85-4ec3-f613-03ef05c951de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knowledge base question answering',\n",
       " 'knowledge base question answering ( qa )',\n",
       " 'qa',\n",
       " 'semantic parsing',\n",
       " 'to',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'kb qa',\n",
       " 'kb qa',\n",
       " 'for',\n",
       " 'each input question',\n",
       " 'construct',\n",
       " 'explicit structural semantic parse ( semantic graph )',\n",
       " 'semantic parses',\n",
       " 'deterministically converted to',\n",
       " 'query',\n",
       " 'to extract',\n",
       " 'answers',\n",
       " 'from',\n",
       " 'kb']"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_entity_ph[1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "siPuxj7yW0zk",
    "outputId": "bbdbe4ac-9140-4bb9-f8c7-3f724f767656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed machine-translation 485\n",
      "completed named-entity-recognition 928\n",
      "completed question-answering 1482\n",
      "completed relation-classification 2023\n",
      "completed text-classification 2593\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "##### validation dataset reading\n",
    "\n",
    "val_input_dir = \"/content/valid/\"\n",
    "val_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
    "         \"relation-classification\", \"text-classification\"]\n",
    "val_input_stanza_list = []\n",
    "val_input_sent_num_list = []\n",
    "val_file_name_list = []\n",
    "val_input_entity_list = []\n",
    "valid_triplet_list = []\n",
    "valid_total_triplets = 0\n",
    "for fls in val_list_of_folders:\n",
    "  for i in os.listdir(val_input_dir + fls + '/'):\n",
    "    triplet_lines_list = []\n",
    "    for files in os.listdir(val_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None, map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        val_input_stanza_list.append(stanza_lines_list)\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        sentence_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
    "        val_input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
    "      if files.endswith(\"entities.txt\"):\n",
    "        entities_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        entities_list = list(filter(None,map(lambda x:x.lower(),entities_file.read().splitlines()))) # filter empty strings and split into lines\n",
    "        val_input_entity_list.append(entities_list)\n",
    "    val_file_name_list.append(fls + '/' + str(i))\n",
    "    for files in os.listdir(val_input_dir + fls + '/' + str(i) + '/triples'):\n",
    "      #unit_name.append(files)\n",
    "      info_unit_file = open(val_input_dir + fls + '/' + str(i) + '/triples/' + files, \"r\")\n",
    "      if (files != 'research-problem.txt') and (files!='code.txt'):\n",
    "      ####### start insert code\n",
    "        triplet_file = open(val_input_dir + fls + '/' + str(i) + '/' + 'triples' + '/' + files)\n",
    "        triplet_lines = (triplet_file.read()).lower()\n",
    "        triplet_lines_string = list(filter(None,triplet_lines.splitlines()))\n",
    "        triplets_broken_into_three = list(map(lambda x: x[1:-1].split(\"||\")[1], triplet_lines_string)) \n",
    "        triplet_lines_list = triplets_broken_into_three[1:] + triplet_lines_list\n",
    "    valid_triplet_list.append(triplet_lines_list)\n",
    "    valid_total_triplets += len(triplet_lines_list)\n",
    "    ###### finish insertion\n",
    "  print(\"completed\",fls,valid_total_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DSc1BabqlaJ",
    "outputId": "b26d4b9e-d846-480e-9969-5456216faec6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tasks',\n",
       " 'outperforms',\n",
       " 'for',\n",
       " 'on',\n",
       " 'tasks',\n",
       " 'outperform',\n",
       " 'for',\n",
       " 'in the term of',\n",
       " 'outperforms',\n",
       " 'learned from',\n",
       " 'outperforms',\n",
       " 'by',\n",
       " 'tasks',\n",
       " 'improve the results of',\n",
       " 'by',\n",
       " 'in terms of',\n",
       " 'outperforms',\n",
       " 'on',\n",
       " 'improves',\n",
       " 'by']"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_triplet_list[1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1w7EJETW02H"
   },
   "outputs": [],
   "source": [
    "##valid label formation\n",
    "valid_entity_label=[]\n",
    "valid_entity_ph = []\n",
    "for i in range(len(val_file_name_list)):\n",
    "  t2_label=[0]*len(val_input_entity_list[i])\n",
    "  entity_phr = [j.lower().split('\\t') for j in val_input_entity_list[i]]\n",
    "  entity_phr.sort(key=lambda x: (int(x[0]),int(x[1])))\n",
    "  entity_ph_only = [j[3] for j in entity_phr]\n",
    "  valid_entity_ph.append(entity_ph_only)\n",
    "  for k in range(len(entity_ph_only)):\n",
    "    if entity_ph_only[k] in valid_triplet_list[i]:\n",
    "      t2_label[k]=1\n",
    "  valid_entity_label.append(t2_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gATX-jmqlfS",
    "outputId": "0fd9917b-dce6-466c-cb7f-6dd87eef3700"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0]\n",
      "['frequency - agnostic word representation', 'word embeddings learned in several tasks are biased towards word frequency', 'learned embeddings of rare words and popular words behave differently', 'embeddings of rare words and popular words actually lie in different subregions of the space', 'different behaviors of the embeddings of popular words and rare words are problematic', 'to learn', 'frequency - agnostic word embedding ( frage )', 'minimize', 'task - specific loss', 'by optimizing', 'task - specific parameters', 'together with', 'word embeddings', 'introduce', 'another discriminator', 'a word embedding', 'input', 'classifies', 'popular / rare word', 'discriminator']\n"
     ]
    }
   ],
   "source": [
    "print(valid_entity_label[1][:20])\n",
    "print(valid_entity_ph[1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bReh6gJHqkfn"
   },
   "outputs": [],
   "source": [
    "## single list conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2LbNFHQq6zf",
    "outputId": "122e65f4-4007-4f1e-92cb-d671057f0918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train all entities size 30485 30485\n",
      "Valid all entities size 4777 4777\n",
      "Train all entities size 13745 13745\n",
      "Valid all entities size 3161 3161\n"
     ]
    }
   ],
   "source": [
    "### Flatten train and valid dataset\n",
    "from collections import Counter\n",
    "train_ph_list = [j for sub in train_entity_ph for j in sub]\n",
    "train_label_list = [j for sub in train_entity_label for j in sub]\n",
    "valid_ph_list = [j for sub in valid_entity_ph for j in sub]\n",
    "valid_label_list = [j for sub in valid_entity_label for j in sub]\n",
    "print(\"Train all entities size\", len(train_ph_list), len(train_label_list))\n",
    "print(\"Valid all entities size\", len(valid_ph_list), len(valid_label_list))\n",
    "train_ph_list, train_label_list = list(zip(*(set(zip(train_ph_list,train_label_list)))))\n",
    "valid_ph_list, valid_label_list = list(zip(*(set(zip(valid_ph_list,valid_label_list)))))\n",
    "print(\"Train all entities size\", len(train_ph_list), len(train_label_list))\n",
    "print(\"Valid all entities size\", len(valid_ph_list), len(valid_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_81m6q8uw6uz",
    "outputId": "3c1118b4-752a-4f1a-ed83-6663efd15c88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11241, 2504]"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Counter(train_label_list).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvWMV3lOzXG_"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RJyxW_lz1ab"
   },
   "outputs": [],
   "source": [
    "########### Pre-processing complete for train and valid. \n",
    "########### Start model building\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "seed_val = 66\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMBFFvoTz3zS"
   },
   "outputs": [],
   "source": [
    "##### Parameters for the model\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-05\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"allenai/biomed_roberta_base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\t\t\t\n",
    "MAX_LEN = 25\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0U1QX4Yz32t"
   },
   "outputs": [],
   "source": [
    "#### Dataset Loader\n",
    "class Triage(Dataset):\n",
    "    def __init__(self, data_in,data_out, tokenizer, max_len):\n",
    "        self.len = len(data_in)\n",
    "        self.data = data_in\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label = data_out\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # title = str(self.data.TITLE[index])\n",
    "        # title = \" \".join(title.split())\n",
    "        title = self.data[index]\n",
    "        inputs = self.tokenizer.encode_plus(title,\n",
    "            None,add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_length = True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        lengths = inputs['length']\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'lengths': torch.tensor(lengths, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.label[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "      return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbjymZ4Cz36B"
   },
   "outputs": [],
   "source": [
    "#### Train and Validation Loader \n",
    "training_set = Triage(train_ph_list,train_label_list, tokenizer, MAX_LEN)\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,'shuffle': True,'num_workers': 0}\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "valid_set = Triage(valid_ph_list,valid_label_list, tokenizer, MAX_LEN)\n",
    "valid_params = {'batch_size': VALID_BATCH_SIZE,'shuffle': True,'num_workers': 0}\n",
    "valid_loader = DataLoader(valid_set, **valid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYcL-Pwbz393",
    "outputId": "02e550d7-5181-4ca0-e366-7099c395b5a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SCIBERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(768, 400, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (l2): Dropout(p=0.1, inplace=False)\n",
       "  (l3): Linear(in_features=800, out_features=400, bias=True)\n",
       "  (l4): Linear(in_features=400, out_features=100, bias=True)\n",
       "  (l5): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SciBERT + Bi-LSTM model for binary classification\n",
    "class SCIBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SCIBERTClass, self).__init__()\n",
    "        #self.l1 = AutoModel.from_pretrained(\"allenai/biomed_roberta_base\", output_hidden_states=True)\n",
    "        self.l1 = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased',  output_hidden_states=True)\n",
    "        #self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm = torch.nn.LSTM(768, 400, num_layers=2, batch_first = True, bidirectional=True, dropout=0.1)\n",
    "        self.l2 = torch.nn.Dropout(0.1)\n",
    "        self.l3 = torch.nn.Linear( 800, 400)\n",
    "        self.l4 = torch.nn.Linear(400,100) # check with layer with 30\n",
    "        self.l5 = torch.nn.Linear(100,2)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, lengths):\n",
    "        encoded_layers = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)[2]\n",
    "        scibert_hidden_layer = encoded_layers[12]\n",
    "        #bert_hidden_layer = bert_hidden_layer.permute(1, 0, 2)   #permute rotates the tensor. if tensor.shape = 3,4,5  tensor.permute(1,0,2), then tensor,shape= 4,3,5  (batch_size, sequence_length, hidden_size)\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(scibert_hidden_layer, lengths, batch_first=True, enforce_sorted=False)) #enforce_sorted=False  #pack_padded_sequence(data and batch_sizes\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        output_hidden = self.l2(output_hidden)            # no size change\n",
    "        output_2 = self.l3(output_hidden)\n",
    "        output_2 = torch.nn.ReLU()(output_2)\n",
    "        #output_3 = self.l2(output_2)\n",
    "        output_4 = self.l4(output_2)\n",
    "        #output_4 = torch.nn.ReLU()(output_4)\n",
    "        output_5 = self.l5(output_4)\n",
    "        return output_5\n",
    "\n",
    "model = SCIBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_05vppl0Lix"
   },
   "outputs": [],
   "source": [
    "#### Optimizer and Loss function\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr=LEARNING_RATE)\n",
    "c_weights = torch.tensor([list(Counter(train_label_list).values())[0], list(Counter(train_label_list).values())[1]], dtype=torch.float32).to(device)\n",
    "c_weights = 1.0/(c_weights/c_weights.sum())\n",
    "c_weights = c_weights/c_weights.sum()\n",
    "loss_function = torch.nn.CrossEntropyLoss(weight = c_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq4gNHJA0pjR"
   },
   "outputs": [],
   "source": [
    "def calculate_evaluation(f_targets, f_outputs):\n",
    "  accuracy = metrics.accuracy_score(f_targets, f_outputs)\n",
    "  eval = metrics.precision_recall_fscore_support(f_targets, f_outputs, average='micro')\n",
    "  return accuracy, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIiNyuXZ0LmY"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps=0\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        lengths = data['lengths'].squeeze(1)#.to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        #print(\"check shape\",ids.shape,lengths.shape, targets.dtype)\n",
    "        outputs = model(ids, mask, token_type_ids, lengths)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "        # if _%5000==0:\n",
    "        #     print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "        _, output_idx = torch.max(outputs, dim =1)\n",
    "        fin_outputs.extend(output_idx.cpu().detach().numpy().tolist())\n",
    "\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    accuracy, evaluate = calculate_evaluation(fin_targets, fin_outputs)\n",
    "    #f_outputs = np.array(fin_outputs) >= 0.5\n",
    "    #accuracy = metrics.accuracy_score(fin_targets, f_outputs)\n",
    "    #f1_score_micro = metrics.f1_score(fin_targets, f_outputs, average='micro')\n",
    "    #f1_score_macro = metrics.f1_score(fin_targets, f_outputs, average='macro')\n",
    "    #print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"Train Epoch {epoch:02} | Loss:{epoch_loss} | Acc:{accuracy} | Prec:{evaluate[0]} | Rec:{evaluate[1]} | Mic-F1:{evaluate[2]}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLG6werW0LqG"
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, valid_loader):\n",
    "  model.eval()\n",
    "  fin_targets=[]\n",
    "  fin_outputs=[]\n",
    "  nb_tr_steps = 0\n",
    "  tr_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for _, data in enumerate(valid_loader, 0):\n",
    "      ids = data['ids'].to(device, dtype = torch.long)\n",
    "      mask = data['mask'].to(device, dtype = torch.long)\n",
    "      token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "      lengths = data['lengths'].squeeze(1)#.to(device, dtype = torch.long)\n",
    "      targets = data['targets'].to(device, dtype = torch.long)\n",
    "      outputs = model(ids, mask, token_type_ids, lengths)\n",
    "      #temp_id_list = list(map(int, ids.cpu().numpy().tolist())) \n",
    "      # c1 = tokenizer.convert_ids_to_tokens(ids= ids.cpu().numpy().tolist()[0], skip_special_tokens= True)\n",
    "      # print(\"check\",c1)\n",
    "      loss = loss_function(outputs, targets)\n",
    "      tr_loss += loss.item()\n",
    "      fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "      _, output_idx = torch.max(outputs, dim =1)\n",
    "      fin_outputs.extend(output_idx.cpu().detach().numpy().tolist())\n",
    "      nb_tr_steps += 1\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    accuracy, evaluate = calculate_evaluation(fin_targets, fin_outputs)\n",
    "    #print(fin_targets[:15],fin_outputs[:15])\n",
    "    print(f\"Valid Epoch {epoch:02} | Loss:{epoch_loss} | Acc:{accuracy} | Prec:{evaluate[0]} | Rec:{evaluate[1]} | Mic-F1:{evaluate[2]}\")\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "i7bTfAva0Ls1",
    "outputId": "270a9989-1cd1-4c8c-85a8-a85552902e45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 00 | Loss:0.1849026964955725 | Acc:0.9422335394688978 | Prec:0.9422335394688978 | Rec:0.9422335394688978 | Mic-F1:0.9422335394688978\n",
      "Valid Epoch 00 | Loss:0.36055578059055887 | Acc:0.9218601708320152 | Prec:0.9218601708320152 | Rec:0.9218601708320152 | Mic-F1:0.9218601708320152\n",
      "Train Epoch 01 | Loss:0.1047591246469596 | Acc:0.9612222626409603 | Prec:0.9612222626409603 | Rec:0.9612222626409603 | Mic-F1:0.9612222626409603\n",
      "Valid Epoch 01 | Loss:0.4630517481327659 | Acc:0.9304017715912686 | Prec:0.9304017715912686 | Rec:0.9304017715912686 | Mic-F1:0.9304017715912686\n",
      "Train Epoch 02 | Loss:0.08770885153153782 | Acc:0.9673335758457621 | Prec:0.9673335758457621 | Rec:0.9673335758457621 | Mic-F1:0.9673335758457621\n",
      "Valid Epoch 02 | Loss:0.4863064005933326 | Acc:0.9310344827586207 | Prec:0.9310344827586207 | Rec:0.9310344827586207 | Mic-F1:0.9310344827586207\n",
      "Train Epoch 03 | Loss:0.07374544699248577 | Acc:0.9732266278646781 | Prec:0.9732266278646781 | Rec:0.9732266278646781 | Mic-F1:0.9732266278646781\n",
      "Valid Epoch 03 | Loss:0.5323604931960805 | Acc:0.9338816830117052 | Prec:0.9338816830117052 | Rec:0.9338816830117052 | Mic-F1:0.9338816830117052\n",
      "Train Epoch 04 | Loss:0.06284923601332446 | Acc:0.9747544561658785 | Prec:0.9747544561658785 | Rec:0.9747544561658785 | Mic-F1:0.9747544561658784\n",
      "Valid Epoch 04 | Loss:0.585188098863294 | Acc:0.9322999050933249 | Prec:0.9322999050933249 | Rec:0.9322999050933249 | Mic-F1:0.9322999050933249\n",
      "Train Epoch 05 | Loss:0.0514021418918314 | Acc:0.978246635140051 | Prec:0.978246635140051 | Rec:0.978246635140051 | Mic-F1:0.978246635140051\n",
      "Valid Epoch 05 | Loss:0.7159804765398454 | Acc:0.9351471053464093 | Prec:0.9351471053464093 | Rec:0.9351471053464093 | Mic-F1:0.9351471053464093\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-d5adcbd9c05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#new model saved in new location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-a08b6c35dfbe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mfin_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#new model saved in new location\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    validation(epoch, model, valid_loader)\n",
    "    if epoch==3:\n",
    "      torch.save(model,f\"/content/drive/MyDrive/phase-2-task3/{epoch}-predicate-task3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFsd_L6EAALd"
   },
   "source": [
    "# Inference for predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njGvn3zwAEoB"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "########### evaluating model\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "import random\n",
    "seed_val = 66\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0afrXsVkAErQ"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/sub5_ph22/\"\n",
    "# Create a SciBERT + Bi-LSTM model for binary classification\n",
    "class SCIBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SCIBERTClass, self).__init__()\n",
    "        #self.l1 = AutoModel.from_pretrained(\"allenai/biomed_roberta_base\", output_hidden_states=True)\n",
    "        self.l1 = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased',  output_hidden_states=True)\n",
    "        #self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm = torch.nn.LSTM(768, 400, num_layers=2, batch_first = True, bidirectional=True, dropout=0.1)\n",
    "        self.l2 = torch.nn.Dropout(0.1)\n",
    "        self.l3 = torch.nn.Linear( 800, 400)\n",
    "        self.l4 = torch.nn.Linear(400,100) # check with layer with 30\n",
    "        self.l5 = torch.nn.Linear(100,2)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, lengths):\n",
    "        encoded_layers = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)[2]\n",
    "        scibert_hidden_layer = encoded_layers[12]\n",
    "        #bert_hidden_layer = bert_hidden_layer.permute(1, 0, 2)   #permute rotates the tensor. if tensor.shape = 3,4,5  tensor.permute(1,0,2), then tensor,shape= 4,3,5  (batch_size, sequence_length, hidden_size)\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(scibert_hidden_layer, lengths, batch_first=True, enforce_sorted=False)) #enforce_sorted=False  #pack_padded_sequence(data and batch_sizes\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        output_hidden = self.l2(output_hidden)            # no size change\n",
    "        output_2 = self.l3(output_hidden)\n",
    "        output_2 = torch.nn.ReLU()(output_2)\n",
    "        #output_3 = self.l2(output_2)\n",
    "        output_4 = self.l4(output_2)\n",
    "        #output_4 = torch.nn.ReLU()(output_4)\n",
    "        output_5 = self.l5(output_4)\n",
    "        return output_5\n",
    "\n",
    "model = SCIBERTClass()\n",
    "model.to(device)\n",
    "model = torch.load(\"/content/drive/MyDrive/phase-2-task3/3-predicate-task3.pt\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "MAX_LEN = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3kR6zYhAEup"
   },
   "outputs": [],
   "source": [
    "infoname_test_to_idx = {'approach.txt': 0, 'model.txt': 1, 'dataset.txt': 2, \n",
    "        'experimental-setup.txt': 3, 'hyperparameters.txt': 4,  'baselines.txt': 5, 'results.txt': 6, \n",
    "        'tasks.txt': 7, 'experiments.txt': 8, 'ablation-analysis.txt': 9}\n",
    "\n",
    "infoname_idx_to_test = {v: k for k, v in infoname_test_to_idx.items()}\n",
    "\n",
    "infoname_test_to_first_triplet = {'approach.txt': 'Approach', 'model.txt': 'Model', 'dataset.txt': 'Dataset', \n",
    "        'experimental-setup.txt': 'Experimental setup', 'hyperparameters.txt': 'Hyperparameters',  'baselines.txt': 'Baselines', 'results.txt': 'Results', \n",
    "        'tasks.txt': 'Tasks', 'experiments.txt': 'Experiments', 'ablation-analysis.txt': 'Ablation analysis'} #research problem and code has no first triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imIqknJgWip8"
   },
   "outputs": [],
   "source": [
    "### Triple classifier model\n",
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "#self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "class SCIBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SCIBERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', output_hidden_states=True)\n",
    "        self.lstm = torch.nn.LSTM(768, 400, num_layers=2, batch_first = True, bidirectional=True)\n",
    "        self.l2 = torch.nn.Dropout(0.2)\n",
    "        self.l3 = torch.nn.Linear(800, 400)\n",
    "        self.l4 = torch.nn.Linear(400,100)\n",
    "        self.l5 = torch.nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, lengths):\n",
    "        encoded_layers = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)[2]\n",
    "        scibert_hidden_layer = encoded_layers[12]\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(scibert_hidden_layer, lengths, batch_first=True, enforce_sorted=False)) #enforce_sorted=False  #pack_padded_sequence(data and batch_sizes\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        output_hidden = self.l2(output_hidden)            # no size change\n",
    "        output_2 = self.l3(output_hidden)\n",
    "        output_2 = torch.nn.ReLU()(output_2)\n",
    "        output_4 = self.l4(output_2)\n",
    "        output_5 = self.l5(output_4)\n",
    "        return output_5\n",
    "\n",
    "model_trip = SCIBERTClass()\n",
    "model_trip.to(device)\n",
    "model_trip = torch.load(\"/content/drive/MyDrive/modify_scibert-T1/1task3.pt\")\n",
    "model_trip.eval()\n",
    "tokenizer_trip = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "MAX_LEN_trip = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3XuCwBsVAEy-",
    "outputId": "0b93e137-001f-4394-ceda-e82964e49c30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed constituency_parsing\n",
      "completed coreference_resolution\n",
      "completed data-to-text_generation\n",
      "completed dependency_parsing\n",
      "completed document_classification\n",
      "completed entity_linking\n",
      "completed face_alignment\n",
      "completed face_detection\n",
      "completed hypernym_discovery\n",
      "completed natural_language_inference\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#####Test model\n",
    "#test_input_dir = \"/content/drive/My Drive/Dev_v2/\"\n",
    "test_input_dir = \"/content/drive/MyDrive/sub5_ph22/\"\n",
    "# test_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
    "#          \"relation-classification\", \"text-classification\"]\n",
    "test_list_of_folders = list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
    "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
    "                   \"document_classification\",\"entity_linking\",\n",
    "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
    "                   \"natural_language_inference\"]\n",
    "test_input_stanza_list = []\n",
    "test_input_sent_num_list = []\n",
    "test_file_name_list = []\n",
    "test_input_entity_list = []\n",
    "test_input_triple_list = []\n",
    "test_input_unit_name = []\n",
    "test_total_phrases = 0\n",
    "test_triplet_list = []\n",
    "test_total_triplets = 0\n",
    "for fls in test_list_of_folders:\n",
    "  for i in os.listdir(test_input_dir + fls + '/'):\n",
    "    triple_list = []\n",
    "    unit_name = []\n",
    "    triplet_lines_list = []\n",
    "    for files in os.listdir(test_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None, map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        test_input_stanza_list.append(stanza_lines_list)\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        sentence_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
    "        test_input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
    "      if files.endswith(\"entities.txt\"):\n",
    "        entities_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        entities_list = list(filter(None,map(lambda x:x,entities_file.read().splitlines()))) # filter empty strings and split into lines\n",
    "        test_input_entity_list.append(entities_list)\n",
    "      ### start embedd function for the combined task\n",
    "    t_entity_list = [j.split('\\t') for j in entities_list]\n",
    "    t_entity_list.sort(key=lambda x: (int(x[0]),int(x[1]))) #reverse = True)\n",
    "    t_entity_ph =  [j[3] for j in t_entity_list] # entity label in sorted order\n",
    "    t_entity_label = [] # each entity corresponding label, 1= predicate and 0 = non-predicate\n",
    "    #temp_list1 = []\n",
    "    with torch.no_grad():\n",
    "      for t5 in range(len(t_entity_ph)): #lowercase the string input to classifier\n",
    "        test_input = tokenizer.encode_plus(t_entity_ph[t5].lower(), None, add_special_tokens=True, \n",
    "                                           max_length= MAX_LEN, pad_to_max_length=True, \n",
    "                                           return_token_type_ids=True, truncation=True, \n",
    "                                           return_length = True,return_tensors=\"pt\")\n",
    "        test_ids = test_input['input_ids'].to(device, dtype = torch.long)\n",
    "        test_mask = test_input['attention_mask'].to(device, dtype = torch.long)\n",
    "        test_token_type_ids = test_input['token_type_ids'].to(device, dtype = torch.long)\n",
    "        test_lengths = test_input['length']#.squeeze(dim=1)#.to(device, dtype = torch.long)\n",
    "        #print(\"shape\",test_ids.shape, test_mask.shape, test_token_type_ids.shape, test_lengths)\n",
    "        outputs = model(test_ids, test_mask, test_token_type_ids, test_lengths)\n",
    "        _, output_idx = torch.max(outputs, dim =1)\n",
    "        t_output_class = output_idx.cpu().detach().numpy().tolist()\n",
    "        t_entity_label.append(t_output_class[0])\n",
    "        # if t_output_class[0]==1:\n",
    "        #   temp_list1.append(t_entity_ph[t5].lower())\n",
    "    #print(\"pred list\",temp_list1,t_entity_label)\n",
    "    ### start triple formation from predicate\n",
    "    for counterr, i1 in enumerate(zip(t_entity_ph, t_entity_label)):\n",
    "      if (i1[1]==1):\n",
    "        if ((counterr-1)>=0) and ((counterr+1)<len(t_entity_ph)):\n",
    "          B_search = counterr-1\n",
    "          while (B_search>=0):\n",
    "            if (t_entity_label[B_search]==0):\n",
    "              break\n",
    "            else:\n",
    "              B_search = B_search - 1\n",
    "          L_search = counterr+1\n",
    "          while (L_search < len(t_entity_ph)): # coorection should be less than 1\n",
    "            if (t_entity_label[L_search]==0):\n",
    "              break\n",
    "            else:\n",
    "              L_search = L_search + 1    \n",
    "          if (B_search>=0) and (L_search<len(t_entity_ph)):\n",
    "            triple_list = triple_list + [[t_entity_ph[B_search],i1[0],t_entity_ph[L_search]]]\n",
    "\n",
    "    ### end triple formation\n",
    "    output_info_folder = os.path.join(test_input_dir,fls,str(i),'triples')\n",
    "    #print(\"list\", os.listdir(test_input_dir + fls + '/' + str(i)+'/triples/'))\n",
    "    info_applicable = [infoname_test_to_idx[t11] for t11 in os.listdir(output_info_folder) if t11 !='research-problem.txt' and t11 !='code.txt'] ## info indicies applicable to the document\n",
    "\n",
    "    ### start triple classification into info unit\n",
    "    with torch.no_grad():\n",
    "      for t7 in range(len(triple_list)): #lowercase the string input to classifier\n",
    "        test_in_trip = tokenizer_trip.encode_plus((' '.join(triple_list[t7])).lower(), None, add_special_tokens=True, \n",
    "                                           max_length= MAX_LEN_trip, pad_to_max_length=True, \n",
    "                                           return_token_type_ids=True, truncation=True, \n",
    "                                           return_length = True,return_tensors=\"pt\")\n",
    "        test_ids = test_in_trip['input_ids'].to(device, dtype = torch.long)\n",
    "        test_mask = test_in_trip['attention_mask'].to(device, dtype = torch.long)\n",
    "        test_token_type_ids = test_in_trip['token_type_ids'].to(device, dtype = torch.long)\n",
    "        test_lengths = test_in_trip['length']#.squeeze(dim=1)#.to(device, dtype = torch.long)\n",
    "        #print(\"shape\",test_ids.shape, test_mask.shape, test_token_type_ids.shape, test_lengths)\n",
    "        outputs_trip = model_trip(test_ids, test_mask, test_token_type_ids, test_lengths)\n",
    "        #_, output_idx_trip = torch.max(outputs, dim =1)\n",
    "        output_all_class = outputs_trip.cpu().detach().numpy().tolist()\n",
    "        #print(t_output_class,'(' + '||'.join(t_triplet_formed[0])+')')\n",
    "        output_app_class = [output_all_class[0][t12] if t12 in info_applicable else -10000 for t12 in range(len(output_all_class[0]))]\n",
    "        \n",
    "        output_file=open(os.path.join(output_info_folder, infoname_idx_to_test[output_app_class.index(max(output_app_class))]),\"a\")\n",
    "        output_file.write('(' + '||'.join(triple_list[t7])+')'+\"\\n\")\n",
    "        # if output_app_class.index(max(output_app_class)) not in info_applicable:\n",
    "        #   print(\"this\", info_applicable, len(output_all_class[0]), output_app_class.index(max(output_app_class)))\n",
    "\n",
    "        #for t7 in range(len(t_output_class)):\n",
    "          #print(infoname_test_to_idx[t_output_class[t7]])\n",
    "        # output_file=open(os.path.join(output_info_folder, infoname_idx_to_test[t_output_class[0]]),\"a\")\n",
    "        # output_file.write('(' + '||'.join(t_triplet_formed[t6])+')'+\"\\n\")\n",
    "    ### end triple classification into info unit\n",
    "  print(\"completed\",fls)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ax6rW2zkfIM",
    "outputId": "f286fa9c-1c4a-479e-d5e1-781a8550ec32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['RC', 'develop', 'novel context zoom - in network ( ConZNet )'],\n",
       " ['novel context zoom - in network ( ConZNet )', 'for', 'RC tasks'],\n",
       " ['RC tasks', 'skip through', 'irrelevant parts'],\n",
       " ['irrelevant parts', 'of', 'document'],\n",
       " ['document', 'generate', 'answer'],\n",
       " ['answer', 'using', 'only the relevant regions of text'],\n",
       " ['ConZNet architecture', 'consists of', 'two phases'],\n",
       " ['two phases', 'In', 'first phase'],\n",
       " ['first phase', 'identify', 'relevant regions'],\n",
       " ['relevant regions', 'of', 'text'],\n",
       " ['text', 'by employing', 'reinforcement learning algorithm'],\n",
       " ['second phase', 'based on', 'encoder - decoder architecture'],\n",
       " ['identified regions of text', 'generates', 'answer'],\n",
       " ['answer', 'by using', 'residual self - attention network'],\n",
       " ['residual self - attention network', 'as', 'encoder'],\n",
       " ['RNNbased sequence generator', 'along with', 'pointer network'],\n",
       " ['pointer network', 'as', 'decoder'],\n",
       " ['ability', 'to select', 'relevant regions'],\n",
       " ['relevant regions', 'of', 'text'],\n",
       " ['text', 'not just based on', 'question'],\n",
       " ['question', 'also on', 'regions'],\n",
       " ['regions', 'related to', 'each other'],\n",
       " ['our decoder', 'combines', 'span prediction'],\n",
       " ['sequence generation', 'allows', 'decoder'],\n",
       " ['decoder', 'to copy', 'words'],\n",
       " ['words', 'from', 'relevant regions'],\n",
       " ['relevant regions', 'of', 'text'],\n",
       " ['text', 'to generate', 'words'],\n",
       " ['words', 'from', 'fixed vocabulary'],\n",
       " ['fixed vocabulary', 'compare', 'our model'],\n",
       " ['our model', 'against', 'reported models'],\n",
       " ['reported models', 'in', 'Seq2Seq'],\n",
       " ['Multi-range Reasoning Unit ( MRU )', 'implemented', 'two baseline models'],\n",
       " ['two baseline models', 'with', 'Context Zoom layer'],\n",
       " ['Context Zoom layer', 'model', 'Python'],\n",
       " ['Context Zoom layer', 'implemented using', 'Python'],\n",
       " ['weights', 'of', 'Glorot Initialization'],\n",
       " ['weights', 'model', 'Glorot Initialization'],\n",
       " ['weights', 'initialized by', 'Glorot Initialization'],\n",
       " ['biases', 'initialized with', '300 dimensional word vectors'],\n",
       " ['biases', 'zeros', '300 dimensional word vectors'],\n",
       " ['biases', 'use', '300 dimensional word vectors'],\n",
       " ['300 dimensional word vectors', 'from', 'GloVe'],\n",
       " ['GloVe', 'with', '840 billion pre-trained vectors'],\n",
       " ['840 billion pre-trained vectors', 'to initialize', 'word embeddings'],\n",
       " ['All the words', 'not appear in', 'Glove'],\n",
       " ['Glove', 'initialized by', 'uniform random distribution'],\n",
       " ['Glove', 'sampling', 'uniform random distribution'],\n",
       " ['Glove', 'from', 'uniform random distribution'],\n",
       " ['uniform random distribution', 'apply', 'dropout'],\n",
       " ['dropout', 'between', 'layers'],\n",
       " ['layers', 'with', 'keep probability'],\n",
       " ['keep probability', 'of', '0.8'],\n",
       " ['number of hidden units', 'set to', '100'],\n",
       " ['100', 'trained', 'our model'],\n",
       " ['our model', 'with', 'AdaDelta ( Zeiler , 2012 ) optimizer'],\n",
       " ['AdaDelta ( Zeiler , 2012 ) optimizer', 'for', '50 epochs'],\n",
       " ['initial learning rate', 'of', '0.1'],\n",
       " ['minibatch size', 'of', '32'],\n",
       " ['32', 'performance', 'our model'],\n",
       " ['32', 'of', 'our model'],\n",
       " ['our model', 'gradually dropped', 'sample size 7 onwards'],\n",
       " ['our model', 'from', 'sample size 7 onwards'],\n",
       " ['sample size 7 onwards', 'shows', 'evidence'],\n",
       " ['only a few relevant sentences', 'sufficient to', 'answer'],\n",
       " ['improved dramatically', 'with', 'sample sizes 3 and 5'],\n",
       " ['sample sizes 3 and 5', 'compared to', 'sample size of 1'],\n",
       " ['self - attention mechanism', 'in', 'Context zoom layer'],\n",
       " ['important component', 'to identify', 'related relevant sentences']]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45dNFR_mBkzW",
    "outputId": "5a1609c0-15f6-49e5-849c-29489e93817d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constituency_parsing 1\n",
      "coreference_resolution 8\n",
      "coreference_resolution 4\n",
      "data-to-text_generation 0\n",
      "dependency_parsing 8\n",
      "entity_linking 7\n",
      "face_alignment 8\n",
      "face_alignment 5\n",
      "face_alignment 13\n",
      "face_alignment 12\n",
      "face_alignment 1\n",
      "face_detection 6\n",
      "face_detection 21\n",
      "face_detection 7\n",
      "face_detection 4\n",
      "face_detection 15\n",
      "face_detection 13\n",
      "natural_language_inference 4\n",
      "natural_language_inference 26\n",
      "natural_language_inference 22\n",
      "natural_language_inference 25\n"
     ]
    }
   ],
   "source": [
    "test_input_dir = \"/content/drive/MyDrive/sub5_ph22/\"\n",
    "# Test dataset reading\n",
    "import os\n",
    "from shutil import copyfile\n",
    "test_list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
    "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
    "                   \"document_classification\",\"entity_linking\",\n",
    "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
    "                   \"natural_language_inference\"]\n",
    "t_files = 0\n",
    "for fls in test_list_of_folders:\n",
    "  for i in os.listdir(test_input_dir + fls + '/'):\n",
    "    if 'ablation-analysis.txt' in os.listdir(test_input_dir + fls + '/' + str(i)+'/triples'):\n",
    "    #if 'model.txt' not in os.listdir(test_input_dir + fls + '/' + str(i)+'/triples') and 'approach.txt' not in os.listdir(test_input_dir + fls + '/' + str(i)+'/triples'):\n",
    "      t_files=t_files+1\n",
    "      print(fls,i)\n",
    "    # for files in os.listdir(test_input_dir + fls + '/' + str(i)+'/triples'):\n",
    "    #   if files.endswith(\"model.txt\"):\n",
    "    #     t_files=t_files+1\n",
    "  #print(\"completed\",fls,t_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jFn-P3xsTLf",
    "outputId": "40019027-8fc1-4450-dd59-094b2a6c8fc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed sent constituency_parsing 9\n",
      "completed entity constituency_parsing\n",
      "completed sent coreference_resolution 10\n",
      "completed entity coreference_resolution\n",
      "completed sent data-to-text_generation 7\n",
      "completed entity data-to-text_generation\n",
      "completed sent dependency_parsing 9\n",
      "completed entity dependency_parsing\n",
      "completed sent document_classification 21\n",
      "completed entity document_classification\n",
      "completed sent entity_linking 17\n",
      "completed entity entity_linking\n",
      "completed sent face_alignment 19\n",
      "completed entity face_alignment\n",
      "completed sent face_detection 22\n",
      "completed entity face_detection\n",
      "completed sent hypernym_discovery 9\n",
      "completed entity hypernym_discovery\n",
      "completed sent natural_language_inference 32\n",
      "completed entity natural_language_inference\n"
     ]
    }
   ],
   "source": [
    "# Test dataset reading\n",
    "import os\n",
    "from shutil import copyfile\n",
    "output_dir = \"/content/drive/My Drive/sub9_p22/\"\n",
    "test1_input_dir = \"/content/drive/My Drive/sub1_p22/\"\n",
    "test_list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
    "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
    "                   \"document_classification\",\"entity_linking\",\n",
    "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
    "                   \"natural_language_inference\"]\n",
    "test1_file_name_list = []\n",
    "t_files = 0\n",
    "for fls in test_list_of_folders:\n",
    "  count=0\n",
    "  for i in os.listdir(output_dir + fls + '/'):\n",
    "    count=count+1\n",
    "    for files in os.listdir(output_dir + fls + '/' + str(i)+'/triples/'):\n",
    "      if files.endswith(\"research-problem.txt\"):\n",
    "        #os.remove(os.path.join(output_dir,fls,str(i),'triples',files))\n",
    "        #copyfile(test1_input_dir + fls + '/' + str(i) + '/' + files, output_dir + fls + '/' + str(i) + '/' + files)\n",
    "        t_files=t_files+1\n",
    "    test1_file_name_list.append(fls + '/' + str(i))\n",
    "  print(\"completed sent\",fls,count)\n",
    "  print(\"completed entity\",fls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5jS62SDyJfj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "y7v26VGi47T2",
    "vG4lEjR-5_Jw",
    "xIrX-GYao_HZ"
   ],
   "name": "30/01/21 T3 and Novel Predicate Selection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01678666028e4273afd75e2b045ce180": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0605bc767b2a4bdda845f89ee8813e66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2b7b21f157604fa2b8286e21973ca647": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e73bcabc7a24731b5d5c230e6a91f4d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ef8f3cd00fe4836abe4daa601845d9b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c777914bf824e07b95f9ea6804df26f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b7253170d6ed4f2ea5f8e64a6c78ea12",
       "IPY_MODEL_fd494f86c5544b509d534f0b8fe06735"
      ],
      "layout": "IPY_MODEL_01678666028e4273afd75e2b045ce180"
     }
    },
    "552b66794a10470f8115c8978017009f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ef8f3cd00fe4836abe4daa601845d9b",
      "max": 442221694,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0605bc767b2a4bdda845f89ee8813e66",
      "value": 442221694
     }
    },
    "5c06cc0b4b7540b7bba0b10686761285": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "663e0e8bc0214b9bb23cc289b831fa67": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74252d88d06a44a99551c2c7f11098a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ee9a58040734516a2d3cfe72b487d86",
      "max": 385,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f0b9258d2e3e4677990651dc85c6d380",
      "value": 385
     }
    },
    "7b03c110ebe14542ad2c3f2eab0e8d23": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7def957200934c688c3984d65a042f18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_74252d88d06a44a99551c2c7f11098a9",
       "IPY_MODEL_a292ce36f6504636bd83f7a6815b37b5"
      ],
      "layout": "IPY_MODEL_7b03c110ebe14542ad2c3f2eab0e8d23"
     }
    },
    "7ee9a58040734516a2d3cfe72b487d86": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a52121f3e21446d9995a0eae5dfa9cb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9175e21ac0e84129915d6fc5c3164e7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a292ce36f6504636bd83f7a6815b37b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e73bcabc7a24731b5d5c230e6a91f4d",
      "placeholder": "​",
      "style": "IPY_MODEL_da0ca5b647ef4cac9f7040ffdfc64509",
      "value": " 385/385 [12:49&lt;00:00, 2.00s/B]"
     }
    },
    "b23bb2d100484b3388fe5588cf33c61e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7253170d6ed4f2ea5f8e64a6c78ea12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b7b21f157604fa2b8286e21973ca647",
      "max": 227845,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb9fcf8f754d4aceba9281389785e4fe",
      "value": 227845
     }
    },
    "be0b351448c242048aeca38148ede6dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_552b66794a10470f8115c8978017009f",
       "IPY_MODEL_c7437879651b44999e8d5c1519faf03b"
      ],
      "layout": "IPY_MODEL_b23bb2d100484b3388fe5588cf33c61e"
     }
    },
    "c7437879651b44999e8d5c1519faf03b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a52121f3e21446d9995a0eae5dfa9cb",
      "placeholder": "​",
      "style": "IPY_MODEL_5c06cc0b4b7540b7bba0b10686761285",
      "value": " 442M/442M [00:24&lt;00:00, 18.1MB/s]"
     }
    },
    "cb9fcf8f754d4aceba9281389785e4fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "da0ca5b647ef4cac9f7040ffdfc64509": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0b9258d2e3e4677990651dc85c6d380": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fd494f86c5544b509d534f0b8fe06735": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_663e0e8bc0214b9bb23cc289b831fa67",
      "placeholder": "​",
      "style": "IPY_MODEL_9175e21ac0e84129915d6fc5c3164e7b",
      "value": " 228k/228k [00:01&lt;00:00, 208kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
